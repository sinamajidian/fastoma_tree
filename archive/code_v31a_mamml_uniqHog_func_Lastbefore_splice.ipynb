{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0927039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5fe0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5004f57c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bfe573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# IMPORTNAT  this code accept only .fa file as proteome\n",
    "\n",
    "def parse_oma_db(oma_database_address):\n",
    "    \n",
    "    ############### Parsing OMA db ####################\n",
    "    ###################################################\n",
    "\n",
    "    oma_db = db.Database(oma_database_address)\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- OMA data is parsed and its release name is:\", oma_db.get_release_name())\n",
    "    list_speices= [z.uniprot_species_code for z in oma_db.tax.genomes.values()] \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- There are\",len(list_speices),\"species in the OMA database.\")\n",
    "    \n",
    "    return (oma_db, list_speices)\n",
    "\n",
    "\n",
    "def parse_hog_og_map(hog_og_map_address):\n",
    "    \n",
    "    ############### Parsing hog map python dic #############\n",
    "    #########################################################\n",
    "\n",
    "    oma_db = db.Database(oma_database_address)\n",
    "\n",
    "    file = open(hog_og_map_address, \"r\")\n",
    "    contents = file.read()\n",
    "    hog_OG_map = ast.literal_eval(contents)\n",
    "    file.close()\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- The hog-og map is read from file with the length of \", len(hog_OG_map))\n",
    "       \n",
    "    return (hog_OG_map)\n",
    "\n",
    "\n",
    "\n",
    "def HOG_OG_map(oma_database_address):\n",
    "\n",
    "    oma_db = db.Database(oma_database_address)\n",
    "    print(\"OMA data is parsed and its release name is:\", oma_db.get_release_name())\n",
    "    list_speices= [z.uniprot_species_code for z in oma_db.tax.genomes.values()] \n",
    "    print(\"There are\",len(list_speices),\"species in the OMA database.\")\n",
    "\n",
    "    hog_OG_map = {hog_id.decode(\"utf-8\") : og_count_list[0][0] for hog_id, og_count_list in build_hog_to_og_map(oma_db)}\n",
    "    print(\"HOG-OG map is extracted with length of \", len(hog_OG_map))\n",
    "\n",
    "\n",
    "    hog_OG_map_address = oma_database_address+\"_hog_og_map.dic\"\n",
    "    with open(hog_OG_map_address, 'w') as file_map:\n",
    "        print(hog_OG_map, file=file_map)\n",
    "\n",
    "    file_map.close()\n",
    "    print(\"HOG-OG map is written in the file\",hog_OG_map_address)\n",
    "    \n",
    "    return hog_OG_map\n",
    "\n",
    "\n",
    "def parse_proteome(list_speices):\n",
    "    \n",
    "    ############### Parsing query proteome of species #######\n",
    "    #########################################################\n",
    "\n",
    "    \n",
    "    # IMPORTNAT  this code accept only .fa file as proteome\n",
    "    project_files = listdir(project_folder+\"/omamer_search/proteome/\")\n",
    "\n",
    "    query_species_names = []\n",
    "    for file in project_files:\n",
    "        if file.split(\".\")[-1]==\"fa\":\n",
    "            file_name_split = file.split(\".\")[:-1]\n",
    "            query_species_names.append('.'.join(file_name_split))\n",
    "        if file.split(\".\")[-1]==\"fasta\":\n",
    "            file_name_split = file.split(\".\")[:-1]\n",
    "            query_species_names.append('.'.join(file_name_split))\n",
    "\n",
    "    # we may assert existence of query_species_name+\".fa/hogmap\"\n",
    "    query_prot_records_species = [ ]\n",
    "    for query_species_name in query_species_names:\n",
    "        query_prot_address = project_folder +\"omamer_search/proteome/\"+ query_species_name + \".fa\" \n",
    "        \n",
    "        query_prot_records = list(SeqIO.parse(query_prot_address, \"fasta\")) \n",
    "        query_prot_records_species.append(query_prot_records)\n",
    "\n",
    "    query_species_num = len(query_species_names)    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- The are\",str(query_species_num),\"species in the proteome folder.\")\n",
    "\n",
    "    # for development\n",
    "    query_species_num = len(query_species_names)\n",
    "    for species_i in range(query_species_num):\n",
    "        len_prot_record_i = len( query_prot_records_species[species_i] )\n",
    "        species_name_i = query_species_names[species_i]\n",
    "        #print(species_name_i,len_prot_record_i)\n",
    "        if species_name_i in list_speices: \n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(current_time,\"- the species\",species_name_i,\" already exists in the oma database, remove them first\")\n",
    "            exit()\n",
    "\n",
    "      \n",
    "    # The proteins are parsed using  Bio.SeqIO.parse\n",
    "    # the first part of the header line before space \n",
    "    # >tr|A0A2I3FYY2|A0A2I3FYY2_NOMLE Uncharacterized protein OS=Nomascus leucogenys OX=61853 GN=CLPTM1L PE=3 SV=1\n",
    "    # will be \">tr|A0A2I3FYY2|A0A2I3FYY2_NOMLE\"\n",
    "    # [i.id for i in query_prot_records_species[0] if len(i.id)!=30 and len(i.id)!=22 ] #'sp|O47892|CYB_NOMLE',\n",
    "    \n",
    "\n",
    "    \n",
    "    return (query_species_names, query_prot_records_species)\n",
    "\n",
    "def parse_splice_file(query_species_names):\n",
    "\n",
    "    splices_list_species = []\n",
    "    species_name_noSplice=[]\n",
    "    for query_species_name in query_species_names:\n",
    "        query_splice_address = project_folder +\"omamer_search/proteome/\"+ query_species_name + \".splice\" \n",
    "\n",
    "\n",
    "        splices_list = []\n",
    "\n",
    "\n",
    "        try:\n",
    "            query_splice_file = open(query_splice_address,'r');    \n",
    "            for line in query_splice_file:\n",
    "                #print(line)\n",
    "                line_strip=line.strip()\n",
    "                line_split= line_strip.split(\";\") \n",
    "                splices= line_split\n",
    "                splices_list.append(splices)\n",
    "        except: \n",
    "            species_name_noSplice.append(query_species_name)\n",
    "\n",
    "        splices_list_species.append(splices_list)\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- No splicing file found for these species: \", species_name_noSplice)\n",
    "\n",
    "    return splices_list_species\n",
    "\n",
    "\n",
    "\n",
    "def parse_hogmap_omamer(query_species_names):\n",
    "\n",
    "    ################### Parsing omamer's output  ########\n",
    "    #####################################################\n",
    "\n",
    "    query_prot_names_species_mapped = []\n",
    "    query_hogids_species = []\n",
    "    query_prot_seqlen_species = []\n",
    "    query_prot_submedseqlen_species = [] # subfamily-medianseqlen\n",
    "    query_prot_subscore_species = []\n",
    "\n",
    "    for query_species_name in query_species_names:\n",
    "        omamer_output_address = project_folder + \"omamer_search/hogmap/\"+ query_species_name + \".hogmap\"     \n",
    "        omamer_output_file = open(omamer_output_address,'r');\n",
    "\n",
    "        query_prot_names = []\n",
    "        query_hogids = []\n",
    "        query_prot_seqlen = []\n",
    "        query_prot_submedseqlen = []\n",
    "        query_prot_subscore = []\n",
    "\n",
    "        \n",
    "        for line in omamer_output_file:\n",
    "            line_strip=line.strip()\n",
    "            if not line_strip.startswith('qs'):\n",
    "                line_split= line_strip.split(\"\\t\")    \n",
    "                #if line_split[1]!='na':\n",
    "                query_prot_names.append(line_split[0])\n",
    "                query_hogids.append(line_split[1])\n",
    "                query_prot_subscore.append(line_split[4]) # subfamily\n",
    "                query_prot_seqlen.append(line_split[5])\n",
    "                query_prot_submedseqlen.append(line_split[6])\n",
    "                \n",
    "        #print(\"number of proteins in omamer output for \",query_species_name,\"is\",len(query_hogids)) # ,query_hogids\n",
    "        query_prot_names_species_mapped.append(query_prot_names)\n",
    "        query_hogids_species.append(query_hogids)    \n",
    "        query_prot_seqlen_species.append(query_prot_seqlen)\n",
    "        query_prot_submedseqlen_species.append(query_prot_submedseqlen)\n",
    "        query_prot_subscore_species.append(query_prot_subscore)\n",
    "        \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- There are \",len(query_prot_names_species_mapped),\" in hogmap folder.\")\n",
    "    print(current_time,\"- The first species\",query_species_names[0],\" contains \",len(query_hogids_species[0]),\" proteins.\")\n",
    "    print(current_time,\"- The first protein of first species is \",query_prot_names[0])\n",
    "\n",
    "    \n",
    "    return (query_prot_names_species_mapped, query_hogids_species,query_prot_subscore_species , query_prot_seqlen_species, query_prot_submedseqlen_species)\n",
    "    \n",
    "    \n",
    "\n",
    "def filter_prot_mapped(query_species_names, query_prot_records_species,query_prot_names_species_mapped):\n",
    "    # omamer remove very small proteins, \n",
    "    # so  we lose track of order comparing hogmap and fasta file\n",
    "    # the goal here is to remove those from seq record (of the fasta file)\n",
    "    query_prot_records_species_filtered=[]\n",
    "    for species_idx in range(len(query_species_names)):    \n",
    "        # from fasta file\n",
    "        query_species_name=query_species_names[species_idx]\n",
    "        query_prot_records_species_i = query_prot_records_species[species_idx]\n",
    "        query_prot_ids_records = [record.id for record in query_prot_records_species_i]\n",
    "\n",
    "        # from hogmap file\n",
    "        # without proteins that are not mapped on any hogs\n",
    "        query_prot_names_species_i = query_prot_names_species_mapped[species_idx]\n",
    "\n",
    "        if len(query_prot_names_species_i) != len(query_prot_records_species_i):\n",
    "\n",
    "            query_prot_records_filterd=[]\n",
    "            for query_prot_name in query_prot_names_species_i:\n",
    "                if query_prot_name in query_prot_ids_records:\n",
    "                    prot_record_idx = query_prot_ids_records.index(query_prot_name)\n",
    "                    prot_record = query_prot_records_species_i[prot_record_idx]\n",
    "                    query_prot_records_filterd.append(prot_record)\n",
    "                else:\n",
    "                    current_time = datetime.now().strftime(\"%H:%M:%S\")                    \n",
    "                    print(current_time,\"- Error\",query_species_name, query_prot_name)\n",
    "\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")        \n",
    "            print(current_time,\"- For the species\", query_species_name, \", few proteins were ignored by omamer.\")\n",
    "            print(current_time,\"- before filtering: in hogmap\", len(query_prot_names_species_i), \"in proteome\", len(query_prot_records_species_i))\n",
    "            print(current_time,\"- After filtering:  in hogmap\", len(query_prot_names_species_i), \"in proteome\", len(query_prot_records_filterd))            \n",
    "            \n",
    "\n",
    "        else:\n",
    "            query_prot_records_filterd = query_prot_records_species_i\n",
    "\n",
    "        query_prot_records_species_filtered.append(query_prot_records_filterd)\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")        \n",
    "    print(current_time,\"- For the rest of species, all proteins were mapped using OMAmer.\")\n",
    "\n",
    "    return query_prot_records_species_filtered\n",
    "\n",
    "\n",
    "\n",
    "def extract_unique_hog_pure(query_species_names,query_hogids_species, query_prot_names_species_mapped,query_prot_records_species):\n",
    "    ###### Extracting unique HOG list and corresponding query proteins ########\n",
    "    ###########################################################################\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- Extracting proteins mapped only once on a HOG is started\")\n",
    "\n",
    "    query_hogids_filtr_species = []\n",
    "    query_prot_names_filtr_species = []\n",
    "    query_prot_records_filtr_species = []\n",
    "\n",
    "    repeated_hogid_num = 0\n",
    "    \n",
    "    query_species_num = len(query_species_names) \n",
    "    \n",
    "    dic_hogs_list=[]  # a list of dictinaries \n",
    "\n",
    "\n",
    "    for species_i in range(query_species_num):\n",
    "\n",
    "        query_hogids =  query_hogids_species[species_i]\n",
    "        query_prot_names = query_prot_names_species_mapped[species_i]\n",
    "        query_prot_records  = query_prot_records_species[species_i]\n",
    "        \n",
    "        dic_hogs = {}\n",
    "        for prot_i in range(len(query_hogids)):\n",
    "            query_hogid      = query_hogids[prot_i]\n",
    "            query_prot_name  = query_prot_names[prot_i]\n",
    "            query_prot_record= query_prot_records[prot_i]\n",
    "            if query_hogid!= 'na':\n",
    "                if  query_hogid  not in  dic_hogs:\n",
    "                    dic_hogs[query_hogid]=[(query_prot_name,query_prot_record)]\n",
    "                else:\n",
    "                    repeated_hogid_num += 1 \n",
    "                    dic_hogs[query_hogid].append((query_prot_name,query_prot_record))\n",
    "        dic_hogs_list.append(dic_hogs)\n",
    "\n",
    "        \n",
    "    for dic_hogs in dic_hogs_list: # each species\n",
    "        \n",
    "        query_prot_names_filtr = []\n",
    "        query_prot_records_filtr = []\n",
    "\n",
    "        hogid_list = list(dic_hogs.keys())\n",
    "        for hogid in hogid_list:\n",
    "            list_query_prot = dic_hogs[hogid]\n",
    "            if len(list_query_prot)>1:\n",
    "                del dic_hogs[hogid]\n",
    "        #here dic_hogs is updated and  dic_hogs_list  is also updated.\n",
    "        \n",
    "        #print(len(hogid_list),len(dic_hogs))\n",
    "        #for key, value in d.items():\n",
    "        query_hogids_filtr = []\n",
    "        query_prot_names_filtr = []\n",
    "        query_prot_records_filtr = []        \n",
    "        #query_hogids_filtr=list(dic_hogs.keys())\n",
    "        for hogid, tuple_value  in dic_hogs.items():\n",
    "            query_hogids_filtr.append(hogid)\n",
    "            query_prot_names_filtr.append(tuple_value[0][0])\n",
    "            query_prot_records_filtr.append(tuple_value[0][1])\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        query_hogids_filtr_species.append(query_hogids_filtr)\n",
    "        query_prot_names_filtr_species.append(query_prot_names_filtr)\n",
    "        query_prot_records_filtr_species.append(query_prot_records_filtr)        \n",
    "    \n",
    "    current_time  = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- Extracting proteins mapped only once on a HOG is finished\")\n",
    "    num_all_hogs=np.sum([len(dic_hogs) for dic_hogs in dic_hogs_list])\n",
    "    print(current_time,\"- For \",len(dic_hogs_list),\" species, we keep \",num_all_hogs,\"HOGs.\")\n",
    "    \n",
    "\n",
    "    return (query_hogids_filtr_species, query_prot_names_filtr_species, query_prot_records_filtr_species )\n",
    "  \n",
    "\n",
    "def gather_OG(query_species_names, query_hogids_filtr_species, query_prot_names_filtr_species, query_prot_records_filtr_species):\n",
    "\n",
    "    ############ Extracting the most frequent OG  ########\n",
    "    #####################################################\n",
    "\n",
    "    #dict (oma_group_nr -> dict(species, [proteins]))\n",
    "    #Og[555] = {homo_erectus: [blabla, blublu], yellow_bird: [P52134], brown_bear: [P2121,B53223]}\n",
    "\n",
    "    OGs_queries = {}\n",
    "\n",
    "    # hog_OG_map = {}\n",
    "\n",
    "    mostFrequent_OG_list_species = []\n",
    "\n",
    "    frq_most_frequent_og_list_all = [] # for development\n",
    "    \n",
    "    query_species_num = len(query_species_names)  \n",
    "    for species_i in  range(query_species_num):\n",
    "\n",
    "        query_species_name = query_species_names[species_i]\n",
    "        #print(\"\\n\",query_species_name)\n",
    "\n",
    "        query_hogids_filtr = query_hogids_filtr_species[species_i]\n",
    "        query_prot_names_filtr = query_prot_names_filtr_species[species_i]\n",
    "        query_prot_records_filtr = query_prot_records_filtr_species[species_i]\n",
    "\n",
    "        mostFrequent_OG_list=[]\n",
    "\n",
    "        num_query_filtr = len(query_hogids_filtr)\n",
    "        for  item_idx in range(num_query_filtr): #\n",
    "\n",
    "            #query_protein = query_prot_names_filtr[item_idx]\n",
    "            seqRecords_query =  query_prot_records_filtr[item_idx]\n",
    "            seqRecords_query_edited = SeqRecord(Seq(str(seqRecords_query.seq)), query_species_name, '', '')\n",
    "            #print(seqRecords_query_edited)\n",
    "\n",
    "            hog_id= query_hogids_filtr[item_idx]\n",
    "\n",
    "            if not hog_id in hog_OG_map:   # Caculitng  most frq OG for the new hog\n",
    "                mostFrequent_OG= -1\n",
    "                hog_OG_map[hog_id]=mostFrequent_OG\n",
    "\n",
    "            else:  # hog_id is in hog_OG_map dic\n",
    "                #print(\"using the hog-og-map\")\n",
    "                mostFrequent_OG = hog_OG_map[hog_id]\n",
    "\n",
    "            if mostFrequent_OG in OGs_queries:\n",
    "                OGs_queries_k = OGs_queries[mostFrequent_OG]\n",
    "\n",
    "                if not query_species_name in OGs_queries_k:\n",
    "                    OGs_queries_k[query_species_name] = seqRecords_query_edited\n",
    "                    OGs_queries[mostFrequent_OG] = OGs_queries_k\n",
    "            else:\n",
    "                OGs_queries[mostFrequent_OG] = {query_species_name: seqRecords_query_edited} # query_protein = query_prot_names_filtr[item_idx]\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Needed HOH-OG map \",len(OGs_queries),\"are extracted from the map file.\") \n",
    "    \n",
    "    return OGs_queries\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def filter_gathered_OG(OGs_queries, query_species_names,keep_og_treshold_species_query):\n",
    "\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Start filtering OGs of proteme queries with length of \", len(OGs_queries),\"for \",len(query_species_names),\"species.\") \n",
    "        \n",
    "    OGs_queries_filtr={}\n",
    "\n",
    "    query_species_og_kept_set= set()\n",
    "\n",
    "    query_species_num_OGs_list=[]\n",
    "    for OG_id, values in OGs_queries.items():\n",
    "        query_species_og= list(values.keys())\n",
    "\n",
    "        query_species_num_OGs_list.append(len(query_species_og)) \n",
    "        if len(query_species_og) > keep_og_treshold_species_query:\n",
    "            OGs_queries_filtr[OG_id]=values\n",
    "            query_species_og_kept_set= query_species_og_kept_set.union(set(query_species_og))\n",
    "\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Finished filetering from \",len(OGs_queries),\", now\",len(OGs_queries_filtr),\"OGs are remained.\") \n",
    "               \n",
    "\n",
    "\n",
    "    not_included_species =[]\n",
    "    for i in set(query_species_names):\n",
    "        if i not in query_species_og_kept_set:\n",
    "            not_included_species.append(i)\n",
    "\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- These species missed due to the threshold of\",keep_og_treshold_species_query,\":\",not_included_species) \n",
    "               \n",
    "\n",
    "\n",
    "    #query_species_num_OGs_list\n",
    "    # for i in not_included_species:\n",
    "    #     #print(i)\n",
    "    #     for OG_id, values in OGs_queries.items():\n",
    "    #         if i in values:\n",
    "    #             print(values.keys())\n",
    "    # easiet way keep the first one or make list and keep the best\n",
    "    # but better to set trehsold not to misss any species \n",
    "    \n",
    "    return OGs_queries_filtr\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def combine_OG_query_filtr(OGs_queries_filtr, oma_db, threshold_least_query_sepecies_in_OG,kept_oma_species_num):\n",
    "    \n",
    "#     ########## Combine proteins of OG with queries ##################\n",
    "#     #################################################################\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Combining queries with OG started for\",len(OGs_queries_filtr),\"OGs.\") \n",
    "    \n",
    "    species_og_dic={}\n",
    "    \n",
    "    proteins_object_OG_dic={}\n",
    "    for OG_q in OGs_queries_filtr.keys():  # OG found in the query\n",
    "\n",
    "        dic_species_prot = OGs_queries_filtr[OG_q]\n",
    "        if len(dic_species_prot) >threshold_least_query_sepecies_in_OG:\n",
    "            if OG_q != -1:\n",
    "                OG_members = oma_db.oma_group_members(OG_q)\n",
    "                proteins_object_OG = [db.ProteinEntry(oma_db, pr) for pr in OG_members]  # the protein IDs of og members\n",
    "                proteins_object_OG_dic[OG_q]=proteins_object_OG\n",
    "                \n",
    "                species_all_og = [ str(pr.genome.uniprot_species_code) for pr in proteins_object_OG ]\n",
    "\n",
    "                for species in species_all_og:\n",
    "                    if species in species_og_dic:\n",
    "                        #species_og_dic[species].append(OG_q)                        \n",
    "                        species_og_dic[species] +=1 \n",
    "                    else:\n",
    "                        #species_og_dic[species]=[OG_q]\n",
    "                        species_og_dic[species] = 1\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Number of OMA species per OG is calculated\") \n",
    "                         \n",
    "    species_oma_list=[]\n",
    "    og_num_list = []\n",
    "    for species_oma, og_num in species_og_dic.items():\n",
    "        og_num_list.append(og_num)\n",
    "        species_oma_list.append(species_oma)\n",
    "        \n",
    "    id_species_keep = np.argsort(og_num_list)[-kept_oma_species_num:]\n",
    "    species_oma_kept = [species_oma_list[i] for i in id_species_keep]\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- These OMA species are kept\",species_oma_kept) \n",
    "\n",
    "    \n",
    "    seqRecords_OG_queries = []\n",
    "    seqRecords_all_filtr = []\n",
    "    for OG_q in OGs_queries_filtr.keys():  # OG found in the query\n",
    "\n",
    "        dic_species_prot = OGs_queries_filtr[OG_q]\n",
    "        if len(dic_species_prot) >threshold_least_query_sepecies_in_OG:\n",
    "            \n",
    "            seqRecords_query_edited_all = list(dic_species_prot.values())\n",
    "            if OG_q != -1:\n",
    "                proteins_object_OG = proteins_object_OG_dic[OG_q]\n",
    "                 # covnert to biopython objects\n",
    "                seqRecords_OG = []\n",
    "                for pr in proteins_object_OG:\n",
    "                    species_code = str(pr.genome.uniprot_species_code)\n",
    "                    if species_code in species_oma_kept:\n",
    "                        seq_record = SeqRecord(Seq(pr.sequence),species_code,'','') \n",
    "                        seqRecords_OG.append(seq_record)\n",
    "                \n",
    "                seqRecords_OG_queries =seqRecords_OG + seqRecords_query_edited_all\n",
    "                current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "                #print(current_time, \" - Length of OG\",OG_q,\"was\",len(seqRecords_OG),\",now is\",len(seqRecords_OG_queries))\n",
    "                print(current_time, \" - Combining OG\",OG_q,\" with length of \",len(seqRecords_OG),\"\\t with a query \",len(seqRecords_query_edited_all),\" is just finished.\")\n",
    "\n",
    "                seqRecords_all_filtr.append(seqRecords_OG_queries)\n",
    "\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(\"\\n\", current_time, \"- Combining queries with OG is finished! number of OGs\",len(seqRecords_all_filtr)) # \n",
    "    \n",
    "    \n",
    "    \n",
    "    open_file = open(project_folder+\"fastoma_core/_file_combined_OGs_filtr.pkl\", \"wb\")\n",
    "    pickle.dump(seqRecords_all_filtr, open_file)\n",
    "    open_file.close()\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(\"\\n\", current_time, \"- The variable seqRecords_all_filtr is saved as\", project_folder+\"fastoma_core/_file_combined_OGs_filtr.pkl\") # \n",
    "    \n",
    "    return(seqRecords_all_filtr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_msa_OG(seqRecords_OG_queries):\n",
    "    ############## MSA  ##############\n",
    "    ##################################\n",
    "    #current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    #print(current_time, \"- working on new OG with length of \",len(seqRecords_OG_queries))\n",
    "\n",
    "    wrapper_mafft = mafft.Mafft(seqRecords_OG_queries,datatype=\"PROTEIN\") \n",
    "    # MAfft error: Alphabet 'U' is unknown. -> add --anysymbol argument needed to define in the sourse code\n",
    "    # workaround sed \"s/U/X/g\"\n",
    "    \n",
    "    wrapper_mafft.options.options['--retree'].set_value(1)\n",
    "\n",
    "\n",
    "    run_mafft = wrapper_mafft() # it's wrapper  storing the result  and time \n",
    "    time_taken_mafft = wrapper_mafft.elapsed_time\n",
    "\n",
    "    result_mafft = wrapper_mafft.result \n",
    "    time_taken_mafft2 = wrapper_mafft.elapsed_time\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    #print(current_time,\"- time elapsed for MSA: \",time_taken_mafft2)\n",
    "    print(current_time,\"- MSA for an OG is just finished: \",time_taken_mafft2)\n",
    "\n",
    "    return(result_mafft)\n",
    "   \n",
    "\n",
    "\n",
    "def run_msa_OG_parallel(seqRecords_all,number_max_workers):\n",
    "        \n",
    "    iterotr_OGs = 0 \n",
    "    \n",
    "    result_mafft_all_species=[]\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Parallel msa is started for \",len(seqRecords_all),\" OGs.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=number_max_workers) as executor: \n",
    "        for seqRecords_OG_queries, output_values in zip(seqRecords_all, executor.map(run_msa_OG, seqRecords_all)):\n",
    "            result_mafft_all_species.append(output_values)\n",
    "\n",
    "            \n",
    "    \n",
    "    open_file = open(project_folder+\"fastoma_core/_file_msas.pkl\", \"wb\")\n",
    "    pickle.dump(result_mafft_all_species, open_file)\n",
    "    open_file.close()\n",
    "    \n",
    "    return result_mafft_all_species\n",
    "    \n",
    "\n",
    "def filter_ogs(result_mafft_all_species,ogs_keep_number):\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Filtering MSA started.\")\n",
    "\n",
    "    density_ogs=[]\n",
    "    for msa_og in result_mafft_all_species:\n",
    "\n",
    "        gap_count_og= 0\n",
    "        all_count_og=0\n",
    "        for record in msa_og:\n",
    "            seq=str(record.seq)\n",
    "            gap_count_og += seq.count(\"-\") + seq.count(\"?\") + seq.count(\".\") +seq.count(\"~\")    \n",
    "        #gap_count_ogs.append(gap_count_og)\n",
    "        density_og=gap_count_og/(len(msa_og)*len(msa_og[0]))\n",
    "        density_ogs.append(density_og)\n",
    "        #if density_ogs> treshold_density:\n",
    "    plt.hist(density_ogs,bins=100) # , bins=10\n",
    "    #plt.show()\n",
    "    plt.savefig(project_folder+\"fastoma_core/_density_ogs.pdf\")\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- The histogram of density ogs is saved.\")\n",
    "    \n",
    "    id_og_keep = np.argsort(density_ogs)[-ogs_keep_number:]\n",
    "    result_mafft_all_species_filtr = [result_mafft_all_species[i] for i in id_og_keep]\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Filtering MSA finished, keeping\",len(result_mafft_all_species_filtr),\"out of\",len(result_mafft_all_species))       \n",
    "    \n",
    "\n",
    "    open_file = open(project_folder+\"fastoma_core/_\"+str(ogs_keep_number)+\"_file_msas_filtered.pkl\", \"wb\")\n",
    "    pickle.dump(result_mafft_all_species_filtr, open_file)\n",
    "    open_file.close()\n",
    "\n",
    "        \n",
    "    return result_mafft_all_species_filtr\n",
    "\n",
    "\n",
    "\n",
    "def concatante_alignments(result_mafft_all_species,ogs_keep_number):\n",
    "    ############## Concatante alignments  ##############\n",
    "    ####################################################\n",
    "\n",
    "    #alignments= result_maf2_all\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- MSA concatenation started\")\n",
    "    \n",
    "    \n",
    "    alignments= result_mafft_all_species\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Alignments len\",len(alignments))\n",
    "    #print([len(aln) for aln in alignments ])\n",
    "    #print([len(seq) for aln in alignments for seq in aln])\n",
    "\n",
    "    all_labels_raw = [seq.id for aln in alignments for seq in aln]\n",
    "    all_labels = set(all_labels_raw)\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- ids: \",len(all_labels),len(all_labels_raw))\n",
    "    \n",
    "    # Make a dictionary to store info as we go along\n",
    "    # (defaultdict is convenient -- asking for a missing key gives back an empty list)\n",
    "    concat_buf = defaultdict(list)\n",
    "\n",
    "    # Assume all alignments have same alphabet\n",
    "    alphabet = alignments[0]._alphabet\n",
    "\n",
    "    for aln in alignments:\n",
    "        length = aln.get_alignment_length()\n",
    "        #print(\"length\",length)\n",
    "        # check if any labels are missing in the current alignment\n",
    "        these_labels = set(rec.id for rec in aln)\n",
    "        missing = all_labels - these_labels\n",
    "        #print(missing)\n",
    "        # if any are missing, create unknown data of the right length,\n",
    "        # stuff the string representation into the concat_buf dict\n",
    "        for label in missing:\n",
    "            new_seq = UnknownSeq(length, alphabet=alphabet)\n",
    "            concat_buf[label].append(str(new_seq))\n",
    "\n",
    "        # else stuff the string representation into the concat_buf dict\n",
    "        for rec in aln:\n",
    "            concat_buf[rec.id].append(str(rec.seq))\n",
    "\n",
    "    # Stitch all the substrings together using join (most efficient way),\n",
    "    # and build the Biopython data structures Seq, SeqRecord and MultipleSeqAlignment\n",
    "    msa = MultipleSeqAlignment(SeqRecord(Seq(''.join(seq_arr), alphabet=alphabet), id=label)\n",
    "                                for (label, seq_arr) in concat_buf.items())\n",
    "\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- MSA concatenation finished\")\n",
    "\n",
    "    ##########\n",
    "    # print the borders of concatatnion  in nexus format\n",
    "    # http://www.iqtree.org/doc/Advanced-Tutorial\n",
    "    #######\n",
    "    out_name_msa=project_folder+\"fastoma_core/_\"+str(ogs_keep_number)+\"_msa_concatanated.txt\"\n",
    "    handle_msa_fasta = open(out_name_msa,\"w\")\n",
    "    SeqIO.write(msa, handle_msa_fasta,\"fasta\")\n",
    "    handle_msa_fasta.close()\n",
    "\n",
    "\n",
    "    # sequences_all=[]\n",
    "    # for spec, sequences in concat_buf.items():\n",
    "    #      sequences_all.append(sequences)\n",
    "    # for i in range(1,100):\n",
    "    #     print(len(sequences_all[0][i]),len(sequences_all[2][i]) ,len(sequences_all[40][i]) )\n",
    "    ## therefore, all of them are with same length of sequences\n",
    "\n",
    "    species_0=list(concat_buf.keys())[0]\n",
    "    sequences =  concat_buf[species_0]\n",
    "    len_all = [len(i) for i in sequences] \n",
    "#     borders_in_supermatrix=[]\n",
    "#     borders_in_supermatrix.append((1,len_all[0] )) \n",
    "#     accumalte = len_all[0]\n",
    "#     for i in range(1,100):\n",
    "#         borders_in_supermatrix.append((accumalte+1,accumalte+len_all[i]))\n",
    "#         accumalte +=len_all[i]\n",
    "\n",
    "#     print(len(msa),len(msa[0]),accumalte)\n",
    "#     #check it works or not\n",
    "#     # for i in range(100):\n",
    "#     #     print(borders_in_supermatrix[i][1]-borders_in_supermatrix[i][0]+1,len_all[i])\n",
    "\n",
    "#     #borders_in_supermatrix\n",
    "    \n",
    "#     # begin sets;\n",
    "#     #     charset part1 = 1-100;\n",
    "#     #     charset part2 = 101-384;\n",
    "#     #     charpartition mine = HKY+G:part1, GTR+I+G:part2;\n",
    "#     # end;\n",
    "#     borders_out_file_add=project_folder+\"border_out_parition.nex\"\n",
    "#     borders_out_file = open(borders_out_file_add,'w')\n",
    "\n",
    "#     borders_out_file.write(\"#nexus\"+\"\\n\")\n",
    "#     borders_out_file.write(\"begin sets;\"+\"\\n\")\n",
    "\n",
    "#     for i, val in enumerate(borders_in_supermatrix):\n",
    "\n",
    "#         line=\"    charset part\"+str(i)+\" = \"+str(val[0])+\"-\"+str(val[1])+\";\"\n",
    "#         #print(line)\n",
    "#         borders_out_file.write(line+'\\n')\n",
    "\n",
    "#     borders_out_file.write(\"end;\\n\")\n",
    "#     borders_out_file.close()\n",
    "#     print(borders_out_file_add)\n",
    "    \n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- MSA concatenation has been written in the file\",out_name_msa,\"with length\", len(msa),msa.get_alignment_length()) # super matrix size\n",
    "    \n",
    "    \n",
    "    return msa\n",
    "    \n",
    "\n",
    "def msa_filter_row(msa,tresh_ratio_gap_row,query_species_names,ogs_keep_number):\n",
    "\n",
    "    msa_filtered_row = [] # msa_fltr\n",
    "    ratio_records=[]\n",
    "    for record in msa:\n",
    "        species_name = record.id\n",
    "        seq = record.seq\n",
    "        seqLen = len(record)\n",
    "        \n",
    "        gap_count=seq.count(\"-\") + seq.count(\"?\") + seq.count(\".\") +seq.count(\"~\")\n",
    "                \n",
    "        ratio_record_nongap= 1-gap_count/seqLen\n",
    "        ratio_records.append(round(ratio_record_nongap,3))\n",
    "\n",
    "        if ratio_record_nongap > tresh_ratio_gap_row:\n",
    "            msa_filtered_row.append(record)\n",
    "        elif species_name in query_species_names : \n",
    "            msa_filtered_row.append(record)\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(current_time, \"- Many row-wise gap for query\",species_name,\"with a ratio of\",ratio_record_nongap) \n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "    print(current_time, \"- Row-wise filtering of MSA is finished.\") \n",
    "    print(current_time, \"- Out of \",len(msa),\"species,\",len(msa_filtered_row),\"species (row of msa) remained.\")\n",
    "\n",
    "    out_name_msa=project_folder+\"fastoma_core_\"+str(ogs_keep_number)+\"_msa_concatanated_filtered_row_\"+str(tresh_ratio_gap_row)+\".txt\"\n",
    "    handle_msa_fasta = open(out_name_msa,\"w\")\n",
    "    SeqIO.write(msa_filtered_row, handle_msa_fasta,\"fasta\")\n",
    "    handle_msa_fasta.close()\n",
    "    \n",
    "    print(current_time, \"- MSA Row-wise filtered stored in file.\") # super matrix size\n",
    "    \n",
    "    \n",
    "    return msa_filtered_row\n",
    "    \n",
    "    \n",
    "\n",
    "def msa_filter_col(msa_filtered_row, tresh_ratio_gap_col,tresh_ratio_gap_row,ogs_keep_number):\n",
    "\n",
    "    ratio_col_all = []\n",
    "\n",
    "    length_record= len(msa_filtered_row[1])\n",
    "    num_records = len(msa_filtered_row)\n",
    "\n",
    "\n",
    "    keep_cols = []\n",
    "    for col_i in range(length_record):  # inspired by https://github.com/andreas-wilm/compbio-utils/blob/master/prune_aln_cols.py \n",
    "\n",
    "        col_values = [record.seq[col_i] for record in msa_filtered_row]\n",
    "\n",
    "        gap_count=col_values.count(\"-\") + col_values.count(\"?\") + col_values.count(\".\") +col_values.count(\"~\")\n",
    "\n",
    "        ratio_col_nongap = 1- gap_count/num_records\n",
    "        ratio_col_all.append(ratio_col_nongap)\n",
    "        if ratio_col_nongap  > tresh_ratio_gap_col:\n",
    "            keep_cols.append(col_i)\n",
    "\n",
    "\n",
    "    plt.hist(ratio_col_all,bins=100) # , bins=10\n",
    "    #plt.show()\n",
    "    plt.savefig(project_folder+\"fastoma_core/_ratio_col.pdf\")\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Columns indecis extracted. Out of \", length_record,\"columns,\",len(keep_cols),\"is remained.\") \n",
    "\n",
    "    msa_filtered_row_col = []\n",
    "\n",
    "    for record in msa_filtered_row :\n",
    "        record_seq = str(record.seq)\n",
    "\n",
    "        record_seq_edited  = ''.join([record_seq[i] for i in keep_cols  ])\n",
    "        record_edited= SeqRecord(Seq(record_seq_edited), record.id, '', '')\n",
    "        msa_filtered_row_col.append(record_edited)                         \n",
    "\n",
    "\n",
    "    out_name_msa=project_folder+\"fastoma_core/_\"+str(ogs_keep_number)+\"_msa_concatanated_filtered_row_\"+str(tresh_ratio_gap_row)+\"_col_\"+str(tresh_ratio_gap_col)+\".txt\"\n",
    "    handle_msa_fasta = open(out_name_msa,\"w\")\n",
    "    SeqIO.write(msa_filtered_row_col, handle_msa_fasta,\"fasta\")\n",
    "    handle_msa_fasta.close()\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Column-wise filtering of MSA is finished\",len(msa_filtered_row_col),len(msa_filtered_row_col[0])) \n",
    "       \n",
    "    #msa_filtered_row_col = MultipleSeqAlignment(msa_filtered_row_col)\n",
    "    return msa_filtered_row_col\n",
    "\n",
    "\n",
    "def draw_tree(msa):\n",
    "    ############## Tree inference  ###################\n",
    "    ##################################################\n",
    "\n",
    "    wrapper_tree=fasttree.Fasttree(msa,datatype=\"PROTEIN\")\n",
    "    wrapper_tree.options.options['-fastest']    \n",
    "    result_tree1 = wrapper_tree()\n",
    "\n",
    "    time_taken_tree = wrapper_tree.elapsed_time \n",
    "    time_taken_tree\n",
    "\n",
    "    result_tree2 = wrapper_tree.result\n",
    "    tree_nwk=str(result_tree2[\"tree\"])\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- \",len(tree_nwk))\n",
    "\n",
    "    out_name_tree=project_folder+\"fastoma_core/_tree.txt\"\n",
    "    file1 = open(out_name_tree,\"w\")\n",
    "    file1.write(tree_nwk)\n",
    "    file1.close() \n",
    "    return tree_nwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c023767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "import numpy as np\n",
    "import pyoma.browser.db as db\n",
    "import concurrent.futures\n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "import zoo.wrappers.aligners.mafft as mafft  # mafft should be installed beforehand\n",
    "# import zoo.wrappers.treebuilders.fasttree as fasttree\n",
    "\n",
    "from datetime import datetime\n",
    "from sys import argv\n",
    "from os import listdir\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq, UnknownSeq\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import matplotlib   #for development \n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pyoma.browser.db as db\n",
    "from pyoma.browser.hoghelper import build_hog_to_og_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f67b4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:31:15 - OMA data is parsed and its release name is: OmaStandalone; 1.0.x\n",
      "10:31:15 - There are 10 species in the OMA database.\n",
      "OMA data is parsed and its release name is: OmaStandalone; 1.0.x\n",
      "There are 10 species in the OMA database.\n",
      "HOG-OG map is extracted with length of  38795\n",
      "HOG-OG map is written in the file /work/FAC/FBM/DBC/cdessim2/default/smajidi1/fastoma/mammal/mml27/omamer_database/oma_path/OmaServer.h5_hog_og_map.dic\n",
      "10:31:28 - The are 25 species in the proteome folder.\n",
      "10:31:28 - No splicing file found for these species:  ['CERAT', 'MACNE_', 'OTOGA_', 'CHLSB', 'PAPAN', 'GORGO_', 'PANPA', 'BOVIN_', 'MOUSE_', 'MACFA', 'MANLE_', 'MICMU', 'RHIBE', 'PANTR_', 'CARSF_', 'AOTNA', 'PONAB', 'SAIBB_', 'RHIRO', 'HUMAN', 'MACMU', 'CALJA', 'COLAP_', 'NOMLE']\n",
      "10:31:29 - There are  25  in hogmap folder.\n",
      "10:31:29 - The first species CERAT  contains  46067  proteins.\n",
      "10:31:29 - The first protein of first species is  NOMLE00001\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # a global variable\n",
    "    project_folder = \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/fastoma/mammal/mml27/\"\n",
    "    \n",
    "    oma_database_address = project_folder+\"omamer_database/oma_path/OmaServer.h5\"\n",
    "        \n",
    "    (oma_db, list_speices) = parse_oma_db(oma_database_address)\n",
    "\n",
    "    ## if you already  have the hog-og-map, parse it: \n",
    "    # hog_og_map_address=oma_database_address+\"_hog_og_map.dic\"    \n",
    "    # (hog_OG_map)= parse_hog_og_map(hog_og_map_address)\n",
    "\n",
    "    ## if you you don't have the hog-og-map, run this: \n",
    "    hog_OG_map = HOG_OG_map(oma_database_address)\n",
    "    \n",
    "    \n",
    "    \n",
    "    (query_species_names, query_prot_records_species) = parse_proteome(list_speices)   \n",
    "    \n",
    "    \n",
    "    # we assume that the transcript file is in order. e.g. [PROCO00019,PROCO00020] is with the same order in the proteom file and hogmap file.\n",
    "    splices_list_species = parse_splice_file(query_species_names)\n",
    "\n",
    "    #(query_prot_names_species_mapped, query_hogids_species) = parse_hogmap_omamer(query_species_names) \n",
    "    (query_prot_names_species_mapped, query_hogids_species,query_prot_subscore_species , query_prot_seqlen_species, query_prot_submedseqlen_species) = parse_hogmap_omamer(query_species_names) \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7116451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c46ce43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6118fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d70ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "    query_prot_records_species_filtered=  filter_prot_mapped(query_species_names, query_prot_records_species, query_prot_names_species_mapped)\n",
    "\n",
    "    (query_hogids_filtr_species, query_prot_names_filtr_species, query_prot_records_filtr_species) = extract_unique_hog_pure(query_species_names,query_hogids_species, query_prot_names_species_mapped, query_prot_records_species_filtered) \n",
    "\n",
    "    OGs_queries = gather_OG(query_species_names, query_hogids_filtr_species, query_prot_names_filtr_species, query_prot_records_filtr_species)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b9fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d95af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    keep_og_treshold_species_query =  int(0.95* len(query_species_names))\n",
    "    print(keep_og_treshold_species_query)\n",
    "    OGs_queries_filtr= filter_gathered_OG(OGs_queries, query_species_names,keep_og_treshold_species_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a699e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    threshold_least_query_sepecies_in_OG =  keep_og_treshold_species_query# 15\n",
    "    \n",
    "    kept_oma_species_num = 10 # 20\n",
    "    \n",
    "    seqRecords_all_filtr = combine_OG_query_filtr(OGs_queries_filtr, oma_db, threshold_least_query_sepecies_in_OG,kept_oma_species_num)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517196f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f018039b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ad9f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " \n",
    "    number_max_workers = 3 # round(int(argv[3])*1.3)\n",
    "    result_mafft_all_species = run_msa_OG_parallel(seqRecords_all_filtr,number_max_workers)\n",
    "\n",
    "    \n",
    "    all_id=[]\n",
    "    for a in result_mafft_all_species:\n",
    "        for i in a:\n",
    "            all_id.append(i.id)\n",
    "    len(all_id),len(set(all_id))\n",
    "\n",
    "    list_removed_species=[]\n",
    "    for i in query_species_names:\n",
    "        if i not in set(all_id):\n",
    "            list_removed_species.append(i)\n",
    "\n",
    "    if len(list_removed_species):\n",
    "        print(\"These species are removed from the input proteome:\",list_removed_species)\n",
    "    else:\n",
    "        print(\"All input species are included in the MSA.\")\n",
    "\n",
    "    list_added_species=[]\n",
    "    for i in set(all_id):\n",
    "        if i not in query_species_names:\n",
    "            list_added_species.append(i)        \n",
    "    if len(list_removed_species):\n",
    "        print(\"These species are added from OMA database:\",list_added_species)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    ogs_keep_number = 100\n",
    "    result_mafft_all_species_filtr = filter_ogs(result_mafft_all_species,ogs_keep_number)\n",
    "    msa= concatante_alignments(result_mafft_all_species_filtr, ogs_keep_number)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec23946",
   "metadata": {},
   "source": [
    "## Load pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92625d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=\"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/fastoma/mammal/mml_v4/omamer_v4_search/fastoma_out/\"\n",
    "# a+= \"_file_msas.pkl\"\n",
    "# open_file = open(a, \"rb\") #project_folder+\"_file_msas2.pkl\"\n",
    "# result_mafft_all_species = pickle.load(open_file)\n",
    "# open_file.close()\n",
    "# print(\"seq read is loaded\", len(result_mafft_all_species))\n",
    "\n",
    "# result_mafft_all_species[0][:]\n",
    "# ogs_keep_number = 82\n",
    "# result_mafft_all_species_filtr = filter_ogs(result_mafft_all_species,ogs_keep_number)\n",
    "# msa= concatante_alignments(result_mafft_all_species_filtr, ogs_keep_number)\n",
    "\n",
    "\n",
    "# for record in msa:\n",
    "#     if record.id ==\"S0003\":\n",
    "#         a= record.seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df8aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e795a95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84925a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36432298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4a2790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e4932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reading pkl file \n",
    "# open_file = open(project_folder+\"_file_msas2.pkl\", \"rb\")\n",
    "# result_mafft_all_species = pickle.load(open_file)\n",
    "# open_file.close()\n",
    "# print(\"seq read is loaded\", len(seqRecords_all))\n",
    "\n",
    "#print(current_time, \"- Row-wise filtering of MSA is started.\")        \n",
    "#tresh_ratio_gap_row = 0.3\n",
    "#msa_filtered_row = msa_filter_row(msa,tresh_ratio_gap_row,query_species_names,ogs_keep_number)\n",
    "#print(current_time, \"- Column-wise filtering of MSA is started.\") \n",
    "#tresh_ratio_gap_col = 0.3\n",
    "#msa_filtered_row_col=  msa_filter_col(msa_filtered_row, tresh_ratio_gap_col,tresh_ratio_gap_row,ogs_keep_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178fcdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06c87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69c9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99382e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oma_database_address = \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/fastoma/mammal/mml_v4/omamer_v4_database/oma_path_/OmaServer.h5\"\n",
    "#oma_db = db.Database(oma_database_address+\"\")\n",
    "\n",
    "#oma_db.tax.as_dict()\n",
    "#oma_db.tax.genomes.values()\n",
    "#oma_db.tax.newick()\n",
    "#import pyoma.browser.models as models\n",
    "# models.Genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc3ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse splice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a46d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_species_names[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5603c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(query_prot_names_species_mapped, query_hogids_species, query_prot_seqlen_species, query_prot_submedseqlen_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f835b473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7983d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af77936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d6ab68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e7901e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7674, ['PROCO00009', 'PROCO00010', 'PROCO00011', 'PROCO00012'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splices_list_species[18]),splices_list_species[18][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "562f7fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splices_list_species[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_prot_records_species_filtered=  filter_prot_mapped(query_species_names, query_prot_records_species, query_prot_names_species_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea76eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_prot_mapped(query_species_names, query_prot_records_species,query_prot_names_species_mapped):\n",
    "    # omamer remove very small proteins, \n",
    "    # so  we lose track of order comparing hogmap and fasta file\n",
    "    # the goal here is to remove those from seq record (of the fasta file)\n",
    "    query_prot_records_species_filtered=[]\n",
    "    for species_idx in range(len(query_species_names)):    \n",
    "        # from fasta file\n",
    "        query_species_name=query_species_names[species_idx]\n",
    "        query_prot_records_species_i = query_prot_records_species[species_idx]\n",
    "        query_prot_ids_records = [record.id for record in query_prot_records_species_i]\n",
    "\n",
    "        # from hogmap file\n",
    "        # without proteins that are not mapped on any hogs\n",
    "        query_prot_names_species_i = query_prot_names_species_mapped[species_idx]\n",
    "\n",
    "        if len(query_prot_names_species_i) != len(query_prot_records_species_i):\n",
    "\n",
    "            query_prot_records_filterd=[]\n",
    "            for query_prot_name in query_prot_names_species_i:\n",
    "                if query_prot_name in query_prot_ids_records:\n",
    "                    prot_record_idx = query_prot_ids_records.index(query_prot_name)\n",
    "                    prot_record = query_prot_records_species_i[prot_record_idx]\n",
    "                    query_prot_records_filterd.append(prot_record)\n",
    "                else:\n",
    "                    current_time = datetime.now().strftime(\"%H:%M:%S\")                    \n",
    "                    print(current_time,\"- Error\",query_species_name, query_prot_name)\n",
    "\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")        \n",
    "            print(current_time,\"- For the species\", query_species_name, \", few proteins were ignored by omamer.\")\n",
    "            print(current_time,\"- before filtering: in hogmap\", len(query_prot_names_species_i), \"in proteome\", len(query_prot_records_species_i))\n",
    "            print(current_time,\"- After filtering:  in hogmap\", len(query_prot_names_species_i), \"in proteome\", len(query_prot_records_filterd))            \n",
    "            \n",
    "\n",
    "        else:\n",
    "            query_prot_records_filterd = query_prot_records_species_i\n",
    "\n",
    "        query_prot_records_species_filtered.append(query_prot_records_filterd)\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")        \n",
    "    print(current_time,\"- For the rest of species, all proteins were mapped using OMAmer.\")\n",
    "\n",
    "    return query_prot_records_species_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19468f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(query_prot_names_species_mapped, query_hogids_species, query_prot_seqlen_species, query_prot_submedseqlen_species) = parse_hogmap_omamer(query_species_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f92927e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32202, 32202, 32202, 32202, 32202)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=18\n",
    "query_hogids = query_hogids_species[i]\n",
    "query_prot_seqlen = query_prot_seqlen_species[i]\n",
    "query_prot_submedseqlen  = query_prot_submedseqlen_species[i]\n",
    "\n",
    "query_prot_records= query_prot_records_species[i]\n",
    "query_prot_names_mapped= query_prot_names_species_mapped[i]\n",
    "#query_species_names\n",
    "splices_list = splices_list_species[i]\n",
    "query_prot_subscore= query_prot_subscore_species[i]\n",
    "\n",
    "len(query_hogids), len(query_prot_seqlen), len(query_prot_submedseqlen), len(query_prot_records), len(query_prot_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3eb3a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7674"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(splices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0522944f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21951\n"
     ]
    }
   ],
   "source": [
    "all1 = []\n",
    "for i in splices_list:\n",
    "    all1 += i\n",
    "print(len(all1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "38821bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "previous_idx_prt=0\n",
    "\n",
    "\n",
    "# for small backbone, it should be smaller to have enough gene markers\n",
    "subfamily_score_tresh= 0.5  # output of hog\n",
    "\n",
    "##  we assume that there are at most 50 transcripts for each protein\n",
    "max_num_transcript_each_protein=50\n",
    "\n",
    "best_splice_protid_all= []\n",
    "for splices in splices_list:#[:20]:\n",
    "    \n",
    "    diff_len_splices = []\n",
    "    for prot_id in splices:\n",
    "        ##  we assume that the transcript file is in order. e.g. [PROCO00019,PROCO00020] is with the same order in the proteom file and hogmap file.\n",
    "        \n",
    "        query_prot_names_mapped_slice = query_prot_names_mapped[previous_idx_prt : previous_idx_prt+max_num_transcript_each_protein]\n",
    "        idx_prt = query_prot_names_mapped_slice.index(prot_id) + previous_idx_prt\n",
    "        previous_idx_prt = idx_prt\n",
    "\n",
    "        query_hogid=query_hogids[idx_prt]\n",
    "        query_prot_subscore =query_prot_subscore[idx_prt]\n",
    "        \n",
    "        #print(prot_id,query_hogids[idx_prt],query_prot_subscore[idx_prt], query_prot_seqlen[idx_prt],query_prot_submedseqlen[idx_prt])\n",
    "        if query_prot_subscore[idx_prt] == \"na\":\n",
    "            continue # ignore the rest of the code in the for loop\n",
    "        \n",
    "        if float(query_prot_subscore[idx_prt]) > subfamily_score_tresh:\n",
    "            diff_len = int(query_prot_seqlen[idx_prt])-int(query_prot_submedseqlen[idx_prt])\n",
    "            diff_len_splices.append(abs(diff_len))\n",
    "    if len(diff_len_splices):\n",
    "        best_splice_protid = splices[diff_len_splices.index(min(diff_len_splices))]\n",
    "        #print(best_splice_protid)\n",
    "        best_splice_protid_all.append(best_splice_protid)\n",
    "        best_splice_hogid_all.append(query_hogids)\n",
    "    #print(splices,diff_len_splices)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3cb04217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32202, 6630)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_prot_names_mapped),len(best_splice_protid_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d228f175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROCO00001',\n",
       " 'PROCO00002',\n",
       " 'PROCO00003',\n",
       " 'PROCO00004',\n",
       " 'PROCO00005',\n",
       " 'PROCO00006',\n",
       " 'PROCO00007',\n",
       " 'PROCO00008',\n",
       " 'PROCO00009',\n",
       " 'PROCO00010']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_prot_names_mapped[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "127b00f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prots_spliced = []\n",
    "for splices in splices_list:\n",
    "    prots_spliced += splices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ce14e804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21951"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prots_spliced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99958b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "163788f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32202"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_prot_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "#for species_idx in range(len(query_species_names)):    \n",
    "    # from fasta file\n",
    "    \n",
    "    query_species_name=query_species_names[species_idx]\n",
    "#     \n",
    "#     query_prot_ids_records = [record.id for record in query_prot_records_species_i]\n",
    "\n",
    "#     # from hogmap file\n",
    "#     # without proteins that are not mapped on any hogs\n",
    "#     query_prot_names_species_i = query_prot_names_species_mapped[species_idx]\n",
    "\n",
    "#     if len(query_prot_names_species_i) != len(query_prot_records_species_i):\n",
    "\n",
    "#         query_prot_records_filterd=[]\n",
    "#         for query_prot_name in query_prot_names_species_i:\n",
    "#             if query_prot_name in query_prot_ids_records:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3bc64a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_idx=18\n",
    "query_species_name=query_species_names[species_idx]\n",
    "query_prot_records_species_i = query_prot_records_species[species_idx]\n",
    "\n",
    "query_prot_ids_records = [record.id for record in query_prot_records_species_i]\n",
    "\n",
    "query_prot_names_species_i = query_prot_names_species_mapped[species_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ea2d2c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32202"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_prot_records_species_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b56288a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prot_records_filterd = []\n",
    "for prot_idx in range(len(query_prot_names_species_i[:100])\n",
    "    prot_record = query_prot_records_species_i[prot_idx]\n",
    "    prot_name = prot_record.id\n",
    "                      \n",
    "    if prot_name not in prots_spliced:\n",
    "        \n",
    "        query_prot_records_filterd.append(prot_record)\n",
    "                \n",
    "    elif query_prot_name in best_splice_protid_all:\n",
    "        query_prot_records_filterd.append(prot_record)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bca90a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROCO01000'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b73c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6092cdc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a4f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
