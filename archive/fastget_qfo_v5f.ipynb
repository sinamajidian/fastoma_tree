{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdfefe91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start= 1\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1fe123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d539300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "    \n",
    "# IMPORTNAT  this code accept only .fa file as proteome\n",
    "\n",
    " \n",
    "def parse_oma_db(oma_database_address):\n",
    "    \n",
    "    ############### Parsing OMA db ####################\n",
    "    ###################################################\n",
    "\n",
    "    oma_db = db.Database(oma_database_address)\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- OMA data is parsed and its release name is:\", oma_db.get_release_name())\n",
    "    list_oma_speices = [z.uniprot_species_code for z in oma_db.tax.genomes.values()] \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- There are\",len(list_oma_speices),\"species in the OMA database.\")\n",
    "    \n",
    "    return (oma_db, list_oma_speices)\n",
    "\n",
    "\n",
    "def parse_proteome(list_oma_speices):\n",
    "    \n",
    "    ############### Parsing query proteome of species #######\n",
    "    #########################################################\n",
    "\n",
    "    \n",
    "    # IMPORTNAT  this code accept only .fa file as proteome\n",
    "    project_files = listdir(address_working_folder+\"/omamer_search/proteome/\")\n",
    "\n",
    "    query_species_names = []\n",
    "    for file in project_files:\n",
    "        if file.split(\".\")[-1] == \"fa\":\n",
    "            file_name_split = file.split(\".\")[:-1]\n",
    "            query_species_names.append('.'.join(file_name_split))\n",
    "        if file.split(\".\")[-1] == \"fasta\":\n",
    "            file_name_split = file.split(\".\")[:-1]\n",
    "            query_species_names.append('.'.join(file_name_split))\n",
    "\n",
    "    # we may assert existence of query_species_name+\".fa/hogmap\"\n",
    "    prots_record_allspecies = [ ]\n",
    "    for query_species_name in query_species_names:\n",
    "        prot_address = address_working_folder +\"omamer_search/proteome/\"+ query_species_name + \".fa\" \n",
    "        \n",
    "        prots_record = list(SeqIO.parse(prot_address, \"fasta\")) \n",
    "        prots_record_allspecies.append(prots_record)\n",
    "\n",
    "    query_species_num = len(query_species_names)    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- The are\",str(query_species_num),\"species in the proteome folder.\")\n",
    "\n",
    "    # for development\n",
    "    for species_i in range(query_species_num):\n",
    "        len_prot_record_i = len( prots_record_allspecies[species_i] )\n",
    "        species_name_i = query_species_names[species_i]\n",
    "        #print(species_name_i,len_prot_record_i)\n",
    "        if species_name_i in list_oma_speices: \n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(current_time,\"- the species\",species_name_i,\" already exists in the oma database, remove them first\")\n",
    "            exit()\n",
    "\n",
    "      \n",
    "    # The proteins are parsed using  Bio.SeqIO.parse\n",
    "    # the first part of the header line before space \n",
    "    # >tr|A0A2I3FYY2|A0A2I3FYY2_NOMLE Uncharacterized protein OS=Nomascus leucogenys OX=61853 GN=CLPTM1L PE=3 SV=1\n",
    "    # will be \">tr|A0A2I3FYY2|A0A2I3FYY2_NOMLE\"\n",
    "    # [i.id for i in prots_record_allspecies[0] if len(i.id)!=30 and len(i.id)!=22 ] #'sp|O47892|CYB_NOMLE',\n",
    "    \n",
    "\n",
    "    \n",
    "    return (query_species_names, prots_record_allspecies)\n",
    "\n",
    "\n",
    "def parse_hogmap_omamer(query_species_names):\n",
    "\n",
    "    ################### Parsing omamer's output  ########\n",
    "    #####################################################\n",
    "    \n",
    "    prots_hogmap_name_allspecies = []\n",
    "    prots_hogmap_hogid_allspecies = []\n",
    "    prots_hogmap_subfscore_allspecies = []\n",
    "    prots_hogmap_seqlen_allspecies = []\n",
    "    prots_hogmap_subfmedseqlen_allspecies = []\n",
    "\n",
    "    for query_species_name in query_species_names:\n",
    "        omamer_output_address = address_working_folder + \"omamer_search/hogmap/\"+ query_species_name + \".hogmap\"     \n",
    "        omamer_output_file = open(omamer_output_address,'r');\n",
    "        prots_hogmap_name = []\n",
    "        prots_hogmap_hogid = []\n",
    "        prots_hogmap_subfscore = []\n",
    "        prots_hogmap_seqlen = []\n",
    "        prots_hogmap_subfmedseqlen = []\n",
    "        \n",
    "        for line in omamer_output_file:\n",
    "            line_strip=line.strip()\n",
    "            if not line_strip.startswith('qs'):\n",
    "                line_split= line_strip.split(\"\\t\")    \n",
    "                #if line_split[1]!='na':\n",
    "                prots_hogmap_name.append(line_split[0])\n",
    "                prots_hogmap_hogid.append(line_split[1])\n",
    "                prots_hogmap_subfscore.append(line_split[4]) # subfamily\n",
    "                prots_hogmap_seqlen.append(line_split[5])\n",
    "                prots_hogmap_subfmedseqlen.append(line_split[6])\n",
    "                \n",
    "        prots_hogmap_name_allspecies.append(prots_hogmap_name)\n",
    "        prots_hogmap_hogid_allspecies.append(prots_hogmap_hogid)\n",
    "        prots_hogmap_subfscore_allspecies.append(prots_hogmap_subfscore)\n",
    "        prots_hogmap_seqlen_allspecies.append(prots_hogmap_seqlen)\n",
    "        prots_hogmap_subfmedseqlen_allspecies.append(prots_hogmap_subfmedseqlen)\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- There are \",len(prots_hogmap_name_allspecies),\" species in the hogmap folder.\")\n",
    "    print(current_time,\"- The first species\",query_species_names[0],\" contains \",len(prots_hogmap_hogid_allspecies[0]),\" proteins.\")\n",
    "    print(current_time,\"- The first protein of first species is \", prots_hogmap_name_allspecies[0][0])\n",
    "\n",
    "    hogmap_allspecies = (prots_hogmap_name_allspecies, prots_hogmap_hogid_allspecies, prots_hogmap_subfscore_allspecies, prots_hogmap_seqlen_allspecies, prots_hogmap_subfmedseqlen_allspecies)\n",
    "    return  hogmap_allspecies\n",
    "    \n",
    "    \n",
    "    \n",
    "def filter_prot_mapped(query_species_names, query_prot_records_species,query_prot_names_species_mapped):\n",
    "    # omamer remove very small proteins, \n",
    "    # so  we lose track of order comparing hogmap and fasta file\n",
    "    # the goal here is to remove those from seq record (of the fasta file)\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- Filtering proteins started.\")\n",
    "\n",
    "    query_prot_records_species_filtered=[]\n",
    "    for species_idx in range(len(query_species_names)):    \n",
    "        # from fasta file\n",
    "        query_species_name=query_species_names[species_idx]\n",
    "        print(query_species_name)\n",
    "        query_prot_records_species_i = query_prot_records_species[species_idx]\n",
    "        query_prot_ids_records = [record.id for record in query_prot_records_species_i]\n",
    "\n",
    "        # from hogmap file\n",
    "        # without proteins that are not mapped on any hogs\n",
    "        query_prot_names_species_i = query_prot_names_species_mapped[species_idx]\n",
    "\n",
    "        if len(query_prot_names_species_i) != len(query_prot_records_species_i):\n",
    "\n",
    "            query_prot_records_filterd=[]\n",
    "            for query_prot_name in query_prot_names_species_i:\n",
    "                if query_prot_name in query_prot_ids_records:\n",
    "                    prot_record_idx = query_prot_ids_records.index(query_prot_name)\n",
    "                    prot_record = query_prot_records_species_i[prot_record_idx]\n",
    "                    query_prot_records_filterd.append(prot_record)\n",
    "                else:\n",
    "                    current_time = datetime.now().strftime(\"%H:%M:%S\")                    \n",
    "                    print(current_time,\"- Error\",query_species_name, query_prot_name)\n",
    "\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")        \n",
    "            print(current_time,\"- For the species\", query_species_name, \", few proteins were ignored by omamer.\")\n",
    "            print(current_time,\"- before filtering: in hogmap\", len(query_prot_names_species_i), \"in proteome\", len(query_prot_records_species_i))\n",
    "            print(current_time,\"- After filtering:  in hogmap\", len(query_prot_names_species_i), \"in proteome\", len(query_prot_records_filterd))            \n",
    "            \n",
    "\n",
    "        else:\n",
    "            query_prot_records_filterd = query_prot_records_species_i\n",
    "\n",
    "        query_prot_records_species_filtered.append(query_prot_records_filterd)\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")        \n",
    "    print(current_time,\"- For the rest of species, all proteins were mapped using OMAmer.\")\n",
    "\n",
    "    return query_prot_records_species_filtered\n",
    "\n",
    "\n",
    "def run_one_msa(seqRecords_queries):\n",
    "    ############## MSA  ##############\n",
    "    ##################################\n",
    "    #current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    #print(current_time, \"- working on new OG with length of \",len(seqRecords_OG_queries))\n",
    "    \n",
    "    \n",
    "    wrapper_mafft = mafft.Mafft(seqRecords_queries,datatype=\"PROTEIN\") \n",
    "    # MAfft error: Alphabet 'U' is unknown. -> add --anysymbol argument needed to define in the sourse code\n",
    "    # workaround sed \"s/U/X/g\"\n",
    "    \n",
    "    wrapper_mafft.options.options['--retree'].set_value(1)\n",
    "\n",
    "\n",
    "    run_mafft = wrapper_mafft() # it's wrapper  storing the result  and time \n",
    "    time_taken_mafft = wrapper_mafft.elapsed_time\n",
    "\n",
    "    result_mafft = wrapper_mafft.result \n",
    "    time_taken_mafft2 = wrapper_mafft.elapsed_time\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    #print(current_time,\"- time elapsed for MSA: \",time_taken_mafft2)\n",
    "    #print(current_time,\"- MSA for an OG is just finished: \",time_taken_mafft2)\n",
    "\n",
    "    return(result_mafft)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def draw_tree(msa, tree_out_file):\n",
    "    ############## Tree inference  ###################\n",
    "    ##################################################\n",
    "\n",
    "    wrapper_tree=fasttree.Fasttree(msa,datatype=\"PROTEIN\")\n",
    "    wrapper_tree.options.options['-fastest']    \n",
    "    result_tree1 = wrapper_tree()\n",
    "\n",
    "    time_taken_tree = wrapper_tree.elapsed_time \n",
    "    time_taken_tree\n",
    "\n",
    "    result_tree2 = wrapper_tree.result\n",
    "    tree_nwk=str(result_tree2[\"tree\"])\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    #print(current_time,\"- \",len(tree_nwk))\n",
    "\n",
    "    if len(tree_out_file)>255: tree_out_file = tree_out_file[:255]\n",
    "    file1 = open(tree_out_file,\"w\")\n",
    "    file1.write(tree_nwk)\n",
    "    file1.write(\";\\n\")\n",
    "    file1.close() \n",
    "    return tree_nwk\n",
    "\n",
    "\n",
    "\n",
    "def merge_msa(list_msas):\n",
    "    \n",
    "    #logging.debug(list_msas)\n",
    "    #logging.debug(str(list_msas[0][0].seq)+\"\\n\")\n",
    "    #logging.debug(str(list_msas[0][0].id)+\"\\n\")\n",
    "    #logging.debug(str(list_msas[1][0].seq)+\"\\n\")\n",
    "    #logging.debug(str(list_msas[0][0].id)+\"\\n\")\n",
    "    \n",
    "    # each element of msa should be  a MultipleSeqAlignment\n",
    "    wrapper_mafft_merge = mafft.Mafft(list_msas, datatype=\"PROTEIN\") \n",
    "    wrapper_mafft_merge.options['--merge'].active = True\n",
    "    merged = wrapper_mafft_merge()\n",
    "    \n",
    "    \n",
    "    print(len(list_msas),\"msas are merged into one with the length of \",len(merged),len(merged[0]) )\n",
    "    return merged\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# def compact_distance_matrix_tree(tree_input):\n",
    "\n",
    "#     # tree_input   in ete3 format\n",
    "#     # output  a vector upper triangulare \n",
    "    \n",
    "#     tree_leaves=[]\n",
    "#     for node in tree_input.traverse(strategy=\"postorder\"):\n",
    "#         if node.is_leaf() : \n",
    "#             node_name = node.name\n",
    "#             tree_leaves.append(node_name)\n",
    "\n",
    "\n",
    "#     leaves_num = len(tree_leaves)\n",
    "#     distance_matrix = np.zeros([leaves_num,leaves_num])\n",
    "\n",
    "#     for i in range(leaves_num):\n",
    "#         for j in range(leaves_num):\n",
    "#             if i < j:\n",
    "#                 value= round(tree_input.get_distance(tree_leaves[i],tree_leaves[j]),3)\n",
    "#                 distance_matrix[i][j]= value\n",
    "#                 distance_matrix[j][i]= value\n",
    "\n",
    "#     y=[]\n",
    "#     for i in range(len(distance_matrix)):\n",
    "#         for j in range(len(distance_matrix)):\n",
    "#             if i<j:\n",
    "#                 val= distance_matrix[i][j]\n",
    "#                 y.append(val)\n",
    "#     return (y,tree_leaves)\n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "def lable_SD_internal_nodes(tree_out):\n",
    "\n",
    "    species_name_dic={}\n",
    "    counter_S=0\n",
    "    counter_D=0\n",
    "    \n",
    "    for node in tree_out.traverse(strategy = \"postorder\"):\n",
    "        #print(\"** now working on node \",node.name) # node_children\n",
    "\n",
    "        if node.is_leaf() :\n",
    "            prot_i = node.name\n",
    "            species_name_dic[node] = { str(prot_i).split(\"|\")[-1].split(\"_\")[-1] }\n",
    "\n",
    "        else:\n",
    "            node.name= \"S/D\"\n",
    "            leaves_list = node.get_leaves()\n",
    "            #print(\"leaves_list\", leaves_list)\n",
    "            \n",
    "\n",
    "            species_name_set = set([ str(prot_i).split(\"|\")[-1].split(\"_\")[-1] for prot_i in leaves_list])\n",
    "            #print(\"species_name_set\", species_name_set)\n",
    "            species_name_dic[node] = species_name_set\n",
    "\n",
    "\n",
    "            node_children = node.children\n",
    "            #print(node_children)\n",
    "            node_children_species_list = [species_name_dic[node_child] for node_child in node_children] # list of sets\n",
    "            \n",
    "            #print(\"node_children_species_list\", node_children_species_list)\n",
    "            \n",
    "            node_children_species_intersection = set.intersection(*node_children_species_list)\n",
    "\n",
    "            if  node_children_species_intersection :\n",
    "                #print(\"node_children_species_list\",node_children_species_list)\n",
    "                counter_D += 1\n",
    "                node.name = \"D\"+str(counter_D)\n",
    "                \n",
    "            else:\n",
    "                counter_S += 1\n",
    "                node.name = \"S\"+str(counter_S)\n",
    "\n",
    "    return tree_out\n",
    "\n",
    "def add_species_name(query_prot_records_species,query_species_names):\n",
    "\n",
    "    for ix in range(len(query_species_names)):\n",
    "        query_species_name = query_species_names[ix]\n",
    "        query_prot_records = query_prot_records_species[ix]\n",
    "        for i_prot in range(len(query_prot_records)):\n",
    "            query_prot_record = query_prot_records[i_prot]\n",
    "            query_prot_record.description += \"|species|\"+query_species_name\n",
    "            \n",
    "    return query_prot_records_species\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_omaID_file(omaID_address):\n",
    "    \n",
    "    omaID_file = open(omaID_address,'r')\n",
    "\n",
    "    taxonID_omaID={}\n",
    "    omaID_taxonID={}\n",
    "\n",
    "    # omaID_scienceFull={}\n",
    "    # scienceFull_omaID={}\n",
    "    scientific_taxonID={}\n",
    "    taxonID_scientific={}\n",
    "\n",
    "    #  !!! limitation  ignoring strains and isolate\n",
    "    for line in omaID_file:\n",
    "        line_strip = line.strip()\n",
    "        if line_strip.startswith('#'):\n",
    "            pass\n",
    "            #header_lines_list.append(line_strip)\n",
    "        else:\n",
    "            line_parts = line_strip.split('\\t')\n",
    "\n",
    "            omaID = line_parts[0]\n",
    "            taxonID = line_parts[1]\n",
    "            taxonID_omaID[taxonID] = omaID\n",
    "            omaID_taxonID[omaID] = taxonID\n",
    "\n",
    "            scientific = line_parts[2]\n",
    "            taxonID_scientific[taxonID] = scientific\n",
    "            scientific_taxonID[scientific] = taxonID\n",
    "\n",
    "    omaID_file.close()\n",
    "\n",
    "    print(\"-- The map for OMA taxonID of\",len(taxonID_omaID),\"records have read.\") \n",
    "    \n",
    "    return (taxonID_omaID,omaID_taxonID,taxonID_scientific,scientific_taxonID)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_rootHOGs(prots_hogmap_hogid_allspecies, address_out_hog):\n",
    "\n",
    "    prots_hogmap_rhogid_allspecies = []\n",
    "    for prots_hogmap_hogid in prots_hogmap_hogid_allspecies:\n",
    "        prots_hogmap_rhogid = []\n",
    "        for prot_hogmap_hogid in prots_hogmap_hogid:\n",
    "            prot_hogmap_rhogid=prot_hogmap_hogid.split(\".\")[0]\n",
    "            prots_hogmap_rhogid.append(prot_hogmap_rhogid)\n",
    "\n",
    "        prots_hogmap_rhogid_allspecies.append(prots_hogmap_rhogid)\n",
    "\n",
    "\n",
    "    rhogid_prot_idx_dic = {}\n",
    "    for species_idx in range(len(query_species_names)):\n",
    "\n",
    "        species_name = query_species_names[species_idx]\n",
    "\n",
    "        prots_hogmap_rhogid = prots_hogmap_rhogid_allspecies[species_idx]\n",
    "\n",
    "        for prots_hogmap_idx in range(len(prots_hogmap_rhogid)):\n",
    "\n",
    "            prot_hogmap_rhogid = prots_hogmap_rhogid[prots_hogmap_idx]\n",
    "            if prot_hogmap_rhogid in rhogid_prot_idx_dic:\n",
    "                rhogid_prot_idx_dic[prot_hogmap_rhogid].append((species_idx, prots_hogmap_idx))\n",
    "            else:\n",
    "                rhogid_prot_idx_dic[prot_hogmap_rhogid] = [(species_idx, prots_hogmap_idx)]\n",
    "    print(len(rhogid_prot_idx_dic)) #  rhogid_prot_idx_dic['HOG:0018405']\n",
    "\n",
    "\n",
    "\n",
    "    rhogids_prot_records_query = [ ]\n",
    "    rhogids_list = []\n",
    "    for rhogid in rhogid_prot_idx_dic.keys() :\n",
    "        rhogid_prot_records = []\n",
    "        if rhogid != \"na\" and len(rhogid)>1:\n",
    "            rhogids_list.append(rhogid)\n",
    "            rhogid_prot_idx =  rhogid_prot_idx_dic[rhogid]\n",
    "            for (species_idx, prots_hogmap_idx) in rhogid_prot_idx:\n",
    "                prot_record = query_prot_records_species_filtered[species_idx][prots_hogmap_idx] \n",
    "                #print(prot_record)\n",
    "                rhogid_prot_records.append(prot_record)\n",
    "\n",
    "            rhogids_prot_records_query.append(rhogid_prot_records) \n",
    "        else:\n",
    "            print(\"root hog na / lenght of one \",rhogid)\n",
    "    print(len(rhogids_prot_records_query),len(rhogids_prot_records_query[0]))\n",
    "    \n",
    "    \n",
    "    #rhogids_prot_records = []\n",
    "    rhogid_num_list= []\n",
    "    for rhogid_idx in range(len(rhogids_list)):\n",
    "        #if rhogid_idx %500 ==0 : print(rhogid_idx)\n",
    "        rhogid_prot_records_query= rhogids_prot_records_query[rhogid_idx] \n",
    "\n",
    "        rhogid = rhogids_list[rhogid_idx]\n",
    "\n",
    "        rhogid_B= rhogid.split(\":\")[1]\n",
    "        rhogid_num= int(rhogid_B[1:] ) # # B0613860\n",
    "        rhogid_num_list.append(rhogid_num)\n",
    "        if  len(rhogid_prot_records_query) < 100  and len(rhogid_prot_records_query) > 2 :\n",
    "    #         rhogids_prot_records_oma = []\n",
    "    #         for hog_elements in oma_db.member_of_fam(rhogid_num):   # this gets the member of roothog 2 (HOG:000002)\n",
    "    #             prot_hog_element = ProteinEntry(oma_db, hog_elements)\n",
    "    #             #print(prot_hog_element.omaid, prot_hog_element.hog_family_nr, len(prot_hog_element.sequence),prot_hog_element.sequence[0])\n",
    "    #             rhogids_prot_records_oma.append(SeqRecord(Seq(prot_hog_element.sequence), id=prot_hog_element.omaid))\n",
    "    #         rhogids_prot_records_both= rhogids_prot_records_oma +  rhogid_prot_records_query\n",
    "    #         rhogids_prot_records.append(rhogids_prot_records_both)\n",
    "            SeqIO.write(rhogid_prot_records_query, address_out_hog+\"HOG_\"+str(rhogid_num)+\".fa\", \"fasta\")\n",
    "\n",
    "    print(\"all HOGs   (>2 <100) has written.\",len(rhogids_prot_records_query),len(rhogids_list), len(rhogid_prot_records_query), len(rhogid_prot_records_query[0]))\n",
    "\n",
    "    return (rhogid_num_list, rhogids_prot_records_query)\n",
    "    \n",
    "    \n",
    "def read_species_tree(tree_address):\n",
    "    print(tree_address)\n",
    "    #print(round(os.path.getsize(tree_address)/1000),\"kb\")\n",
    "\n",
    "    project = Phyloxml()\n",
    "    project.build_from_file(tree_address)\n",
    "    # Each tree contains the same methods as a PhyloTree object\n",
    "    for species_tree in project.get_phylogeny():\n",
    "        len(species_tree)\n",
    "        #print(species_tree)\n",
    "\n",
    "    for node_species_tree in species_tree.traverse(strategy = \"postorder\"):\n",
    "        if node_species_tree.is_leaf():\n",
    "            temp1 =node_species_tree.phyloxml_clade.get_taxonomy()[0]\n",
    "            #print(temp1.get_code())     \n",
    "            node_species_tree.name = temp1.get_code()\n",
    "    #print(len(species_tree))\n",
    "    #print(species_tree)\n",
    "    \n",
    "    return (species_tree)   \n",
    "    \n",
    "\n",
    "    \n",
    "def traverse_geneTree_assign_hog(gene_tree, merged_msa):\n",
    "    # gene_tree should be labeled with S/ D\n",
    "    tree_leaves = [ i.name for i in gene_tree.get_leaves()] \n",
    "    assigned_leaves_to_hog = []\n",
    "    sub_msas_list_this_level = []\n",
    "\n",
    "    for node in gene_tree.traverse(strategy = \"preorder\"): # start from root\n",
    "        #print(\"Leaves assigned to hog are \", assigned_leaves_to_hog)\n",
    "        #print(\"Traversing gene tree. Now at node\", node.name)\n",
    "\n",
    "        if node.is_root() and node.name[0] == \"S\":    \n",
    "            sub_msas_list_this_level = [merged_msa]\n",
    "            assigned_leaves_to_hog = tree_leaves\n",
    "            # we do not need to traverse the gene tree any more \n",
    "            break\n",
    "\n",
    "        if not node.is_leaf() : \n",
    "            node_leaves_name = [ i.name for i in node.get_leaves() ] \n",
    "            \n",
    "            # ?? ?? \n",
    "            if node_leaves_name[0] in assigned_leaves_to_hog:\n",
    "                # if one of them is there, since  preorder,  all should be there ???    \n",
    "                # the node are assigned to hogs, we are done. \n",
    "                # it is not needed to check the children of a speciecation\n",
    "                continue  # go to next node\n",
    "\n",
    "            if node.name[0] ==\"S\":\n",
    "                # this is a sub-hog.\n",
    "                assigned_leaves_to_hog += node_leaves_name\n",
    "                leaves_msa = [i.id for i in merged_msa]\n",
    "                idx_species_list = [leaves_msa.index(i) for i in node_leaves_name] \n",
    "                #print(\"idx_species_list\",idx_species_list)\n",
    "                #print(\"node_leaves_name\",node_leaves_name)\n",
    "                \n",
    "                sub_msa_seq_list = [  merged_msa[i]   for i in idx_species_list]  # sub_msas_list_lowerlevel \n",
    "                sub_msa = run_one_msa(sub_msa_seq_list)\n",
    "                sub_msas_list_this_level.append(sub_msa)\n",
    "\n",
    "                if  set(tree_leaves)==set(assigned_leaves_to_hog) :\n",
    "                    # all leaves are assigned to hogs, we are done. \n",
    "                    break\n",
    "\n",
    "        if node.is_leaf() and not node.name in assigned_leaves_to_hog:\n",
    "            #  singletone leaf, can be a hog. \n",
    "            assigned_leaves_to_hog.append(node.name)\n",
    "            #print(\"assigned_leaves_to_hog\",assigned_leaves_to_hog)\n",
    "            leaves_msa = [i.id for i in merged_msa]\n",
    "            #print(\"leaves_msa\",leaves_msa)\n",
    "\n",
    "            idx_species = leaves_msa.index(node.name)\n",
    "            sub_msa = MultipleSeqAlignment([merged_msa[idx_species]])\n",
    "            sub_msas_list_this_level.append(sub_msa)\n",
    "            \n",
    "    return (assigned_leaves_to_hog, sub_msas_list_this_level)\n",
    "            \n",
    "\n",
    "def prepare_species_tree(rhog_i, species_tree):\n",
    "\n",
    "    species_names_rhog= []\n",
    "    prot_names_rhog=[]\n",
    "    for rec in rhog_i:\n",
    "        prot_name= rec.name # # 'tr|E3JPS4|E3JPS4_PUCGT\n",
    "        #prot_name = prot_name_full.split(\"|\")[1].strip() # # 'tr|E3JPS4|E3JPS4_PUCGT\n",
    "        species_name = prot_name.split(\"|\")[-1].split(\"_\")[-1]\n",
    "        if species_name=='RAT': species_name=\"RATNO\"\n",
    "\n",
    "        species_names_rhog.append(species_name)\n",
    "        prot_names_rhog.append(prot_name)\n",
    "\n",
    "    species_names_uniqe = set(species_names_rhog)\n",
    "    print(\"number of unique species in the rHOG\", len(species_names_uniqe))\n",
    "\n",
    "    \n",
    "    species_tree.prune(species_names_uniqe, preserve_branch_length=True )\n",
    "    species_tree.write()\n",
    "\n",
    "    for node in species_tree.traverse(strategy = \"postorder\"):\n",
    "        node_name = node.name\n",
    "        if len(node_name) <1: \n",
    "            if node.is_leaf():\n",
    "                node.name = \"leaf_\"+str(num_leaves_no_name)\n",
    "            else:\n",
    "                node_children = node.children\n",
    "                list_children_names = [node_child.name for node_child in node_children]\n",
    "                node.name = '_'.join(list_children_names)\n",
    "\n",
    "    print(\"Working on the following species tree.\")\n",
    "    #print(species_tree)\n",
    "    \n",
    "    return (species_tree, species_names_rhog, prot_names_rhog)\n",
    "\n",
    "\n",
    "\n",
    "def find_paralog_thisLevel(tree_leaves, subHOG_thisLevel):\n",
    "    \n",
    "    paralog_set_thisLevel = set()\n",
    "    for i in range(len(tree_leaves)):\n",
    "        prot_i = tree_leaves[i]\n",
    "        species_i = prot_i.split(\"|\")[-1].split(\"_\")[-1]\n",
    "        for j in range(i+1):\n",
    "            prot_j = tree_leaves[j]\n",
    "            species_j = prot_j.split(\"|\")[-1].split(\"_\")[-1]\n",
    "            if species_i == species_j:\n",
    "                paralog_set_thisLevel.add((prot_i,prot_j))\n",
    "                paralog_set_thisLevel.add((prot_j,prot_i))\n",
    "\n",
    "    for i in range(len(subHOG_thisLevel)):\n",
    "        subHOG_i = subHOG_thisLevel[i]\n",
    "        for j in range(i):\n",
    "            subHOG_j = subHOG_thisLevel[j]\n",
    "            for i1 in range(len(subHOG_i)):\n",
    "                prot_i = subHOG_i[i1]\n",
    "                for j1 in range(len(subHOG_j)):\n",
    "                    prot_j = subHOG_j[j1]\n",
    "                    paralog_set_thisLevel.add((prot_i,prot_j))\n",
    "                    paralog_set_thisLevel.add((prot_j,prot_i))\n",
    " \n",
    "    return paralog_set_thisLevel\n",
    "\n",
    "\n",
    "\n",
    "def infer_HOG_thisLevel(node_species_tree, rhog_i, species_names_rhog, dic_sub_msas, rhogid_num):\n",
    "\n",
    "    sub_msa_list_lowerLevel = [] # including subHOGS of lower level \n",
    "    for node_child in node_species_tree.children:\n",
    "        if  node_child.is_leaf():\n",
    "            node_species_name = node_child.name\n",
    "            # for a extant species \n",
    "            # extracting prot sequeincg of the species from the rootHOG\n",
    "            interest_list = [idx  for idx in range(len(species_names_rhog)) if species_names_rhog[idx] == node_species_name ]\n",
    "            rhog_part = [rhog_i[i] for i in interest_list]\n",
    "            sub_msa = [MultipleSeqAlignment([i]) for i in rhog_part] \n",
    "        else:   # the child node is an internal node, subHOGs are inferred till now in traversing.\n",
    "            print(\"sub msa for internal node\", node_child.name,\"is read from dic.\")\n",
    "            if node_child.name in dic_sub_msas:\n",
    "                sub_msa  = dic_sub_msas[node_child.name]\n",
    "            else:\n",
    "                print(\"error 131, no sub msa for the internal node \",node_child.name, node_child)\n",
    "                assert 2==1 \n",
    "        sub_msa_list_lowerLevel += sub_msa\n",
    "\n",
    "\n",
    "    if len(sub_msa_list_lowerLevel) <2:\n",
    "        print(\"**** issue **  \", len(sub_msa_list_lowerLevel),sub_msa_list_lowerLevel)\n",
    "        return (-1,-1,-1)\n",
    "    \n",
    "    print(\"There are\",len(sub_msa_list_lowerLevel),\" subHOGs in the lower level.\")\n",
    "    #print(\"We want to infer subHOGs at this level,i.e. merge few of them.\")    \n",
    "    #time.sleep(1)     # shall I wait to have all  msa run finsihed? \n",
    "    \n",
    "    #all hog of child\n",
    "    #get msa\n",
    "    \n",
    "    merged_msa = merge_msa(sub_msa_list_lowerLevel) \n",
    "\n",
    "    print(\"All subHOGs are merged, merged msa is with length of\",len(merged_msa), len(merged_msa[0]),\".\")\n",
    "\n",
    "    gene_tree_file =  address_working_folder + \"/gene_trees_test/tree_\"+str(rhogid_num)+\"_\"+str(node_species_tree.name)+\".nwk\"\n",
    "    \n",
    "    gene_tree_raw = draw_tree(merged_msa, gene_tree_file)\n",
    "    gene_tree = Tree(gene_tree_raw+\";\", format=0)\n",
    "    print(\"Gene tree is infered with length of\",len(gene_tree),\".\")\n",
    "\n",
    "    #gene_tree_i +=1\n",
    "    R = gene_tree.get_midpoint_outgroup()\n",
    "    gene_tree.set_outgroup(R)\n",
    "    #print(\"Midpoint rooting is done for gene tree.\")\n",
    "\n",
    "    gene_tree = lable_SD_internal_nodes(gene_tree)\n",
    "    print(\"Overlap speciation is done for internal nodes of gene tree.\")\n",
    "    print(str(gene_tree.write(format=1))[:-1]+str(gene_tree.name)+\":0;\")\n",
    "    #print(gene_tree)\n",
    "\n",
    "    subHOG_thisLevel = []\n",
    "    # for  each speciestion node\n",
    "        #HOG([ sub hog  for in withing speciestion node], msa= merged_msa    )\n",
    "    \n",
    "    (assigned_leaves_to_hog, sub_msas_list_this_level) = traverse_geneTree_assign_hog(gene_tree, merged_msa)\n",
    "\n",
    "    tree_leaves = [ i.name for i in gene_tree.get_leaves()] \n",
    "    if set(tree_leaves)-set(assigned_leaves_to_hog) :\n",
    "        print(\"error 234\",assigned_leaves_to_hog, sub_msas_list_this_level)\n",
    "\n",
    "    dic_sub_msas[node_species_tree.name] = sub_msas_list_this_level \n",
    "    print(\"Number of hog at the taxonomic level\", node_species_tree.name ,\"is \",len(sub_msas_list_this_level),\". The number of proteins per hog is\", [len(i) for i in  sub_msas_list_this_level])    \n",
    "\n",
    "    # compare with  subHOG_lowerLevel merge if needed two are from different \n",
    "    subHOG_thisLevel =[]\n",
    "    for submsa in sub_msas_list_this_level :\n",
    "        subHOG_thisLevel.append([seq.id for seq in submsa])\n",
    "    #print(\"subHOGs are:\", subHOG_thisLevel)\n",
    "\n",
    "    paralog_set_thisLevel = find_paralog_thisLevel(tree_leaves, subHOG_thisLevel)\n",
    "\n",
    "    return (subHOG_thisLevel, paralog_set_thisLevel, dic_sub_msas)\n",
    "\n",
    "\n",
    "\n",
    "def write_ortholog_rhog(parlog_set_rhog, prot_names_rhog, address_orhto_pair_file):\n",
    "\n",
    "    all_pairs= set()\n",
    "    for i in range(len(prot_names_rhog)):\n",
    "        for j in range(i+1):\n",
    "            prot_i = prot_names_rhog[i]\n",
    "            prot_j = prot_names_rhog[j]\n",
    "            all_pairs.add((prot_i,prot_j))\n",
    "         \n",
    "    #print(all_pairs)\n",
    "    #print(\"para\", parlog_set_rhog)\n",
    "    file_ortho = open(address_orhto_pair_file, \"a\")\n",
    "    for pair in all_pairs:\n",
    "        if pair not in parlog_set_rhog:\n",
    "            prot_i_5letter= pair[0].split(\"|\")[1].strip()\n",
    "            prot_j_5letter= pair[1].split(\"|\")[1].strip()\n",
    "            #print(\"**ortho pair is**\",prot_i_5letter,prot_j_5letter)\n",
    "            file_ortho.write(prot_i_5letter+\"\\t\"+prot_j_5letter+\"\\n\")\n",
    "            \n",
    "    file_ortho.close() \n",
    "    \n",
    "    print(\"Some ortho pairs are appended in\", address_orhto_pair_file)\n",
    "\n",
    "    return 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def traverse_speciesTree_inferHOG(species_tree, rhog_i, species_names_rhog, rhogid_num, prot_names_rhog):\n",
    "\n",
    "    subHOG_all = []\n",
    "    dic_sub_msas = {}\n",
    "    parlog_set_rhog = set()\n",
    "\n",
    "    # finding hogs at each level of species tree (from leaves to root, bottom up)    \n",
    "    for node_species_tree in species_tree.traverse(strategy = \"postorder\"):\n",
    "        #dic_sub_hogs[node_species_tree.name] = []\n",
    "        dic_sub_msas[node_species_tree.name] = []\n",
    "        if node_species_tree.is_leaf() : \n",
    "            continue\n",
    "        print(\"\\n\"+\"*\"*15+\"\\n\",\"Finding hogs for the taxonomic level:\", node_species_tree.name,\"\\n\")\n",
    "\n",
    "        (subHOG_thisLevel, paralog_set_thisLevel, dic_sub_msas)=  infer_HOG_thisLevel(node_species_tree, rhog_i, species_names_rhog, dic_sub_msas, rhogid_num)\n",
    "        if subHOG_thisLevel == -1: continue\n",
    "        \n",
    "        print(\"subHOG_thisLevel\",subHOG_thisLevel)\n",
    "        subHOG_all.append(subHOG_thisLevel)\n",
    "        \n",
    "        parlog_set_rhog |= paralog_set_thisLevel\n",
    "\n",
    "        \n",
    "    write_ortho = write_ortholog_rhog(parlog_set_rhog, prot_names_rhog, address_orhto_pair_file)\n",
    "\n",
    "\n",
    "    return (subHOG_all)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_inference(rhogid_num):\n",
    "\n",
    "    print(\"\\n\"+\"=\"*50+\"\\n\",\"Working on root hog:\", rhogid_num,\"\\n\") \n",
    "\n",
    "    prot_address = address_out_hog+\"HOG_\"+str(rhogid_num)+\".fa\"\n",
    "    rhog_i = list(SeqIO.parse(prot_address, \"fasta\")) \n",
    "    print(\"number of proteins in the rHOG\", len(rhog_i))\n",
    "    (species_tree) = read_species_tree(tree_address)\n",
    "    (species_tree, species_names_rhog, prot_names_rhog) = prepare_species_tree(rhog_i, species_tree)\n",
    "\n",
    "    if len(rhog_i) ==1:\n",
    "        subHOG_all_rhog = [prot_names_rhog]\n",
    "        ortholog_set_rhog = set()\n",
    "        \n",
    "    else:\n",
    "        (subHOG_all_rhog) = traverse_speciesTree_inferHOG(species_tree, rhog_i, species_names_rhog, rhogid_num, prot_names_rhog)\n",
    "    \n",
    "    print(\"Working on root hog:\", rhogid_num,\" is finished.\\n\"+\"=\"*50+\"\\n\") \n",
    "\n",
    "    return (subHOG_all_rhog ) \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#from scipy.cluster.hierarchy import dendrogram, linkage, ward, leaves_list, fcluster\n",
    "\n",
    "import ete3\n",
    "from ete3 import Tree\n",
    "from ete3 import Phyloxml\n",
    "\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "#import ast\n",
    "#import pickle\n",
    "#import zoo\n",
    "#zoo.__file__\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import zoo.wrappers.aligners.mafft as mafft  # mafft should be installed beforehand\n",
    "import zoo.wrappers.treebuilders.fasttree as fasttree\n",
    "import logging\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sys import argv\n",
    "from os import listdir\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq # , UnknownSeqs\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib   #for development \n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "df094512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyoma.browser.models import ProteinEntry\n",
    "import pyoma.browser.db as db\n",
    "from pyoma.browser.hoghelper import build_hog_to_og_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6db34cc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "outdated database version, but only minor version change: 3.5 != 3.4. Some functions might fail\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program has started. The oma database address is in  /work/FAC/FBM/DBC/cdessim2/default/smajidi1/fastget/qfo/omamer_database/oma_path/OmaServer.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot load SequenceSearch. Any future call to seq_search will fail!\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/software/miniconda3/lib/python3.8/site-packages/pyoma/browser/db.py\", line 2035, in __init__\n",
      "    self.seq_idx = self.seq_idx()\n",
      "  File \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/software/miniconda3/lib/python3.8/site-packages/tables/link.py\", line 393, in __call__\n",
      "    self.extfile = tables.open_file(filename, **kwargs)\n",
      "  File \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/software/miniconda3/lib/python3.8/site-packages/pyoma/browser/db.py\", line 113, in synchronized_open_file\n",
      "    return _tables_file._original_open_file(*args, **kwargs)\n",
      "  File \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/software/miniconda3/lib/python3.8/site-packages/tables/file.py\", line 315, in open_file\n",
      "    return File(filename, mode, title, root_uep, filters, **kwargs)\n",
      "  File \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/software/miniconda3/lib/python3.8/site-packages/pyoma/browser/db.py\", line 103, in __init__\n",
      "    super(ThreadsafeFile, self).__init__(*args, **kargs)\n",
      "  File \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/software/miniconda3/lib/python3.8/site-packages/tables/file.py\", line 778, in __init__\n",
      "    self._g_new(filename, mode, **params)\n",
      "  File \"tables/hdf5extension.pyx\", line 374, in tables.hdf5extension.File._g_new\n",
      "  File \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/software/miniconda3/lib/python3.8/site-packages/tables/utils.py\", line 154, in check_file_access\n",
      "    raise IOError(\"``%s`` does not exist\" % (filename,))\n",
      "OSError: ``/work/FAC/FBM/DBC/cdessim2/default/smajidi1/fastget/qfo/omamer_database/oma_path/OmaServer.h5.idx`` does not exist\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/software/miniconda3/lib/python3.8/site-packages/pyoma/browser/db.py\", line 189, in __init__\n",
      "    self.seq_search = SequenceSearch(self)\n",
      "  File \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/software/miniconda3/lib/python3.8/site-packages/pyoma/browser/db.py\", line 2040, in __init__\n",
      "    raise DBConsistencyError(\n",
      "pyoma.browser.db.DBConsistencyError: Suffix index for protein sequences is not available: ``/work/FAC/FBM/DBC/cdessim2/default/smajidi1/fastget/qfo/omamer_database/oma_path/OmaServer.h5.idx`` does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:32:58 - OMA data is parsed and its release name is: All.Dec2021\n",
      "15:32:58 - There are 2496 species in the OMA database.\n",
      "15:33:17 - The are 78 species in the proteome folder.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     print(\"program is started \")\n",
    "#     address_working_folder = \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/fastget/qfo/\"\n",
    "#     address_out_hog= address_working_folder+ \"hog_out_g2_s100/\"\n",
    "    \n",
    "#     rHog_is_ready= False \n",
    "#     if not rHog_is_ready : \n",
    "\n",
    "if 1:\n",
    "        oma_database_address = address_working_folder+\"omamer_database/oma_path/OmaServer.h5\"\n",
    "        print(\"program has started. The oma database address is in \",oma_database_address)\n",
    "        (oma_db, list_oma_speices) = parse_oma_db(oma_database_address)\n",
    "\n",
    "        (query_species_names, query_prot_records_species) = parse_proteome(list_oma_speices)   \n",
    "        query_prot_records_species = add_species_name(query_prot_records_species,query_species_names)\n",
    "\n",
    "#         hogmap_allspecies = parse_hogmap_omamer(query_species_names)\n",
    "#         (query_prot_names_species_mapped, prots_hogmap_hogid_allspecies, prots_hogmap_subfscore_allspecies, prots_hogmap_seqlen_allspecies, prots_hogmap_subfmedseqlen_allspecies) = hogmap_allspecies \n",
    "#         query_prot_records_species_filtered =  filter_prot_mapped(query_species_names, query_prot_records_species, query_prot_names_species_mapped)\n",
    "\n",
    "#         print(len(query_prot_records_species_filtered),len(query_prot_records_species_filtered[0]))\n",
    "\n",
    "#         ## Write  rootHoGs as fasta file\n",
    "#         (rhogid_num_list, rhogids_prot_records_query) =  write_rootHOGs(prots_hogmap_hogid_allspecies, address_out_hog)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a970d9f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(\"program is started \")\n",
    "    \n",
    "#     address_working_folder = \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/fastget/qfo/\"\n",
    "#     address_out_hog= address_working_folder+ \"hog_out_g2_s100/\"\n",
    "    \n",
    "#     rHog_is_ready= True \n",
    "#     if not rHog_is_ready : pass\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     tree_address = address_working_folder+\"lineage_tree_qfo.phyloxml\"\n",
    "    \n",
    "#     rhog_files = listdir(address_out_hog)\n",
    "#     rhogid_num_list= []\n",
    "#     for rhog_file in rhog_files:\n",
    "#         if rhog_file.split(\".\")[-1] == \"fa\":\n",
    "#             rhogid_num = int(rhog_file.split(\".\")[0].split(\"_\")[1])\n",
    "#             rhogid_num_list.append(rhogid_num)\n",
    "\n",
    "    \n",
    "#     try : \n",
    "#         rhogid_num_list2 = rhogid_num_list[1002:1110] #[159556, 105904] #rhogid_num_list[2000:2010]\n",
    "#         dic_solved={}\n",
    "#         list_done=[]\n",
    "#         for i in rhogid_num_list2: dic_solved[i] = 0\n",
    "\n",
    "#         subHOG_all= [] # list of list\n",
    "#         # make sure the file is empty\n",
    "#         address_orhto_pair_file =  address_working_folder+\"ortho_pair_first.tsv\"\n",
    "#         file_ortho = open(address_orhto_pair_file, \"w\")\n",
    "#         file_ortho.close()\n",
    "\n",
    "\n",
    "#         print(\"parrallel is started\")\n",
    "#         number_max_workers = 20\n",
    "#         with concurrent.futures.ProcessPoolExecutor(max_workers = number_max_workers) as executor: \n",
    "#             for rhogid_num, output_values in zip(rhogid_num_list2, executor.map(run_inference, rhogid_num_list2)):\n",
    "#                 (subHOG_all_rhog) = output_values\n",
    "#                 dic_solved[rhogid_num]=1\n",
    "#                 list_done.append(rhogid_num)\n",
    "                \n",
    "\n",
    "\n",
    "#         print(\"all done !!\")\n",
    "#         print(\"program is finished \")\n",
    "#     except:\n",
    "#         print(\"program faced an error \",rhogid_num)        \n",
    "#         print(\"all\",rhogid_num_list2)\n",
    "#         print(\"****** done\", list_done)\n",
    "#         index_last= rhogid_num_list2.index(list_done[-1])\n",
    "#         print(index_last)\n",
    "#         print(rhogid_num_list2[index_last-1:index_last+5])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "f4ecd936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_HOG_thisLevel(node_species_tree, rhog_i, species_names_rhog, dic_sub_hogs, rhogid_num):\n",
    "\n",
    "    sub_msa_list_lowerLevel = [] # including subHOGS of lower level \n",
    "    subHOGs_children = []\n",
    "\n",
    "    print(\"working on node\", node_species_tree.name,\"with\",len(node_species_tree.children),\"children.\")\n",
    "    for node_child in node_species_tree.children:\n",
    "        if  node_child.is_leaf():\n",
    "            node_species_name = node_child.name\n",
    "            #extracting those proteins of the rHOG that belongs to this species (child node of species tree)             \n",
    "            interest_list = [idx  for idx in range(len(species_names_rhog)) if species_names_rhog[idx] == node_species_name ]\n",
    "            rhog_part = [rhog_i[i] for i in interest_list]\n",
    "            #sub_msa = [MultipleSeqAlignment([i]) for i in rhog_part] \n",
    "            #print(\"len\",len(rhog_part))\n",
    "\n",
    "            #sub_hog_leaf_list=[]\n",
    "            for prot in rhog_part : \n",
    "                sub_hog_leaf = HOG(prot, node_species_name ) # node_species_tree.name\n",
    "                #list_all_hogs_ever.append(sub_hog_leaf)\n",
    "                subHOGs_children.append(sub_hog_leaf)                \n",
    "                #sub_hog_leaf_list.append(sub_hog_leaf)\n",
    "            #sub_msa = sub_hog_leaf_list\n",
    "\n",
    "        else:   # the child node is an internal node, subHOGs are inferred till now during traversing.\n",
    "\n",
    "            print(\"sub msa for internal node\", node_child.name,\"is read from dic.\")\n",
    "            #if node_child.name in dic_sub_msas:\n",
    "                #sub_msa  = dic_sub_msas[node_child.name]\n",
    "            if node_child.name in dic_sub_hogs:\n",
    "                sub_hogs_child  = dic_sub_hogs[node_child.name]\n",
    "                subHOGs_children += sub_hogs_child\n",
    "            else:\n",
    "                print(\"error 131, no sub msa for the internal node \",node_child.name, node_child)\n",
    "                assert 2==1 \n",
    "    temp11=[]\n",
    "    for temp in [i._members for i in subHOGs_children]:\n",
    "        temp11.append([ prot.split('|')[2] for prot in temp])\n",
    "\n",
    "    print(\"there are \",len(subHOGs_children),\"subHOGs lower of this level:\",[i._hogid for i in subHOGs_children],temp11)\n",
    "    print(\"We want to infer subHOGs at this level,i.e. merge few of them.\")    \n",
    "\n",
    "    if len(subHOGs_children) <2:\n",
    "        print(\"**** error 134 *** \", len(subHOGs_children),subHOGs_children)\n",
    "            #return (-1,-1,-1)\n",
    "    else:\n",
    "\n",
    "        sub_msa_list_lowerLevel_ready = [hog._msa for hog in subHOGs_children]\n",
    "        merged_msa = merge_msa(sub_msa_list_lowerLevel_ready) \n",
    "\n",
    "        print(\"All subHOGs are merged, merged msa is with length of\",len(merged_msa), len(merged_msa[0]),\".\")\n",
    "\n",
    "        gene_tree_file =  address_working_folder + \"/gene_trees_test/tree_\"+str(rhogid_num)+\"_\"+str(node_species_tree.name)+\".nwk\"\n",
    "        gene_tree_raw = draw_tree(merged_msa, gene_tree_file)\n",
    "        gene_tree = Tree(gene_tree_raw+\";\", format=0)\n",
    "        print(\"Gene tree is infered with length of\",len(gene_tree),\".\")\n",
    "\n",
    "        #gene_tree_i +=1\n",
    "        R = gene_tree.get_midpoint_outgroup()\n",
    "        gene_tree.set_outgroup(R)\n",
    "        #print(\"Midpoint rooting is done for gene tree.\")\n",
    "        gene_tree = lable_SD_internal_nodes(gene_tree)\n",
    "        print(\"Overlap speciation is done for internal nodes of gene tree, as following:\")\n",
    "        print(str(gene_tree.write(format=1))[:-1]+str(gene_tree.name)+\":0;\")\n",
    "        #print(gene_tree)\n",
    "        #(assigned_leaves_to_hog, sub_msas_list_this_level) = traverse_geneTree_assign_hog(gene_tree, merged_msa)\n",
    "\n",
    "        tree_leaves = [i.name for i in gene_tree.get_leaves() ]\n",
    "        #assigned_leaves_to_hog = []        #sub_msas_list_this_level = []\n",
    "        subHOGs_id_children_assigned = [] # the same as  subHOG_to_be_merged_all_id \n",
    "        HOG_this_level = []\n",
    "        subHOG_to_be_merged_set_other_Snodes = []\n",
    "        subHOG_to_be_merged_set_other_Snodes_flattned_temp  = []\n",
    "        for node in gene_tree.traverse(strategy = \"preorder\", is_leaf_fn= lambda n :   hasattr(n, \"processed\") and   n.processed == True  ): # start from root\n",
    "            #print(\"Leaves assigned to hog are \", assigned_leaves_to_hog)   #print(\"Traversing gene tree. Now at node\", node.name)\n",
    "            if not node.is_leaf() : \n",
    "                node_leaves_name = [ i.name for i in node.get_leaves() ] \n",
    "                print(node_leaves_name)\n",
    "\n",
    "                if node.name[0] ==\"S\":\n",
    "                    # this is a sub-hog.\n",
    "                    subHOG_to_be_merged = [ ]\n",
    "                    for node_leave_name in node_leaves_name:\n",
    "                        #print(node_leave_name)\n",
    "                        for subHOG in subHOGs_children :\n",
    "                            subHOG_members= subHOG._members\n",
    "                            if node_leave_name in subHOG_members:\n",
    "                                \n",
    "                                \n",
    "                                # conflict ????? \n",
    "                        \n",
    "                                # could be improved\n",
    "                                if subHOG._hogid  not in subHOG_to_be_merged_set_other_Snodes_flattned_temp:\n",
    "\n",
    "                                    subHOG_to_be_merged.append(subHOG)\n",
    "                                    subHOGs_id_children_assigned.append(subHOG._hogid)\n",
    "                                else:\n",
    "                                    print(\"issue 184\",node.name,subHOG._hogid, node_leave_name)\n",
    "                                    if  \"processed\" in  node:\n",
    "                                        print(node.name)\n",
    "                                    else:\n",
    "                                        print(\"processed not in \", node.name)\n",
    "                                        \n",
    "                                # print(node_leave_name,\"is in \",subHOG._hogid)\n",
    "                    if subHOG_to_be_merged :\n",
    "                        subHOG_to_be_merged_set = set(subHOG_to_be_merged)     \n",
    "                        taxnomic_range= node_species_tree.name\n",
    "                        HOG_this_node = HOG(subHOG_to_be_merged_set,taxnomic_range , msa= merged_msa) \n",
    "                        HOG_this_level.append(HOG_this_node)\n",
    "                        subHOG_to_be_merged_set_other_Snodes.append([i._hogid for i in subHOG_to_be_merged_set])\n",
    "                        subHOG_to_be_merged_set_other_Snodes_flattned_temp= [item for items in subHOG_to_be_merged_set_other_Snodes for item in items]       \n",
    "                    \n",
    "                    \n",
    "                    #  I don't need to traverse deeper in this clade\n",
    "                    \n",
    "                    node.processed = True\n",
    "                    #print(\"?*?*  \", node.name)\n",
    "                    \n",
    "\n",
    "                    \n",
    "            subHOG_to_be_merged_set_other_Snodes_flattned= [item for items in subHOG_to_be_merged_set_other_Snodes for item in items]       \n",
    "            if  [i._hogid for i in subHOGs_children] == subHOG_to_be_merged_set_other_Snodes_flattned:\n",
    "                break\n",
    "\n",
    "        for subHOG in subHOGs_children :      \n",
    "            # for the single branch  ( D include a  subhog and a S node. )\n",
    "            if  subHOG._hogid  not in subHOGs_id_children_assigned : \n",
    "                #print(\"here\", subHOG)\n",
    "                HOG_this_level.append(subHOG)\n",
    "        prot_list_sbuhog= [i._members for i in HOG_this_level]\n",
    "        prot_list_sbuhog_short = []\n",
    "        for prot_sub_list_sbuhog in prot_list_sbuhog:\n",
    "            prot_list_sbuhog_short.append([ prot.split('|')[2] for prot in prot_sub_list_sbuhog])\n",
    "            \n",
    "        \n",
    "        print(\"- \",len(prot_list_sbuhog_short),\"HOGs are inferred at the level \",node_species_tree.name,\":\",prot_list_sbuhog_short )\n",
    "    print(\"By merging \",subHOG_to_be_merged_set_other_Snodes)\n",
    "\n",
    "        #check for conflic in merging\n",
    "    #     for i in range(subHOG_to_be_merged_set_other_Snodes):\n",
    "    #         if \n",
    "    #         for i in range(subHOG_to_be_merged_set_other_Snodes):\n",
    "    dic_sub_hogs[node_species_tree.name] = HOG_this_level\n",
    "    return (dic_sub_hogs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "2522a542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=2\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d066af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "17a2141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "address_working_folder = \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/fastget/qfo/\"\n",
    "address_out_hog= address_working_folder+ \"hog_out_g2_s100/\"\n",
    "\n",
    "rHog_is_ready= True \n",
    "if not rHog_is_ready : pass\n",
    "\n",
    "tree_address = address_working_folder+\"lineage_tree_qfo.phyloxml\"\n",
    "\n",
    "rhog_files = listdir(address_out_hog)\n",
    "rhogid_num_list= []\n",
    "for rhog_file in rhog_files:\n",
    "    if rhog_file.split(\".\")[-1] == \"fa\":\n",
    "        rhogid_num = int(rhog_file.split(\".\")[0].split(\"_\")[1])\n",
    "        rhogid_num_list.append(rhogid_num)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "subHOG_all= [] # list of list\n",
    "\n",
    "address_orhto_pair_file =  address_working_folder+\"ortho_pair_first_test.tsv\"\n",
    "file_ortho = open(address_orhto_pair_file, \"w\")\n",
    "file_ortho.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb4b416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "eec006a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "f39193a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOG:\n",
    "    \n",
    "    _hogid_iter = 100 \n",
    "    def __init__(self, input_instantiate, taxnomic_range,  msa = None):       # _prot_names\n",
    "        \n",
    "        # the input_instantiate could be either\n",
    "        #     1) a protein as the biopython seq record  SeqRecord(seq=Seq('MAPSSRSPSPRT. ] \n",
    "        # or  2) a set of intances of class HOG   wit a big msa\n",
    "        \n",
    "        # those variable starting with _ are local to the class, should not access directly  (although it is possbile)\n",
    "        self.__class__._hogid_iter += 1\n",
    "        self._hogid= \"hog\"+str(self.__class__._hogid_iter)\n",
    "        self._taxnomic_range = taxnomic_range\n",
    "        #print(\"**** a new HOG is instantiated with id\", self._hogid)\n",
    "        \n",
    "        if  isinstance(input_instantiate, SeqRecord):    #if len(sub_hogs)==1:\n",
    "            only_protein = input_instantiate\n",
    "            # only one seq, only on child, leaf            \n",
    "            self._members = set([only_protein.id])\n",
    "            self._msa =  MultipleSeqAlignment([only_protein])\n",
    "            self._subhogs = []\n",
    "            # <<class 'Bio.Align.MultipleSeqAlignment'> instance (1 records of length 314) at 7f0e86c713d0>\n",
    "            \n",
    "        elif     msa    and   all(isinstance(x, HOG) for x in input_instantiate):  \n",
    "            # here we want to merge few subHOGs and creat a new HOG.\n",
    "            # the n\n",
    "            \n",
    "            sub_hogs = input_instantiate\n",
    "            \n",
    "            hog_members = set()\n",
    "            for sub_hog in sub_hogs: \n",
    "                hog_members |= sub_hog.get_members()  #union\n",
    "            self._members =  hog_members              #set.union(*tup) \n",
    "            # full members \n",
    "            self._subhogs = list(input_instantiate)\n",
    "            \n",
    "           \n",
    "            \n",
    "            # subsampling in msa \n",
    "            max_num_seq=30\n",
    "            \n",
    "            records_full = [record for record in msa if record.id in self._members]\n",
    "            if len(records_full)> max_num_seq: \n",
    "                records_sub_sampled = sample(records_full, max_num_seq)   #  without replacement.\n",
    "                print(\"we are doing subsamping now from\",len(records_full), \"to\",max_num_seq,\"seq\")\n",
    "            else:\n",
    "                records_sub_sampled = records_full\n",
    "            \n",
    "            # removing some columns completely gap -  (not x   )\n",
    "            \n",
    "            # now select those proteins \n",
    "            self._msa =  MultipleSeqAlignment(records_sub_sampled)\n",
    "            # without replacement sampling , \n",
    "            # self._children = sub_hogs # as legacy  ?\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print(\"Error 136,  check the input format to instantiate a HOG class\")\n",
    "            assert False\n",
    "\n",
    "    def __repr__(self):\n",
    "        \n",
    "        return \"class HOG with lenght\"+str(len(self._members))+\"taxonimic range\"+str(self._taxnomic_range)\n",
    "            \n",
    "            \n",
    "    def get_members(self):\n",
    "        return set(self._members)\n",
    "        \n",
    "        #merge, gene tree, midpoint, lable_SD_internal_nodes, traverse_geneTree_assign_hog\n",
    "#     def to_orthoxml(self):\n",
    "#          if len(self._members) == 1:   \n",
    "#             out = \"xml.generef\"\n",
    "#             return out\n",
    "#         else:\n",
    "#             out=1\n",
    "\n",
    "    def to_orthoxml(self, indent=0):\n",
    "        if len(self._subhogs) == 0:\n",
    "            return ET.Element('geneRef', attrib={'id': list(self._members)[0]})\n",
    "        \n",
    "        hog_elemnt = ET.Element('orthologGroup')\n",
    "        def _sorter_key(sh): \n",
    "            return sh._taxnomic_range\n",
    "        self._subhogs.sort(key=_sorter_key)\n",
    "        print(f'{\" \"*indent}subhog: {self._taxnomic_range}:')\n",
    "        for sub_clade, sub_hogs in itertools.groupby(self._subhogs, key=_sorter_key):\n",
    "            list_of_subhogs_of_same_clade = list(sub_hogs)\n",
    "            print(f'{\" \"*(indent+1)} clade: {sub_clade} with {str(len(list_of_subhogs_of_same_clade))}')\n",
    "            if len(list_of_subhogs_of_same_clade) > 1:\n",
    "                paralog_element = ET.Element('paralogGroup')\n",
    "                for sh in list_of_subhogs_of_same_clade:\n",
    "                    paralog_element.append(sh.to_orthoxml(indent+2))\n",
    "                hog_elemnt.append(paralog_element)\n",
    "            else:\n",
    "                hog_elemnt.append(list_of_subhogs_of_same_clade[0].to_orthoxml(indent+2))\n",
    "        return hog_elemnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e508d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "00c392b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Working on root hog: 569382 \n",
      "\n",
      "number of proteins in the rHOG 14\n",
      "/work/FAC/FBM/DBC/cdessim2/default/smajidi1/fastget/qfo/lineage_tree_qfo.phyloxml\n",
      "number of unique species in the rHOG 14\n",
      "Working on the following species tree.\n",
      "\n",
      " species_names_rhog  {'GORGO', 'CHICK', 'CANLF', 'XENTR', 'BOVIN', 'LEPOC', 'ORYLA', 'MONDO', 'MOUSE', 'NEMVE', 'PANTR', 'DANRE', 'HUMAN', 'RATNO'}\n",
      "(NEMVE:0,((LEPOC:0,(ORYLA:0,DANRE:0)1:0)1:0,((((((MOUSE:0,RATNO:0)1:0,(GORGO:0,HUMAN:0,PANTR:0)1:0)1:0,(CANLF:0,BOVIN:0)1:0)1:0,MONDO:0)1:0,CHICK:0)1:0,XENTR:0)1:0)1:0);\n",
      "\n",
      "   /-NEMVE\n",
      "  |\n",
      "  |      /-LEPOC\n",
      "  |   /-|\n",
      "  |  |  |   /-ORYLA\n",
      "  |  |   \\-|\n",
      "--|  |      \\-DANRE\n",
      "  |  |\n",
      "  |  |                  /-MOUSE\n",
      "  |  |               /-|\n",
      "  |  |              |   \\-RATNO\n",
      "  |  |            /-|\n",
      "  |  |           |  |   /-GORGO\n",
      "   \\-|           |  |  |\n",
      "     |           |   \\-|--HUMAN\n",
      "     |         /-|     |\n",
      "     |        |  |      \\-PANTR\n",
      "     |        |  |\n",
      "     |      /-|  |   /-CANLF\n",
      "     |     |  |   \\-|\n",
      "     |     |  |      \\-BOVIN\n",
      "     |   /-|  |\n",
      "     |  |  |   \\-MONDO\n",
      "      \\-|  |\n",
      "        |   \\-CHICK\n",
      "        |\n",
      "         \\-XENTR\n",
      "\n",
      "***************\n",
      " Finding hogs for the taxonomic level: ORYLA_DANRE \n",
      "\n",
      "working on node ORYLA_DANRE with 2 children.\n",
      "there are  2 subHOGs lower of this level: ['hog101', 'hog102'] [['H2MS52_ORYLA'], ['A2BGQ0_DANRE']]\n",
      "We want to infer subHOGs at this level,i.e. merge few of them.\n",
      "2 msas are merged into one with the length of  2 569\n",
      "All subHOGs are merged, merged msa is with length of 2 569 .\n",
      "Gene tree is infered with length of 2 .\n",
      "Overlap speciation is done for internal nodes of gene tree, as following:\n",
      "(tr|H2MS52|H2MS52_ORYLA:0.283307,tr|A2BGQ0|A2BGQ0_DANRE:0.283307)S1:0;\n",
      "['tr|H2MS52|H2MS52_ORYLA', 'tr|A2BGQ0|A2BGQ0_DANRE']\n",
      "-  1 HOGs are inferred at the level  ORYLA_DANRE : [['H2MS52_ORYLA', 'A2BGQ0_DANRE']]\n",
      "By merging  [['hog101', 'hog102']]\n",
      "\n",
      "***************\n",
      " Finding hogs for the taxonomic level: LEPOC_ORYLA_DANRE \n",
      "\n",
      "working on node LEPOC_ORYLA_DANRE with 2 children.\n",
      "sub msa for internal node ORYLA_DANRE is read from dic.\n",
      "there are  2 subHOGs lower of this level: ['hog104', 'hog103'] [['W5NLW9_LEPOC'], ['H2MS52_ORYLA', 'A2BGQ0_DANRE']]\n",
      "We want to infer subHOGs at this level,i.e. merge few of them.\n",
      "2 msas are merged into one with the length of  3 574\n",
      "All subHOGs are merged, merged msa is with length of 3 574 .\n",
      "Gene tree is infered with length of 3 .\n",
      "Overlap speciation is done for internal nodes of gene tree, as following:\n",
      "(tr|H2MS52|H2MS52_ORYLA:0.187423,(tr|W5NLW9|W5NLW9_LEPOC:0.206616,tr|A2BGQ0|A2BGQ0_DANRE:0.250223)S1:0.187423)S2:0;\n",
      "['tr|H2MS52|H2MS52_ORYLA', 'tr|W5NLW9|W5NLW9_LEPOC', 'tr|A2BGQ0|A2BGQ0_DANRE']\n",
      "-  1 HOGs are inferred at the level  LEPOC_ORYLA_DANRE : [['H2MS52_ORYLA', 'W5NLW9_LEPOC', 'A2BGQ0_DANRE']]\n",
      "By merging  [['hog103', 'hog104']]\n",
      "\n",
      "***************\n",
      " Finding hogs for the taxonomic level: MOUSE_RATNO \n",
      "\n",
      "working on node MOUSE_RATNO with 2 children.\n",
      "there are  2 subHOGs lower of this level: ['hog106', 'hog107'] [['AN34B_MOUSE'], ['D3ZKP1_RAT']]\n",
      "We want to infer subHOGs at this level,i.e. merge few of them.\n",
      "2 msas are merged into one with the length of  2 508\n",
      "All subHOGs are merged, merged msa is with length of 2 508 .\n",
      "Gene tree is infered with length of 2 .\n",
      "Overlap speciation is done for internal nodes of gene tree, as following:\n",
      "(sp|Q3UUF8|AN34B_MOUSE:0.024655,tr|D3ZKP1|D3ZKP1_RAT:0.024655)S1:0;\n",
      "['sp|Q3UUF8|AN34B_MOUSE', 'tr|D3ZKP1|D3ZKP1_RAT']\n",
      "-  1 HOGs are inferred at the level  MOUSE_RATNO : [['AN34B_MOUSE', 'D3ZKP1_RAT']]\n",
      "By merging  [['hog106', 'hog107']]\n",
      "\n",
      "***************\n",
      " Finding hogs for the taxonomic level: GORGO_HUMAN_PANTR \n",
      "\n",
      "working on node GORGO_HUMAN_PANTR with 3 children.\n",
      "there are  3 subHOGs lower of this level: ['hog109', 'hog110', 'hog111'] [['A0A2I2ZNV7_GORGO'], ['AN34B_HUMAN'], ['A0A2I3THT9_PANTR']]\n",
      "We want to infer subHOGs at this level,i.e. merge few of them.\n",
      "3 msas are merged into one with the length of  3 514\n",
      "All subHOGs are merged, merged msa is with length of 3 514 .\n",
      "Gene tree is infered with length of 3 .\n",
      "Overlap speciation is done for internal nodes of gene tree, as following:\n",
      "(tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO:0.00539271,(sp|A5PLL1|AN34B_HUMAN:0.00643798,tr|A0A2I3THT9|A0A2I3THT9_PANTR:0.00379808)S1:0.00539271)S2:0;\n",
      "['tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO', 'sp|A5PLL1|AN34B_HUMAN', 'tr|A0A2I3THT9|A0A2I3THT9_PANTR']\n",
      "-  1 HOGs are inferred at the level  GORGO_HUMAN_PANTR : [['A0A2I2ZNV7_GORGO', 'A0A2I3THT9_PANTR', 'AN34B_HUMAN']]\n",
      "By merging  [['hog111', 'hog109', 'hog110']]\n",
      "\n",
      "***************\n",
      " Finding hogs for the taxonomic level: MOUSE_RATNO_GORGO_HUMAN_PANTR \n",
      "\n",
      "working on node MOUSE_RATNO_GORGO_HUMAN_PANTR with 2 children.\n",
      "sub msa for internal node MOUSE_RATNO is read from dic.\n",
      "sub msa for internal node GORGO_HUMAN_PANTR is read from dic.\n",
      "there are  2 subHOGs lower of this level: ['hog108', 'hog112'] [['AN34B_MOUSE', 'D3ZKP1_RAT'], ['A0A2I2ZNV7_GORGO', 'A0A2I3THT9_PANTR', 'AN34B_HUMAN']]\n",
      "We want to infer subHOGs at this level,i.e. merge few of them.\n",
      "2 msas are merged into one with the length of  5 514\n",
      "All subHOGs are merged, merged msa is with length of 5 514 .\n",
      "Gene tree is infered with length of 5 .\n",
      "Overlap speciation is done for internal nodes of gene tree, as following:\n",
      "((sp|Q3UUF8|AN34B_MOUSE:0.0306233,tr|D3ZKP1|D3ZKP1_RAT:0.0193017)S1:0.0846956,(sp|A5PLL1|AN34B_HUMAN:0.00397007,(tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO:0.011159,tr|A0A2I3THT9|A0A2I3THT9_PANTR:0.00379111)S2:0.00203166)S3:0.0846956)S4:0;\n",
      "['sp|Q3UUF8|AN34B_MOUSE', 'tr|D3ZKP1|D3ZKP1_RAT', 'sp|A5PLL1|AN34B_HUMAN', 'tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO', 'tr|A0A2I3THT9|A0A2I3THT9_PANTR']\n",
      "-  1 HOGs are inferred at the level  MOUSE_RATNO_GORGO_HUMAN_PANTR : [['AN34B_MOUSE', 'D3ZKP1_RAT', 'A0A2I3THT9_PANTR', 'AN34B_HUMAN', 'A0A2I2ZNV7_GORGO']]\n",
      "By merging  [['hog112', 'hog108']]\n",
      "\n",
      "***************\n",
      " Finding hogs for the taxonomic level: CANLF_BOVIN \n",
      "\n",
      "working on node CANLF_BOVIN with 2 children.\n",
      "there are  2 subHOGs lower of this level: ['hog114', 'hog115'] [['J9JHK4_CANLF'], ['F1ME24_BOVIN']]\n",
      "We want to infer subHOGs at this level,i.e. merge few of them.\n",
      "2 msas are merged into one with the length of  2 514\n",
      "All subHOGs are merged, merged msa is with length of 2 514 .\n",
      "Gene tree is infered with length of 2 .\n",
      "Overlap speciation is done for internal nodes of gene tree, as following:\n",
      "(tr|J9JHK4|J9JHK4_CANLF:0.065565,tr|F1ME24|F1ME24_BOVIN:0.065565)S1:0;\n",
      "['tr|J9JHK4|J9JHK4_CANLF', 'tr|F1ME24|F1ME24_BOVIN']\n",
      "-  1 HOGs are inferred at the level  CANLF_BOVIN : [['J9JHK4_CANLF', 'F1ME24_BOVIN']]\n",
      "By merging  [['hog115', 'hog114']]\n",
      "\n",
      "***************\n",
      " Finding hogs for the taxonomic level: MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN \n",
      "\n",
      "working on node MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN with 2 children.\n",
      "sub msa for internal node MOUSE_RATNO_GORGO_HUMAN_PANTR is read from dic.\n",
      "sub msa for internal node CANLF_BOVIN is read from dic.\n",
      "there are  2 subHOGs lower of this level: ['hog113', 'hog116'] [['AN34B_MOUSE', 'D3ZKP1_RAT', 'A0A2I3THT9_PANTR', 'AN34B_HUMAN', 'A0A2I2ZNV7_GORGO'], ['J9JHK4_CANLF', 'F1ME24_BOVIN']]\n",
      "We want to infer subHOGs at this level,i.e. merge few of them.\n",
      "2 msas are merged into one with the length of  7 514\n",
      "All subHOGs are merged, merged msa is with length of 7 514 .\n",
      "Gene tree is infered with length of 7 .\n",
      "Overlap speciation is done for internal nodes of gene tree, as following:\n",
      "((sp|Q3UUF8|AN34B_MOUSE:0.0305315,tr|D3ZKP1|D3ZKP1_RAT:0.0196136)S1:0.0672538,((tr|J9JHK4|J9JHK4_CANLF:0.0752992,tr|F1ME24|F1ME24_BOVIN:0.0594543)S2:0.0146059,(sp|A5PLL1|AN34B_HUMAN:0.00201634,(tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO:0.0109517,tr|A0A2I3THT9|A0A2I3THT9_PANTR:0.00379372)S3:0.00421112)S4:0.0396518)S5:0.0672538)S6:0;\n",
      "['sp|Q3UUF8|AN34B_MOUSE', 'tr|D3ZKP1|D3ZKP1_RAT', 'tr|J9JHK4|J9JHK4_CANLF', 'tr|F1ME24|F1ME24_BOVIN', 'sp|A5PLL1|AN34B_HUMAN', 'tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO', 'tr|A0A2I3THT9|A0A2I3THT9_PANTR']\n",
      "-  1 HOGs are inferred at the level  MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN : [['AN34B_MOUSE', 'D3ZKP1_RAT', 'A0A2I3THT9_PANTR', 'AN34B_HUMAN', 'J9JHK4_CANLF', 'F1ME24_BOVIN', 'A0A2I2ZNV7_GORGO']]\n",
      "By merging  [['hog113', 'hog116']]\n",
      "\n",
      "***************\n",
      " Finding hogs for the taxonomic level: MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO \n",
      "\n",
      "working on node MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO with 2 children.\n",
      "sub msa for internal node MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN is read from dic.\n",
      "there are  2 subHOGs lower of this level: ['hog117', 'hog118'] [['AN34B_MOUSE', 'D3ZKP1_RAT', 'A0A2I3THT9_PANTR', 'AN34B_HUMAN', 'J9JHK4_CANLF', 'F1ME24_BOVIN', 'A0A2I2ZNV7_GORGO'], ['A0A5F8H9I8_MONDO']]\n",
      "We want to infer subHOGs at this level,i.e. merge few of them.\n",
      "2 msas are merged into one with the length of  8 538\n",
      "All subHOGs are merged, merged msa is with length of 8 538 .\n",
      "Gene tree is infered with length of 8 .\n",
      "Overlap speciation is done for internal nodes of gene tree, as following:\n",
      "(tr|A0A5F8H9I8|A0A5F8H9I8_MONDO:0.0976652,((sp|Q3UUF8|AN34B_MOUSE:0.0322093,tr|D3ZKP1|D3ZKP1_RAT:0.0180638)S1:0.1084,((tr|J9JHK4|J9JHK4_CANLF:0.0775104,tr|F1ME24|F1ME24_BOVIN:0.0578561)S2:0.0167173,(sp|A5PLL1|AN34B_HUMAN:0.0025502,(tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO:0.0109016,tr|A0A2I3THT9|A0A2I3THT9_PANTR:0.0037809)S3:0.00366316)S4:0.0397094)S5:0.0320605)S6:0.0976652)S7:0;\n",
      "['tr|A0A5F8H9I8|A0A5F8H9I8_MONDO', 'sp|Q3UUF8|AN34B_MOUSE', 'tr|D3ZKP1|D3ZKP1_RAT', 'tr|J9JHK4|J9JHK4_CANLF', 'tr|F1ME24|F1ME24_BOVIN', 'sp|A5PLL1|AN34B_HUMAN', 'tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO', 'tr|A0A2I3THT9|A0A2I3THT9_PANTR']\n",
      "-  1 HOGs are inferred at the level  MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO : [['D3ZKP1_RAT', 'J9JHK4_CANLF', 'F1ME24_BOVIN', 'A0A2I2ZNV7_GORGO', 'A0A5F8H9I8_MONDO', 'AN34B_MOUSE', 'A0A2I3THT9_PANTR', 'AN34B_HUMAN']]\n",
      "By merging  [['hog118', 'hog117']]\n",
      "\n",
      "***************\n",
      " Finding hogs for the taxonomic level: MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK \n",
      "\n",
      "working on node MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK with 2 children.\n",
      "sub msa for internal node MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO is read from dic.\n",
      "there are  2 subHOGs lower of this level: ['hog119', 'hog120'] [['D3ZKP1_RAT', 'J9JHK4_CANLF', 'F1ME24_BOVIN', 'A0A2I2ZNV7_GORGO', 'A0A5F8H9I8_MONDO', 'AN34B_MOUSE', 'A0A2I3THT9_PANTR', 'AN34B_HUMAN'], ['A0A1L1RSA7_CHICK']]\n",
      "We want to infer subHOGs at this level,i.e. merge few of them.\n",
      "2 msas are merged into one with the length of  9 539\n",
      "All subHOGs are merged, merged msa is with length of 9 539 .\n",
      "Gene tree is infered with length of 9 .\n",
      "Overlap speciation is done for internal nodes of gene tree, as following:\n",
      "(tr|A0A1L1RSA7|A0A1L1RSA7_CHICK:0.209155,(tr|A0A5F8H9I8|A0A5F8H9I8_MONDO:0.134417,((sp|Q3UUF8|AN34B_MOUSE:0.031778,tr|D3ZKP1|D3ZKP1_RAT:0.0195907)S1:0.11741,((tr|J9JHK4|J9JHK4_CANLF:0.0785625,tr|F1ME24|F1ME24_BOVIN:0.0583853)S2:0.0151912,(sp|A5PLL1|AN34B_HUMAN:0.00273446,(tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO:0.0110713,tr|A0A2I3THT9|A0A2I3THT9_PANTR:0.00385616)S3:0.00361396)S4:0.0439404)S5:0.0277174)S6:0.0706867)S7:0.209155)S8:0;\n",
      "['tr|A0A1L1RSA7|A0A1L1RSA7_CHICK', 'tr|A0A5F8H9I8|A0A5F8H9I8_MONDO', 'sp|Q3UUF8|AN34B_MOUSE', 'tr|D3ZKP1|D3ZKP1_RAT', 'tr|J9JHK4|J9JHK4_CANLF', 'tr|F1ME24|F1ME24_BOVIN', 'sp|A5PLL1|AN34B_HUMAN', 'tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO', 'tr|A0A2I3THT9|A0A2I3THT9_PANTR']\n",
      "-  1 HOGs are inferred at the level  MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK : [['D3ZKP1_RAT', 'J9JHK4_CANLF', 'F1ME24_BOVIN', 'A0A2I2ZNV7_GORGO', 'A0A1L1RSA7_CHICK', 'A0A5F8H9I8_MONDO', 'AN34B_MOUSE', 'A0A2I3THT9_PANTR', 'AN34B_HUMAN']]\n",
      "By merging  [['hog119', 'hog120']]\n",
      "\n",
      "***************\n",
      " Finding hogs for the taxonomic level: MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR \n",
      "\n",
      "working on node MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR with 2 children.\n",
      "sub msa for internal node MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK is read from dic.\n",
      "there are  2 subHOGs lower of this level: ['hog121', 'hog122'] [['D3ZKP1_RAT', 'J9JHK4_CANLF', 'F1ME24_BOVIN', 'A0A2I2ZNV7_GORGO', 'A0A1L1RSA7_CHICK', 'A0A5F8H9I8_MONDO', 'AN34B_MOUSE', 'A0A2I3THT9_PANTR', 'AN34B_HUMAN'], ['A0A6I8QDM6_XENTR']]\n",
      "We want to infer subHOGs at this level,i.e. merge few of them.\n",
      "2 msas are merged into one with the length of  10 547\n",
      "All subHOGs are merged, merged msa is with length of 10 547 .\n",
      "Gene tree is infered with length of 10 .\n",
      "Overlap speciation is done for internal nodes of gene tree, as following:\n",
      "(tr|A0A1L1RSA7|A0A1L1RSA7_CHICK:0.172242,(tr|A0A6I8QDM6|A0A6I8QDM6_XENTR:0.284642,(tr|A0A5F8H9I8|A0A5F8H9I8_MONDO:0.133933,((tr|J9JHK4|J9JHK4_CANLF:0.0761573,tr|F1ME24|F1ME24_BOVIN:0.0581037)S1:0.0122204,((sp|Q3UUF8|AN34B_MOUSE:0.0309465,tr|D3ZKP1|D3ZKP1_RAT:0.0196613)S2:0.13238,(sp|A5PLL1|AN34B_HUMAN:0.00389636,(tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO:0.0110944,tr|A0A2I3THT9|A0A2I3THT9_PANTR:0.00380783)S3:0.00219618)S4:0.0412641)S5:0.0108898)S6:0.0826317)S7:0.0807324)S8:0.172242)S9:0;\n",
      "['tr|A0A1L1RSA7|A0A1L1RSA7_CHICK', 'tr|A0A6I8QDM6|A0A6I8QDM6_XENTR', 'tr|A0A5F8H9I8|A0A5F8H9I8_MONDO', 'tr|J9JHK4|J9JHK4_CANLF', 'tr|F1ME24|F1ME24_BOVIN', 'sp|Q3UUF8|AN34B_MOUSE', 'tr|D3ZKP1|D3ZKP1_RAT', 'sp|A5PLL1|AN34B_HUMAN', 'tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO', 'tr|A0A2I3THT9|A0A2I3THT9_PANTR']\n",
      "-  1 HOGs are inferred at the level  MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR : [['D3ZKP1_RAT', 'J9JHK4_CANLF', 'F1ME24_BOVIN', 'A0A2I2ZNV7_GORGO', 'A0A1L1RSA7_CHICK', 'A0A5F8H9I8_MONDO', 'AN34B_MOUSE', 'A0A2I3THT9_PANTR', 'AN34B_HUMAN', 'A0A6I8QDM6_XENTR']]\n",
      "By merging  [['hog122', 'hog121']]\n",
      "\n",
      "***************\n",
      " Finding hogs for the taxonomic level: LEPOC_ORYLA_DANRE_MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR \n",
      "\n",
      "working on node LEPOC_ORYLA_DANRE_MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR with 2 children.\n",
      "sub msa for internal node LEPOC_ORYLA_DANRE is read from dic.\n",
      "sub msa for internal node MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR is read from dic.\n",
      "there are  2 subHOGs lower of this level: ['hog105', 'hog123'] [['H2MS52_ORYLA', 'W5NLW9_LEPOC', 'A2BGQ0_DANRE'], ['D3ZKP1_RAT', 'J9JHK4_CANLF', 'F1ME24_BOVIN', 'A0A2I2ZNV7_GORGO', 'A0A1L1RSA7_CHICK', 'A0A5F8H9I8_MONDO', 'AN34B_MOUSE', 'A0A2I3THT9_PANTR', 'AN34B_HUMAN', 'A0A6I8QDM6_XENTR']]\n",
      "We want to infer subHOGs at this level,i.e. merge few of them.\n",
      "2 msas are merged into one with the length of  13 598\n",
      "All subHOGs are merged, merged msa is with length of 13 598 .\n",
      "Gene tree is infered with length of 13 .\n",
      "Overlap speciation is done for internal nodes of gene tree, as following:\n",
      "((tr|W5NLW9|W5NLW9_LEPOC:0.0619081,(tr|H2MS52|H2MS52_ORYLA:0.392427,tr|A2BGQ0|A2BGQ0_DANRE:0.259229)S1:0.161513)S2:0.109288,(tr|A0A6I8QDM6|A0A6I8QDM6_XENTR:0.199778,(tr|A0A1L1RSA7|A0A1L1RSA7_CHICK:0.341027,(tr|A0A5F8H9I8|A0A5F8H9I8_MONDO:0.135132,((tr|J9JHK4|J9JHK4_CANLF:0.0764105,tr|F1ME24|F1ME24_BOVIN:0.0573607)S3:0.0112169,((sp|Q3UUF8|AN34B_MOUSE:0.0307994,tr|D3ZKP1|D3ZKP1_RAT:0.0196719)S4:0.131512,(sp|A5PLL1|AN34B_HUMAN:0.00389278,(tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO:0.0110283,tr|A0A2I3THT9|A0A2I3THT9_PANTR:0.00379641)S5:0.00218662)S6:0.0418621)S7:0.011351)S8:0.0799673)S9:0.0918454)S10:0.0903582)S11:0.109288)S12:0;\n",
      "['tr|W5NLW9|W5NLW9_LEPOC', 'tr|H2MS52|H2MS52_ORYLA', 'tr|A2BGQ0|A2BGQ0_DANRE', 'tr|A0A6I8QDM6|A0A6I8QDM6_XENTR', 'tr|A0A1L1RSA7|A0A1L1RSA7_CHICK', 'tr|A0A5F8H9I8|A0A5F8H9I8_MONDO', 'tr|J9JHK4|J9JHK4_CANLF', 'tr|F1ME24|F1ME24_BOVIN', 'sp|Q3UUF8|AN34B_MOUSE', 'tr|D3ZKP1|D3ZKP1_RAT', 'sp|A5PLL1|AN34B_HUMAN', 'tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO', 'tr|A0A2I3THT9|A0A2I3THT9_PANTR']\n",
      "-  1 HOGs are inferred at the level  LEPOC_ORYLA_DANRE_MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR : [['D3ZKP1_RAT', 'J9JHK4_CANLF', 'F1ME24_BOVIN', 'H2MS52_ORYLA', 'A0A2I2ZNV7_GORGO', 'A0A1L1RSA7_CHICK', 'A0A5F8H9I8_MONDO', 'AN34B_MOUSE', 'A0A2I3THT9_PANTR', 'A2BGQ0_DANRE', 'AN34B_HUMAN', 'W5NLW9_LEPOC', 'A0A6I8QDM6_XENTR']]\n",
      "By merging  [['hog105', 'hog123']]\n",
      "\n",
      "***************\n",
      " Finding hogs for the taxonomic level: NEMVE_LEPOC_ORYLA_DANRE_MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR \n",
      "\n",
      "working on node NEMVE_LEPOC_ORYLA_DANRE_MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR with 2 children.\n",
      "sub msa for internal node LEPOC_ORYLA_DANRE_MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR is read from dic.\n",
      "there are  2 subHOGs lower of this level: ['hog125', 'hog124'] [['A7SYH3_NEMVE'], ['D3ZKP1_RAT', 'J9JHK4_CANLF', 'F1ME24_BOVIN', 'H2MS52_ORYLA', 'A0A2I2ZNV7_GORGO', 'A0A1L1RSA7_CHICK', 'A0A5F8H9I8_MONDO', 'AN34B_MOUSE', 'A0A2I3THT9_PANTR', 'A2BGQ0_DANRE', 'AN34B_HUMAN', 'W5NLW9_LEPOC', 'A0A6I8QDM6_XENTR']]\n",
      "We want to infer subHOGs at this level,i.e. merge few of them.\n",
      "2 msas are merged into one with the length of  14 598\n",
      "All subHOGs are merged, merged msa is with length of 14 598 .\n",
      "Gene tree is infered with length of 14 .\n",
      "Overlap speciation is done for internal nodes of gene tree, as following:\n",
      "(tr|A7SYH3|A7SYH3_NEMVE:0.993435,((tr|W5NLW9|W5NLW9_LEPOC:0.0633128,(tr|H2MS52|H2MS52_ORYLA:0.389069,tr|A2BGQ0|A2BGQ0_DANRE:0.255173)S1:0.15986)S2:0.142653,(tr|A0A6I8QDM6|A0A6I8QDM6_XENTR:0.198877,(tr|A0A1L1RSA7|A0A1L1RSA7_CHICK:0.339338,(tr|A0A5F8H9I8|A0A5F8H9I8_MONDO:0.133588,((tr|J9JHK4|J9JHK4_CANLF:0.0760788,tr|F1ME24|F1ME24_BOVIN:0.056886)S3:0.0110383,((sp|Q3UUF8|AN34B_MOUSE:0.0305661,tr|D3ZKP1|D3ZKP1_RAT:0.0196156)S4:0.130382,(sp|A5PLL1|AN34B_HUMAN:0.00386244,(tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO:0.0109462,tr|A0A2I3THT9|A0A2I3THT9_PANTR:0.00376785)S5:0.00217061)S6:0.0415973)S7:0.0113146)S8:0.0795149)S9:0.08971)S10:0.089355)S11:0.069052)S12:0.993435)S13:0;\n",
      "['tr|A7SYH3|A7SYH3_NEMVE', 'tr|W5NLW9|W5NLW9_LEPOC', 'tr|H2MS52|H2MS52_ORYLA', 'tr|A2BGQ0|A2BGQ0_DANRE', 'tr|A0A6I8QDM6|A0A6I8QDM6_XENTR', 'tr|A0A1L1RSA7|A0A1L1RSA7_CHICK', 'tr|A0A5F8H9I8|A0A5F8H9I8_MONDO', 'tr|J9JHK4|J9JHK4_CANLF', 'tr|F1ME24|F1ME24_BOVIN', 'sp|Q3UUF8|AN34B_MOUSE', 'tr|D3ZKP1|D3ZKP1_RAT', 'sp|A5PLL1|AN34B_HUMAN', 'tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO', 'tr|A0A2I3THT9|A0A2I3THT9_PANTR']\n",
      "-  1 HOGs are inferred at the level  NEMVE_LEPOC_ORYLA_DANRE_MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR : [['A7SYH3_NEMVE', 'D3ZKP1_RAT', 'J9JHK4_CANLF', 'F1ME24_BOVIN', 'H2MS52_ORYLA', 'A0A2I2ZNV7_GORGO', 'A0A1L1RSA7_CHICK', 'A0A5F8H9I8_MONDO', 'AN34B_MOUSE', 'A0A2I3THT9_PANTR', 'A2BGQ0_DANRE', 'AN34B_HUMAN', 'W5NLW9_LEPOC', 'A0A6I8QDM6_XENTR']]\n",
      "By merging  [['hog125', 'hog124']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for rhogid_num in rhogid_num_list[39:40]:  # 39 169\n",
    "\n",
    "    print(\"\\n\"+\"=\"*50+\"\\n\",\"Working on root hog:\", rhogid_num,\"\\n\") \n",
    "\n",
    "    prot_address = address_out_hog+\"HOG_\"+str(rhogid_num)+\".fa\"\n",
    "    rhog_i = list(SeqIO.parse(prot_address, \"fasta\")) \n",
    "    print(\"number of proteins in the rHOG\", len(rhog_i))\n",
    "    (species_tree) = read_species_tree(tree_address)\n",
    "\n",
    "    (species_tree, species_names_rhog, prot_names_rhog) = prepare_species_tree(rhog_i, species_tree)\n",
    "\n",
    "    print(\"\\n species_names_rhog \",set(species_names_rhog))\n",
    "\n",
    "    #species_tree.write()\n",
    "    print(species_tree.write())\n",
    "    print(species_tree)\n",
    "\n",
    "\n",
    "    #subHOG_all = []\n",
    "    dic_sub_hogs = {}\n",
    "    #parlog_set_rhog = set()\n",
    "\n",
    "    # finding hogs at each level of species tree (from leaves to root, bottom up)    \n",
    "    for node_species_tree in species_tree.traverse(strategy = \"postorder\"):\n",
    "        #dic_sub_hogs[node_species_tree.name] = []\n",
    "        #dic_sub_msas[node_species_tree.name] = []\n",
    "        if node_species_tree.is_leaf() : \n",
    "            continue\n",
    "        print(\"\\n\"+\"*\"*15+\"\\n\",\"Finding hogs for the taxonomic level:\", node_species_tree.name,\"\\n\")\n",
    "        dic_sub_msas=[]\n",
    "        (dic_sub_hogs) = infer_HOG_thisLevel(node_species_tree, rhog_i, species_names_rhog, dic_sub_hogs, rhogid_num)\n",
    "        #print(\"&& ** %%\",dic_sub_hogs)\n",
    "        #(subHOG_thisLevel, paralog_set_thisLevel, dic_sub_msas)=  infer_HOG_thisLevel(node_species_tree, rhog_i, species_names_rhog, dic_sub_msas, rhogid_num)\n",
    "\n",
    "\n",
    "    #write_ortho = write_ortholog_rhog(parlog_set_rhog, prot_names_rhog, address_orhto_pair_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c8168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "ac9fa1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= dic_sub_hogs[\"LEPOC_ORYLA_DANRE_MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "347bae47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subhog: LEPOC_ORYLA_DANRE_MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR:\n",
      "  clade: LEPOC_ORYLA_DANRE with 1\n",
      "  subhog: LEPOC_ORYLA_DANRE:\n",
      "    clade: LEPOC with 1\n",
      "    clade: ORYLA_DANRE with 1\n",
      "    subhog: ORYLA_DANRE:\n",
      "      clade: DANRE with 1\n",
      "      clade: ORYLA with 1\n",
      "  clade: MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR with 1\n",
      "  subhog: MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR:\n",
      "    clade: MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK with 1\n",
      "    subhog: MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK:\n",
      "      clade: CHICK with 1\n",
      "      clade: MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO with 1\n",
      "      subhog: MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO:\n",
      "        clade: MONDO with 1\n",
      "        clade: MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN with 1\n",
      "        subhog: MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN:\n",
      "          clade: CANLF_BOVIN with 1\n",
      "          subhog: CANLF_BOVIN:\n",
      "            clade: BOVIN with 1\n",
      "            clade: CANLF with 1\n",
      "          clade: MOUSE_RATNO_GORGO_HUMAN_PANTR with 1\n",
      "          subhog: MOUSE_RATNO_GORGO_HUMAN_PANTR:\n",
      "            clade: GORGO_HUMAN_PANTR with 1\n",
      "            subhog: GORGO_HUMAN_PANTR:\n",
      "              clade: GORGO with 1\n",
      "              clade: HUMAN with 1\n",
      "              clade: PANTR with 1\n",
      "            clade: MOUSE_RATNO with 1\n",
      "            subhog: MOUSE_RATNO:\n",
      "              clade: MOUSE with 1\n",
      "              clade: RATNO with 1\n",
      "    clade: XENTR with 1\n",
      "<?xml version=\"1.0\" ?>\n",
      "<orthologGroup>\n",
      "  <orthologGroup>\n",
      "    <geneRef id=\"tr|W5NLW9|W5NLW9_LEPOC\"/>\n",
      "    <orthologGroup>\n",
      "      <geneRef id=\"tr|A2BGQ0|A2BGQ0_DANRE\"/>\n",
      "      <geneRef id=\"tr|H2MS52|H2MS52_ORYLA\"/>\n",
      "    </orthologGroup>\n",
      "  </orthologGroup>\n",
      "  <orthologGroup>\n",
      "    <orthologGroup>\n",
      "      <geneRef id=\"tr|A0A1L1RSA7|A0A1L1RSA7_CHICK\"/>\n",
      "      <orthologGroup>\n",
      "        <geneRef id=\"tr|A0A5F8H9I8|A0A5F8H9I8_MONDO\"/>\n",
      "        <orthologGroup>\n",
      "          <orthologGroup>\n",
      "            <geneRef id=\"tr|F1ME24|F1ME24_BOVIN\"/>\n",
      "            <geneRef id=\"tr|J9JHK4|J9JHK4_CANLF\"/>\n",
      "          </orthologGroup>\n",
      "          <orthologGroup>\n",
      "            <orthologGroup>\n",
      "              <geneRef id=\"tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO\"/>\n",
      "              <geneRef id=\"sp|A5PLL1|AN34B_HUMAN\"/>\n",
      "              <geneRef id=\"tr|A0A2I3THT9|A0A2I3THT9_PANTR\"/>\n",
      "            </orthologGroup>\n",
      "            <orthologGroup>\n",
      "              <geneRef id=\"sp|Q3UUF8|AN34B_MOUSE\"/>\n",
      "              <geneRef id=\"tr|D3ZKP1|D3ZKP1_RAT\"/>\n",
      "            </orthologGroup>\n",
      "          </orthologGroup>\n",
      "        </orthologGroup>\n",
      "      </orthologGroup>\n",
      "    </orthologGroup>\n",
      "    <geneRef id=\"tr|A0A6I8QDM6|A0A6I8QDM6_XENTR\"/>\n",
      "  </orthologGroup>\n",
      "</orthologGroup>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = a.to_orthoxml()\n",
    "rough_string = ET.tostring(res, 'utf-8')\n",
    "reparsed = minidom.parseString(rough_string)\n",
    "print(reparsed.toprettyxml(indent=\"  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e413c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39be91b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d043fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (query_species_names, query_prot_records_species) = parse_proteome(list_oma_speices)   \n",
    "# query_prot_records_species = add_species_name(query_prot_records_species,query_species_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a23fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "aa35c91c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeqRecord(seq=Seq('MHRRARRMPMRPRRSKRVRNRYTMGTFALHGLTHRLPSASLQTTAARHPDVTQF...HYR'), id='sp|E2FZM4|SOCA_MYCTU', name='sp|E2FZM4|SOCA_MYCTU', description='sp|E2FZM4|SOCA_MYCTU Uncharacterized protein SocA OS=Mycobacterium tuberculosis (strain ATCC 25618 / H37Rv) OX=83332 GN=socA PE=2 SV=1|species|UP000001584_83332', dbxrefs=[])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_prot_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc6cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "82ca59d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<orthoXML><database name=\"human\" NCBITaxId=\"63\"><species name=\"UP000008783_418459\" NCBITaxId=\"1\"><gene id=\"0\" protId=\"tr|E3JPS1|E3JPS1_PUCGT\" /><gene id=\"1\" protId=\"tr|E3JPS2|E3JPS2_PUCGT\" /><gene id=\"2\" protId=\"tr|E3JPS3|E3JPS3_PUCGT\" /></species><species name=\"UP000002485_284812\" NCBITaxId=\"1\"><gene id=\"3\" protId=\"sp|A0ZWU1|EAF1_SCHPO\" /><gene id=\"4\" protId=\"sp|A6X969|YI9B_SCHPO\" /><gene id=\"5\" protId=\"sp|A6X970|YF64_SCHPO\" /></species><species name=\"UP000001584_83332\" NCBITaxId=\"1\"><gene id=\"6\" protId=\"sp|A0A089QKZ7|Y155A_MYCTU\" /><gene id=\"7\" protId=\"sp|A0A089QRB9|MSL3_MYCTU\" /><gene id=\"8\" protId=\"sp|E2FZM4|SOCA_MYCTU\" /></species></database></orthoXML>\n",
      "\n",
      " \n",
      " \n",
      "<?xml version=\"1.0\" ?>\n",
      "<orthoXML>\n",
      "  <database name=\"human\" NCBITaxId=\"63\">\n",
      "    <species name=\"UP000008783_418459\" NCBITaxId=\"1\">\n",
      "      <gene id=\"0\" protId=\"tr|E3JPS1|E3JPS1_PUCGT\"/>\n",
      "      <gene id=\"1\" protId=\"tr|E3JPS2|E3JPS2_PUCGT\"/>\n",
      "      <gene id=\"2\" protId=\"tr|E3JPS3|E3JPS3_PUCGT\"/>\n",
      "    </species>\n",
      "    <species name=\"UP000002485_284812\" NCBITaxId=\"1\">\n",
      "      <gene id=\"3\" protId=\"sp|A0ZWU1|EAF1_SCHPO\"/>\n",
      "      <gene id=\"4\" protId=\"sp|A6X969|YI9B_SCHPO\"/>\n",
      "      <gene id=\"5\" protId=\"sp|A6X970|YF64_SCHPO\"/>\n",
      "    </species>\n",
      "    <species name=\"UP000001584_83332\" NCBITaxId=\"1\">\n",
      "      <gene id=\"6\" protId=\"sp|A0A089QKZ7|Y155A_MYCTU\"/>\n",
      "      <gene id=\"7\" protId=\"sp|A0A089QRB9|MSL3_MYCTU\"/>\n",
      "      <gene id=\"8\" protId=\"sp|E2FZM4|SOCA_MYCTU\"/>\n",
      "    </species>\n",
      "  </database>\n",
      "</orthoXML>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "ortho_file  = ET.Element(\"orthoXML\") # xmlns=\\\"http://orthoXML.org/2011/\\\"\n",
    "database    = ET.SubElement(ortho_file, \"database\", attrib={\"name\":\"human\", \"NCBITaxId\" :\"63\"}) # \n",
    "\n",
    "\n",
    "gene_counter =0\n",
    "gene_id_name = {}\n",
    "\n",
    "for species_i in range(len(query_species_names[12:15])):\n",
    "    species_name = query_species_names[species_i]\n",
    "    species = ET.SubElement(database, \"species\", attrib={\"name\":species_name, \"NCBITaxId\":\"1\"})\n",
    "    query_prot_records =  query_prot_records_species[species_i]\n",
    "    for gene_i in range(len(query_prot_records[12:15])):\n",
    "        query_prot_record= query_prot_records[gene_i]\n",
    "        \n",
    "        gene = ET.SubElement(species, \"gene\", attrib={\"id\":str(gene_counter), \"protId\":query_prot_record.id})\n",
    "        \n",
    "        gene_id_name[query_prot_record.id]= gene_counter\n",
    "        \n",
    "        gene_counter += 1\n",
    "\n",
    "#species.protId\n",
    "        \n",
    "ET.dump(ortho_file)  \n",
    "\n",
    "#ET.tostring(ortho_file, xml_declaration=True, encoding='utf8').decode()\n",
    "et = ET.ElementTree(ortho_file)\n",
    "et.write(\"output1.xml\")\n",
    "# # .write('output.xml')\n",
    "# !cat \"output1.xml\"\n",
    "\n",
    "print(\"\\n \\n \")\n",
    "\n",
    "rough_string = ET.tostring(ortho_file, 'utf-8')\n",
    "reparsed = minidom.parseString(rough_string)\n",
    "print(reparsed.toprettyxml(indent=\"  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ba5d2397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sp|A5PLL1|AN34B_HUMAN',\n",
       " 'sp|Q3UUF8|AN34B_MOUSE',\n",
       " 'tr|A0A1L1RSA7|A0A1L1RSA7_CHICK',\n",
       " 'tr|A0A2I2ZNV7|A0A2I2ZNV7_GORGO',\n",
       " 'tr|A0A2I3THT9|A0A2I3THT9_PANTR',\n",
       " 'tr|A0A5F8H9I8|A0A5F8H9I8_MONDO',\n",
       " 'tr|A0A6I8QDM6|A0A6I8QDM6_XENTR',\n",
       " 'tr|A2BGQ0|A2BGQ0_DANRE',\n",
       " 'tr|A7SYH3|A7SYH3_NEMVE',\n",
       " 'tr|D3ZKP1|D3ZKP1_RAT',\n",
       " 'tr|F1ME24|F1ME24_BOVIN',\n",
       " 'tr|H2MS52|H2MS52_ORYLA',\n",
       " 'tr|J9JHK4|J9JHK4_CANLF',\n",
       " 'tr|W5NLW9|W5NLW9_LEPOC'}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sub_hogs[\"NEMVE_LEPOC_ORYLA_DANRE_MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR\"][0]._members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "34dacd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.HOG at 0x7fe9be2bfa00>]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sub_hogs[\"NEMVE_LEPOC_ORYLA_DANRE_MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR\"]\n",
    "\n",
    "# class object, __reper__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f26f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a48521",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_sub_hogs[\"NEMVE_LEPOC_ORYLA_DANRE_MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN_MONDO_CHICK_XENTR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "e6320828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<orthoXML><database name=\"human\" NCBITaxId=\"63\"><species name=\"UP000008783_418459\" NCBITaxId=\"1\"><gene id=\"1\" protId=\"tr|E3JPS1|E3JPS1_PUCGT\" /><gene id=\"1\" protId=\"tr|E3JPS2|E3JPS2_PUCGT\" /><gene id=\"1\" protId=\"tr|E3JPS3|E3JPS3_PUCGT\" /><gene id=\"1\" protId=\"tr|E3JPS4|E3JPS4_PUCGT\" /><gene id=\"1\" protId=\"tr|E3JPS5|E3JPS5_PUCGT\" /><gene id=\"1\" protId=\"tr|E3JPS7|E3JPS7_PUCGT\" /><gene id=\"1\" protId=\"tr|E3JPS8|E3JPS8_PUCGT\" /><gene id=\"1\" protId=\"tr|E3JPS9|E3JPS9_PUCGT\" /><gene id=\"1\" protId=\"tr|E3JPT0|E3JPT0_PUCGT\" /><gene id=\"1\" protId=\"tr|E3JPT1|E3JPT1_PUCGT\" /></species><species name=\"UP000002485_284812\" NCBITaxId=\"1\"><gene id=\"1\" protId=\"sp|A0ZWU1|EAF1_SCHPO\" /><gene id=\"1\" protId=\"sp|A6X969|YI9B_SCHPO\" /><gene id=\"1\" protId=\"sp|A6X970|YF64_SCHPO\" /><gene id=\"1\" protId=\"sp|A6X972|GON7_SCHPO\" /><gene id=\"1\" protId=\"sp|A6X974|YFQ8_SCHPO\" /><gene id=\"1\" protId=\"sp|A6X977|YEID_SCHPO\" /><gene id=\"1\" protId=\"sp|A6X978|YF12_SCHPO\" /><gene id=\"1\" protId=\"sp|A6X980|SFC7_SCHPO\" /><gene id=\"1\" protId=\"sp|A6X982|YOGD_SCHPO\" /><gene id=\"1\" protId=\"sp|A6X992|YCGL_SCHPO\" /></species><species name=\"UP000001584_83332\" NCBITaxId=\"1\"><gene id=\"1\" protId=\"sp|A0A089QKZ7|Y155A_MYCTU\" /><gene id=\"1\" protId=\"sp|A0A089QRB9|MSL3_MYCTU\" /><gene id=\"1\" protId=\"sp|E2FZM4|SOCA_MYCTU\" /><gene id=\"1\" protId=\"sp|E2FZM5|SOCB_MYCTU\" /><gene id=\"1\" protId=\"tr|I6WX95|I6WX95_MYCTU\" /><gene id=\"1\" protId=\"tr|I6WXK4|I6WXK4_MYCTU\" /><gene id=\"1\" protId=\"tr|I6WXK8|I6WXK8_MYCTU\" /><gene id=\"1\" protId=\"sp|I6WXS6|VPB51_MYCTU\" /><gene id=\"1\" protId=\"tr|I6WY86|I6WY86_MYCTU\" /><gene id=\"1\" protId=\"tr|I6WYT7|I6WYT7_MYCTU\" /></species></database></orthoXML>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "ortho_file  = ET.Element(\"orthoXML\") # xmlns=\\\"http://orthoXML.org/2011/\\\"\n",
    "database    = ET.SubElement(ortho_file, \"database\", attrib={\"name\":\"human\", \"NCBITaxId\" :\"63\"}) # \n",
    "\n",
    "for species_i in range(len(query_species_names[12:15])):\n",
    "    species_name = query_species_names[species_i]\n",
    "    species = ET.Element(\"species\", attrib={\"name\":species_name, \"NCBITaxId\":\"1\"})\n",
    "    database.append(species)\n",
    "    \n",
    "    \n",
    "    query_prot_records =  query_prot_records_species[species_i]\n",
    "    for query_prot_record in query_prot_records[:10]:\n",
    "        gene = ET.SubElement(species, \"gene\", attrib={\"id\":\"1\", \"protId\":query_prot_record.id})\n",
    "\n",
    "#species.protId\n",
    "        \n",
    "ET.dump(ortho_file)  \n",
    "\n",
    "#ET.tostring(ortho_file, xml_declaration=True, encoding='utf8').decode()\n",
    "et = ET.ElementTree(ortho_file)\n",
    "et.write(\"output1.xml\")\n",
    "# # .write('output.xml')\n",
    "# !cat \"output1.xml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ffea3e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" ?>\n",
      "<orthoXML>\n",
      "  <database name=\"human\" NCBITaxId=\"63\">\n",
      "    <species name=\"UP000008783_418459\" NCBITaxId=\"1\">\n",
      "      <gene id=\"1\" protId=\"tr|E3JPS1|E3JPS1_PUCGT\"/>\n",
      "      <gene id=\"1\" protId=\"tr|E3JPS2|E3JPS2_PUCGT\"/>\n",
      "      <gene id=\"1\" protId=\"tr|E3JPS3|E3JPS3_PUCGT\"/>\n",
      "      <gene id=\"1\" protId=\"tr|E3JPS4|E3JPS4_PUCGT\"/>\n",
      "      <gene id=\"1\" protId=\"tr|E3JPS5|E3JPS5_PUCGT\"/>\n",
      "      <gene id=\"1\" protId=\"tr|E3JPS7|E3JPS7_PUCGT\"/>\n",
      "      <gene id=\"1\" protId=\"tr|E3JPS8|E3JPS8_PUCGT\"/>\n",
      "      <gene id=\"1\" protId=\"tr|E3JPS9|E3JPS9_PUCGT\"/>\n",
      "      <gene id=\"1\" protId=\"tr|E3JPT0|E3JPT0_PUCGT\"/>\n",
      "      <gene id=\"1\" protId=\"tr|E3JPT1|E3JPT1_PUCGT\"/>\n",
      "    </species>\n",
      "    <species name=\"UP000002485_284812\" NCBITaxId=\"1\">\n",
      "      <gene id=\"1\" protId=\"sp|A0ZWU1|EAF1_SCHPO\"/>\n",
      "      <gene id=\"1\" protId=\"sp|A6X969|YI9B_SCHPO\"/>\n",
      "      <gene id=\"1\" protId=\"sp|A6X970|YF64_SCHPO\"/>\n",
      "      <gene id=\"1\" protId=\"sp|A6X972|GON7_SCHPO\"/>\n",
      "      <gene id=\"1\" protId=\"sp|A6X974|YFQ8_SCHPO\"/>\n",
      "      <gene id=\"1\" protId=\"sp|A6X977|YEID_SCHPO\"/>\n",
      "      <gene id=\"1\" protId=\"sp|A6X978|YF12_SCHPO\"/>\n",
      "      <gene id=\"1\" protId=\"sp|A6X980|SFC7_SCHPO\"/>\n",
      "      <gene id=\"1\" protId=\"sp|A6X982|YOGD_SCHPO\"/>\n",
      "      <gene id=\"1\" protId=\"sp|A6X992|YCGL_SCHPO\"/>\n",
      "    </species>\n",
      "    <species name=\"UP000001584_83332\" NCBITaxId=\"1\">\n",
      "      <gene id=\"1\" protId=\"sp|A0A089QKZ7|Y155A_MYCTU\"/>\n",
      "      <gene id=\"1\" protId=\"sp|A0A089QRB9|MSL3_MYCTU\"/>\n",
      "      <gene id=\"1\" protId=\"sp|E2FZM4|SOCA_MYCTU\"/>\n",
      "      <gene id=\"1\" protId=\"sp|E2FZM5|SOCB_MYCTU\"/>\n",
      "      <gene id=\"1\" protId=\"tr|I6WX95|I6WX95_MYCTU\"/>\n",
      "      <gene id=\"1\" protId=\"tr|I6WXK4|I6WXK4_MYCTU\"/>\n",
      "      <gene id=\"1\" protId=\"tr|I6WXK8|I6WXK8_MYCTU\"/>\n",
      "      <gene id=\"1\" protId=\"sp|I6WXS6|VPB51_MYCTU\"/>\n",
      "      <gene id=\"1\" protId=\"tr|I6WY86|I6WY86_MYCTU\"/>\n",
      "      <gene id=\"1\" protId=\"tr|I6WYT7|I6WYT7_MYCTU\"/>\n",
      "    </species>\n",
      "  </database>\n",
      "</orthoXML>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rough_string = ET.tostring(ortho_file, 'utf-8')\n",
    "reparsed = minidom.parseString(rough_string)\n",
    "print(reparsed.toprettyxml(indent=\"  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "480f910f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<orthoXML><database name=\"human\" NCBITaxId=\"63\"><groups><orthologGroup><geneRef id=\"8783511\" /><geneRef id=\"8783512\" /><orthologGroup><geneRef id=\"8783511\" /></orthologGroup></orthologGroup><orthologGroup><geneRef id=\"22\" /><geneRef id=\"33\" /><geneRef id=\"44\" /></orthologGroup></groups></database></orthoXML>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "#<groups>\n",
    "#    <orthologGroup id=\"HOG:B0568884\">\n",
    "    \n",
    "ortho_file  = ET.Element(\"orthoXML\") # xmlns=\\\"http://orthoXML.org/2011/\\\"\n",
    "database    = ET.SubElement(ortho_file, \"database\", attrib={\"name\":\"human\", \"NCBITaxId\" :\"63\"}) # \n",
    "\n",
    "groups = ET.SubElement(database, \"groups\")\n",
    "\n",
    "orthologGroup = ET.SubElement(groups, \"orthologGroup\")# , attrib={\"name\":}\n",
    "#<property name=\"TaxRange\" value=\"Neopterygii\"/>\n",
    "\n",
    "geneRef = ET.SubElement(orthologGroup, \"geneRef\", attrib={\"id\":\"8783511\"}) # <geneRef id=\"8783511\"/>\n",
    "geneRef = ET.SubElement(orthologGroup, \"geneRef\", attrib={\"id\":\"8783512\"}) # <geneRef id=\"8783511\"/>\n",
    "orthologGroup = ET.SubElement(orthologGroup, \"orthologGroup\")# , attrib={\"name\":}\n",
    "geneRef = ET.SubElement(orthologGroup, \"geneRef\", attrib={\"id\":\"8783511\"}) # <geneRef id=\"8783511\"/>\n",
    "\n",
    "\n",
    "orthologGroup = ET.SubElement(groups, \"orthologGroup\")# , attrib={\"name\":}\n",
    "#<property name=\"TaxRange\" value=\"Neopterygii\"/>\n",
    "geneRef = ET.SubElement(orthologGroup, \"geneRef\", attrib={\"id\":\"22\"}) # <geneRef id=\"8783511\"/>\n",
    "geneRef = ET.SubElement(orthologGroup, \"geneRef\", attrib={\"id\":\"33\"}) # <geneRef id=\"8783511\"/>\n",
    "geneRef = ET.SubElement(orthologGroup, \"geneRef\", attrib={\"id\":\"44\"}) # <geneRef id=\"8783511\"/>\n",
    "\n",
    "# orthologGroup = ET.SubElement(groups, \"orthologGroup\", attrib={\"name\":})\n",
    "\n",
    "#property_ = ET.SubElement(orthologGroup, \"property\", attrib={\"name\":\"TaxRange\"})\n",
    "#property\n",
    "\n",
    "#     <orthologGroup id=\"HOG:B0568884\">\n",
    "#       <property name=\"TaxRange\" value=\"Gnathostomata\"/>\n",
    "\n",
    "\n",
    "#for species_i in range(len(query_species_names[12:15])):\n",
    "#     species_name = query_species_names[species_i]\n",
    "#     species = ET.SubElement(database, \"species\", attrib={\"name\":species_name, \"NCBITaxId\":\"1\"})\n",
    "#     query_prot_records =  query_prot_records_species[species_i]\n",
    "#     for query_prot_record in query_prot_records[:10]:\n",
    "#         gene = ET.SubElement(species, \"gene\", attrib={\"id\":\"1\", \"protId\":species_name})\n",
    "\n",
    "\n",
    "# for species_i in range(len(query_species_names[12:15])):\n",
    "#     species_name = query_species_names[species_i]\n",
    "#     species = ET.SubElement(database, \"species\", attrib={\"name\":species_name, \"NCBITaxId\":\"1\"})\n",
    "#     query_prot_records =  query_prot_records_species[species_i]\n",
    "#     for query_prot_record in query_prot_records[:10]:\n",
    "#         gene = ET.SubElement(species, \"gene\", attrib={\"id\":\"1\", \"protId\":species_name})\n",
    "\n",
    "ET.dump(ortho_file)  \n",
    "\n",
    "#ET.tostring(ortho_file, xml_declaration=True, encoding='utf8').decode()\n",
    "et = ET.ElementTree(ortho_file)\n",
    "#et.write(\"output1.xml\")\n",
    "# # .write('output.xml')\n",
    "# !cat \"output1.xml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f0974f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" ?>\n",
      "<orthoXML>\n",
      "  <database name=\"human\" NCBITaxId=\"63\">\n",
      "    <groups>\n",
      "      <orthologGroup>\n",
      "        <geneRef id=\"8783511\"/>\n",
      "        <geneRef id=\"8783512\"/>\n",
      "        <orthologGroup>\n",
      "          <geneRef id=\"8783511\"/>\n",
      "        </orthologGroup>\n",
      "      </orthologGroup>\n",
      "      <orthologGroup>\n",
      "        <geneRef id=\"22\"/>\n",
      "        <geneRef id=\"33\"/>\n",
      "        <geneRef id=\"44\"/>\n",
      "      </orthologGroup>\n",
      "    </groups>\n",
      "  </database>\n",
      "</orthoXML>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rough_string = ET.tostring(ortho_file, 'utf-8')\n",
    "reparsed = minidom.parseString(rough_string)\n",
    "print(reparsed.toprettyxml(indent=\"  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2c009f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_id_name[\"sp|A0A089QKZ7|Y155A_MYCTU\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107a5b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73999fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c17a9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in root.iter('species'):\n",
    "    print(movie.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c41557a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_elementtree._element_iterator at 0x7fe9be2c0590>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.iter(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8188ceb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'canonicalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3446039/3355184128.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxml_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"<root>...</root>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanonicalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'canonicalize' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#import lxml.etree as etree\n",
    "\n",
    "xml_data = \"<root>...</root>\"\n",
    "\n",
    "print(canonicalize(xml_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17643072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# modifying tree\n",
    "for rank in root.iter('rank'):\n",
    "    new_rank = int(rank.text) + 1\n",
    "    rank.text = str(new_rank)\n",
    "    rank.set('updated', 'yes')\n",
    "\n",
    "tree.write('output.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    ">>> b = ET.SubElement(a, 'b')\n",
    ">>> c = ET.SubElement(a, 'c')\n",
    ">>> d = ET.SubElement(c, 'd')\n",
    ">>> ET.dump(a)\n",
    "<a><b /><c><d /></c></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11268c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<genes><gene id=\"1\" protId=\"1\" /></genes>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "gene_name = ET.Element('genes')\n",
    "id_val=1\n",
    "protId=1\n",
    "b = ET.SubElement(gene_name, \"gene id=\\\"\"+str(id_val)+\"\\\" protId=\\\"\"+str(protId)+\"\\\"\") #<gene id=\"9361504\" protId=\"KRYMA19363\"/>\n",
    "ET.dump(gene_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b455f2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<orthoXML file1><species /></orthoXML file1>\n",
      "\n",
      " \n",
      " \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Type 'xml.etree.ElementTree.Element' cannot be serialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3446039/3256700718.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n \\n \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mortho_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxml_declaration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# print(\"write it\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree.tostring\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Type 'xml.etree.ElementTree.Element' cannot be serialized."
     ]
    }
   ],
   "source": [
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "ortho_file = ET.Element(\"orthoXML file1\") #  xmlns=\\\"http://orthoXML.org/2011/\\\n",
    "species = ET.SubElement(ortho_file, \"species\")\n",
    "\n",
    "ET.dump(ortho_file)  \n",
    "print(\"\\n \\n \")\n",
    "\n",
    "print(etree.tostring(ortho_file, xml_declaration=True, encoding='utf8', pretty_print=True).decode())\n",
    "\n",
    "# print(\"write it\")\n",
    "# et = ET.ElementTree(ortho_file)\n",
    "# et.write(\"output_1.xml\")\n",
    "# !cat \"output_1.xml\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70edb091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6af90822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2dfbd624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<orthoXML xmlns=\"http://orthoXML.org/2011/\" />"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "92db5eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#print(ET.tostring(ortho_file, xml_declaration=True, encoding='utf8').decode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c7cb5400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<orthoXML><species>sdsd </species></orthoXML>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "ortho_file = ET.Element(\"orthoXML\") # xmlns=\\\"http://orthoXML.org/2011/\\\"\n",
    "species = ET.SubElement(ortho_file, \"species\")\n",
    "species.text= \"sdsd \"\n",
    "\n",
    "\n",
    "#for species_name in query_species_names[:5]:\n",
    "    \n",
    "#     species = ET.SubElement(ortho_file,\"species name=\\\"\"+species_name+\"\\\"\")\n",
    "\n",
    "    #<species name=\"Kryptolebias marmoratus\" NCBITaxId=\"37003\">\n",
    "ET.dump(ortho_file)  \n",
    "\n",
    "#ET.tostring(ortho_file, xml_declaration=True, encoding='utf8').decode()\n",
    "\n",
    "# et = ET.ElementTree(ortho_file)\n",
    "# et.write(\"output1.xml\")\n",
    "# # .write('output.xml')\n",
    "# !cat \"output1.xml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b31acfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5975e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = etree.Element(\"OrthoXML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc01f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3979c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree\n",
    "from xml.dom import minidom\n",
    "\n",
    "def prettify(elem):\n",
    "    \"\"\"Return a pretty-printed XML string for the Element.\n",
    "    \"\"\"\n",
    "    rough_string = ElementTree.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "54383712",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'ElementTree' has no attribute 'tostring'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3446039/567379941.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprettify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0met\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3446039/915500172.py\u001b[0m in \u001b[0;36mprettify\u001b[0;34m(elem)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"Return a pretty-printed XML string for the Element.\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mrough_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mreparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminidom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrough_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoprettyxml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"  \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'ElementTree' has no attribute 'tostring'"
     ]
    }
   ],
   "source": [
    "print(prettify(et))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f0ccef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "from xml.etree.ElementTree import Element\n",
    "from xml.etree.ElementTree import ElementTree\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "62fb748d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<OrthoXML />'\n"
     ]
    }
   ],
   "source": [
    "root =Element(\"OrthoXML\")\n",
    "tree=ElementTree(root)\n",
    "print(etree.tostring(root))\n",
    "\n",
    "name=Element(name)\n",
    "\n",
    "root.append(name)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df42fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
