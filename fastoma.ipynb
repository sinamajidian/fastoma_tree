{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0927039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "import numpy as np\n",
    "from sys import argv\n",
    "import pyoma.browser.db as db\n",
    "import pyoma.browser.models as mod\n",
    "import zoo.wrappers.aligners.mafft as mafft\n",
    "import zoo.wrappers.treebuilders.fasttree as fasttree\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from Bio.Alphabet import IUPAC, SingleLetterAlphabet\n",
    "from Bio.Seq import Seq, UnknownSeq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from collections import defaultdict\n",
    "\n",
    "import pickle\n",
    "    \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from datetime import datetime\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "import ast\n",
    "#  for development \n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "datasets_address= \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/fastoma/archive/\"\n",
    "oma_database_address = datasets_address + \"OmaServer.h5\"\n",
    "hog_og_map_address = datasets_address + \"hog_og_map.dic\"\n",
    "omaID_address = datasets_address+\"oma-species.txt\"\n",
    "bird6ID_address = datasets_address+\"info.tsv\"\n",
    "\n",
    "\n",
    "# very small\n",
    "#project_folder = \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/fastoma/v3a/ST/f4_100S/\" \n",
    "\n",
    "project_folder = \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/fastoma/v3a/A/f7_2kA/\" \n",
    "\n",
    "#project_folder = argv[1]\n",
    "\n",
    "\n",
    "# PANPA.fa  PANPA.hogmap\n",
    "# The species name of query is the name of the file; \n",
    "#  argv[2] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bfe573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_oma(oma_database_address, hog_og_map_address):\n",
    "    \n",
    "    ############### Parsing OMA db ####################\n",
    "    ###################################################\n",
    "\n",
    "    oma_db = db.Database(oma_database_address)\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- OMA data is parsed and its release name is:\", oma_db.get_release_name())\n",
    "    list_speices= [z.uniprot_species_code for z in oma_db.tax.genomes.values()] \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- There are\",len(list_speices),\"species in the OMA database.\")\n",
    "\n",
    "    file = open(hog_og_map_address, \"r\")\n",
    "    contents = file.read()\n",
    "    hog_OG_map = ast.literal_eval(contents)\n",
    "    file.close()\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- The hog-og map is read from file with the length of \", len(hog_OG_map))\n",
    "    \n",
    "    \n",
    "    return (oma_db, hog_OG_map, list_speices)\n",
    "\n",
    "\n",
    "def parse_proteome(project_folder, list_speices):\n",
    "    \n",
    "    ############### Parsing query proteome of species #######\n",
    "    #########################################################\n",
    "\n",
    "    project_files = listdir(project_folder)\n",
    "\n",
    "    query_species_names = []\n",
    "    for file in project_files:\n",
    "        if file.split(\".\")[-1]==\"fa\":\n",
    "            file_name_split = file.split(\".\")[:-1]\n",
    "            query_species_names.append('.'.join(file_name_split))\n",
    "\n",
    "    # we may assert existence of query_species_name+\".fa/hogmap\"\n",
    "    query_prot_records_species = [ ]\n",
    "    for query_species_name in query_species_names:\n",
    "        query_prot_address = project_folder + query_species_name + \".fa\" \n",
    "        query_prot_records = list(SeqIO.parse(query_prot_address, \"fasta\")) \n",
    "        query_prot_records_species.append(query_prot_records)\n",
    "\n",
    "    # for development\n",
    "    query_species_num = len(query_species_names)\n",
    "    for species_i in range(query_species_num):\n",
    "        len_prot_record_i = len( query_prot_records_species[species_i] )\n",
    "        species_name_i = query_species_names[species_i]\n",
    "        #print(species_name_i,len_prot_record_i)\n",
    "        if species_name_i in list_speices: \n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(current_time,\"- the species\",species_name_i,\" already exists in the oma database, remove it first\")\n",
    "            exit()\n",
    "\n",
    "    return (query_species_names, query_prot_records_species)\n",
    "\n",
    "\n",
    "\n",
    "def parse_hogmap_omamer(project_folder , query_species_names):\n",
    "\n",
    "    ################### Parsing omamer's output  ########\n",
    "    #####################################################\n",
    "\n",
    "    query_prot_names_species = []\n",
    "    query_hogids_species = []\n",
    "\n",
    "    for query_species_name in query_species_names:\n",
    "        omamer_output_address = project_folder + query_species_name + \".hogmap\"     \n",
    "        omamer_output_file = open(omamer_output_address,'r');\n",
    "\n",
    "        query_prot_names= []\n",
    "        query_hogids= []\n",
    "\n",
    "        for line in omamer_output_file:\n",
    "            line_strip=line.strip()\n",
    "            if not line_strip.startswith('qs'):\n",
    "                line_split= line_strip.split(\"\\t\")        \n",
    "                query_prot_names.append(line_split[0])\n",
    "                query_hogids.append(line_split[1])\n",
    "        #print(\"number of proteins in omamer output for \",query_species_name,\"is\",len(query_hogids)) # ,query_hogids\n",
    "        query_prot_names_species.append(query_prot_names)\n",
    "        query_hogids_species.append(query_hogids)    \n",
    "        \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- There are \",len(query_prot_names_species),\" in project folder.\")\n",
    "    print(current_time,\"- The first species\",query_prot_names[0],\" contains \",len(query_hogids_species[0]),\" proteins.\")\n",
    "\n",
    "    \n",
    "    return (query_prot_names_species, query_hogids_species)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def extract_unique_hog_pure(query_species_names,query_hogids_species, query_prot_names_species,query_prot_records_species):\n",
    "    ###### Extracting unique HOG list and corresponding query proteins ########\n",
    "    ###########################################################################\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- Extracting proteins mapped only once on a HOG is started\")\n",
    "\n",
    "    query_hogids_filtr_species = []\n",
    "    query_prot_names_filtr_species = []\n",
    "    query_prot_records_filtr_species = []\n",
    "\n",
    "    repeated_hogid_num = 0\n",
    "    \n",
    "    query_species_num = len(query_species_names) \n",
    "    \n",
    "    dic_hogs_list=[]  # a list of dictinaries \n",
    "\n",
    "\n",
    "    for species_i in range(query_species_num):\n",
    "\n",
    "        query_hogids =  query_hogids_species[species_i]\n",
    "        query_prot_names = query_prot_names_species[species_i]\n",
    "        query_prot_records  = query_prot_records_species[species_i]\n",
    "        \n",
    "        dic_hogs = {}\n",
    "        for prot_i in range(len(query_hogids)):\n",
    "            query_hogid      = query_hogids[prot_i]\n",
    "            query_prot_name  = query_prot_names[prot_i]\n",
    "            query_prot_record= query_prot_records[prot_i]\n",
    "            if  query_hogid  not in  dic_hogs:\n",
    "                dic_hogs[query_hogid]=[(query_prot_name,query_prot_record)]\n",
    "            else:\n",
    "                repeated_hogid_num += 1 \n",
    "                dic_hogs[query_hogid].append((query_prot_name,query_prot_record))\n",
    "        dic_hogs_list.append(dic_hogs)\n",
    "\n",
    "        \n",
    "    for dic_hogs in dic_hogs_list: # each species\n",
    "        \n",
    "        query_prot_names_filtr = []\n",
    "        query_prot_records_filtr = []\n",
    "\n",
    "        hogid_list = list(dic_hogs.keys())\n",
    "        for hogid in hogid_list:\n",
    "            list_query_prot = dic_hogs[hogid]\n",
    "            if len(list_query_prot)>1:\n",
    "                del dic_hogs[hogid]\n",
    "        #here dic_hogs is updated and  dic_hogs_list  is also updated.\n",
    "        \n",
    "        #print(len(hogid_list),len(dic_hogs))\n",
    "        #for key, value in d.items():\n",
    "        query_hogids_filtr = []\n",
    "        query_prot_names_filtr = []\n",
    "        query_prot_records_filtr = []        \n",
    "        #query_hogids_filtr=list(dic_hogs.keys())\n",
    "        for hogid, tuple_value  in dic_hogs.items():\n",
    "            query_hogids_filtr.append(hogid)\n",
    "            query_prot_names_filtr.append(tuple_value[0][0])\n",
    "            query_prot_records_filtr.append(tuple_value[0][1])\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        query_hogids_filtr_species.append(query_hogids_filtr)\n",
    "        query_prot_names_filtr_species.append(query_prot_names_filtr)\n",
    "        query_prot_records_filtr_species.append(query_prot_records_filtr)        \n",
    "    \n",
    "    current_time  = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- Extracting proteins mapped only once on a HOG is finished\")\n",
    "    num_all_hogs=np.sum([len(dic_hogs) for dic_hogs in dic_hogs_list])\n",
    "    print(current_time,\"- For \",len(dic_hogs_list),\" species, we keep only\",num_all_hogs,\"HOGs.\")\n",
    "    \n",
    "\n",
    "    return (query_hogids_filtr_species, query_prot_names_filtr_species, query_prot_records_filtr_species )\n",
    "  \n",
    "\n",
    "# def extract_unique_hog(query_species_names,query_hogids_species, query_prot_names_species,query_prot_records_species):\n",
    "#     ###### Extracting unique HOG list and corresponding query proteins ########\n",
    "#     ###########################################################################\n",
    "\n",
    "#     query_hogids_filtr_species = []\n",
    "#     query_prot_names_filtr_species = []\n",
    "#     query_prot_records_filtr_species = []\n",
    "\n",
    "#     repeated_hogid_num = 0\n",
    "    \n",
    "#     query_species_num = len(query_species_names) \n",
    "    \n",
    "#     for species_i in range(query_species_num):\n",
    "#         #print(query_species_names[species_i])\n",
    "\n",
    "#         query_hogids =  query_hogids_species[species_i]\n",
    "#         query_prot_names = query_prot_names_species[species_i]\n",
    "#         query_prot_records  = query_prot_records_species[species_i]\n",
    "\n",
    "\n",
    "\n",
    "#         query_hogids_filtr = []\n",
    "#         query_prot_names_filtr = []\n",
    "#         query_prot_records_filtr = []\n",
    "\n",
    "#         for prot_i in range(len(query_hogids)):\n",
    "\n",
    "#             if not query_hogids[prot_i] in query_hogids_filtr: \n",
    "\n",
    "#                 query_hogids_filtr.append(query_hogids[prot_i])\n",
    "#                 query_prot_names_filtr.append(query_prot_names[prot_i])\n",
    "#                 query_prot_records_filtr.append(query_prot_records[prot_i])\n",
    "#             else:\n",
    "#                 repeated_hogid_num += 1 \n",
    "#                 # for development\n",
    "#                 #print(\"repeated hogid\",query_hogids[prot_i], \" for protein \",query_prot_names[prot_i])\n",
    "#                 # now we keep the first protein query when these are repeated\n",
    "\n",
    "\n",
    "#         query_hogids_filtr_species.append(query_hogids_filtr)\n",
    "#         query_prot_names_filtr_species.append(query_prot_names_filtr)\n",
    "#         query_prot_records_filtr_species.append(query_prot_records_filtr)\n",
    "\n",
    "\n",
    "#         num_query_filtr = len(query_hogids_filtr)\n",
    "#         #print(\"Number of prot queries after filtering is\",num_query_filtr,\"\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "#     return (query_hogids_filtr_species, query_prot_names_filtr_species, query_prot_records_filtr_species )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c023767",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gather_OG(query_species_names, query_hogids_filtr_species, query_prot_names_filtr_species, query_prot_records_filtr_species):\n",
    "\n",
    "    ############ Extracting the most frequent OG  ########\n",
    "    #####################################################\n",
    "\n",
    "    #dict (oma_group_nr -> dict(species, [proteins]))\n",
    "    #Og[555] = {homo_erectus: [blabla, blublu], yellow_bird: [P52134], brown_bear: [P2121,B53223]}\n",
    "\n",
    "    OGs_queries = {}\n",
    "\n",
    "    # hog_OG_map = {}\n",
    "\n",
    "    mostFrequent_OG_list_species = []\n",
    "\n",
    "    frq_most_frequent_og_list_all = [] # for development\n",
    "    \n",
    "    query_species_num = len(query_species_names)  \n",
    "    for species_i in  range(query_species_num):\n",
    "\n",
    "        query_species_name = query_species_names[species_i]\n",
    "        #print(\"\\n\",query_species_name)\n",
    "\n",
    "        query_hogids_filtr = query_hogids_filtr_species[species_i]\n",
    "        query_prot_names_filtr = query_prot_names_filtr_species[species_i]\n",
    "        query_prot_records_filtr = query_prot_records_filtr_species[species_i]\n",
    "\n",
    "        mostFrequent_OG_list=[]\n",
    "\n",
    "        num_query_filtr = len(query_hogids_filtr)\n",
    "        for  item_idx in range(num_query_filtr): #\n",
    "\n",
    "            #query_protein = query_prot_names_filtr[item_idx]\n",
    "            seqRecords_query =  query_prot_records_filtr[item_idx]\n",
    "            seqRecords_query_edited = SeqRecord(Seq(str(seqRecords_query.seq)), query_species_name, '', '')\n",
    "            #print(seqRecords_query_edited)\n",
    "\n",
    "            hog_id= query_hogids_filtr[item_idx]\n",
    "\n",
    "            if not hog_id in hog_OG_map:   # Caculitng  most frq OG for the new hog\n",
    "                mostFrequent_OG= -1\n",
    "                hog_OG_map[hog_id]=mostFrequent_OG\n",
    "\n",
    "            else:  # hog_id is in hog_OG_map dic\n",
    "                #print(\"using the hog-og-map\")\n",
    "                mostFrequent_OG = hog_OG_map[hog_id]\n",
    "\n",
    "            if mostFrequent_OG in OGs_queries:\n",
    "                OGs_queries_k = OGs_queries[mostFrequent_OG]\n",
    "\n",
    "                if not query_species_name in OGs_queries_k:\n",
    "                    OGs_queries_k[query_species_name] = seqRecords_query_edited\n",
    "                    OGs_queries[mostFrequent_OG] = OGs_queries_k\n",
    "            else:\n",
    "                OGs_queries[mostFrequent_OG] = {query_species_name: seqRecords_query_edited} # query_protein = query_prot_names_filtr[item_idx]\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Needed HOH-OG map \",len(OGs_queries),\"are extracted from the map file.\") \n",
    "    \n",
    "    return OGs_queries\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc320ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_OG_query(OGs_queries, oma_db, threshold_least_query_sepecies_in_OG,project_folder):\n",
    "    \n",
    "    ########## Combine proteins of OG with queries ##################\n",
    "    #################################################################\n",
    "    \n",
    "    seqRecords_OG_queries = []\n",
    "    seqRecords_all = []\n",
    "    for OG_q in OGs_queries.keys():  # OG found in the query\n",
    "\n",
    "        dic_species_prot = OGs_queries[OG_q]\n",
    "        if len(dic_species_prot) >threshold_least_query_sepecies_in_OG:\n",
    "\n",
    "            seqRecords_query_edited_all = []\n",
    "            for query_species_name,seqRecords_query_edited  in dic_species_prot.items():\n",
    "                #print(seqRecords_query_edited)\n",
    "                seqRecords_query_edited_all.append(seqRecords_query_edited) \n",
    "\n",
    "\n",
    "            mostFrequent_OG = OG_q\n",
    "            if mostFrequent_OG != -1:\n",
    "                OG_members = oma_db.oma_group_members(mostFrequent_OG)\n",
    "                proteins_object_OG = [db.ProteinEntry(oma_db, pr) for pr in OG_members]  # the protein IDs of og members\n",
    "                 # covnert to biopython objects\n",
    "                seqRecords_OG=[SeqRecord(Seq(pr.sequence),str(pr.genome.uniprot_species_code),'','') for pr in proteins_object_OG]\n",
    "\n",
    "                seqRecords_OG_queries =seqRecords_OG + seqRecords_query_edited_all\n",
    "                current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "                #print(\"length of OG\",mostFrequent_OG,\"was\",len(seqRecords_OG),\",now is\",len(seqRecords_OG_queries))\n",
    "                print(current_time, \" - Combining an OG with length of \",len(seqRecords_OG),\"\\t with a query \",len(seqRecords_query_edited_all),\" is just finished.\")\n",
    "\n",
    "                seqRecords_all.append(seqRecords_OG_queries)\n",
    "\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(\"\\n\", current_time, \"- Combining queries with OG is finished! number of OGs\",len(seqRecords_all)) # \n",
    "    \n",
    "    \n",
    "    \n",
    "    open_file = open(project_folder+\"_file_combined_OGs.pkl\", \"wb\")\n",
    "    pickle.dump(seqRecords_all, open_file)\n",
    "    open_file.close()\n",
    "\n",
    "    \n",
    "    return(seqRecords_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07a84ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_msa_OG(seqRecords_OG_queries):\n",
    "    ############## MSA  ##############\n",
    "    ##################################\n",
    "    #current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    #print(current_time, \"- working on new OG with length of \",len(seqRecords_OG_queries))\n",
    "\n",
    "    wrapper_mafft = mafft.Mafft(seqRecords_OG_queries,datatype=\"PROTEIN\") \n",
    "    # MAfft error: Alphabet 'U' is unknown. -> add --anysymbol argument needed to define in the sourse code\n",
    "    # workaround sed \"s/U/X/g\"\n",
    "    \n",
    "    wrapper_mafft.options.options['--retree'].set_value(1)\n",
    "\n",
    "\n",
    "    run_mafft = wrapper_mafft() # it's wrapper  storing the result  and time \n",
    "    time_taken_mafft = wrapper_mafft.elapsed_time\n",
    "\n",
    "    result_mafft = wrapper_mafft.result \n",
    "    time_taken_mafft2 = wrapper_mafft.elapsed_time\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    #print(current_time,\"- time elapsed for MSA: \",time_taken_mafft2)\n",
    "    print(current_time,\"- MSA for an OG is just finished: \",time_taken_mafft2)\n",
    "\n",
    "    return(result_mafft)\n",
    "   \n",
    "\n",
    "\n",
    "def run_msa_OG_parallel(seqRecords_all,number_max_workers,project_folder):\n",
    "        \n",
    "    iterotr_OGs = 0 \n",
    "    \n",
    "    result_mafft_all_species=[]\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Parallel msa is started for \",len(seqRecords_all),\" OGs.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=number_max_workers) as executor: \n",
    "        for seqRecords_OG_queries, output_values in zip(seqRecords_all, executor.map(run_msa_OG, seqRecords_all)):\n",
    "            result_mafft_all_species.append(output_values)\n",
    "\n",
    "            \n",
    "    \n",
    "    open_file = open(project_folder+\"_file_msas.pkl\", \"wb\")\n",
    "    pickle.dump(result_mafft_all_species, open_file)\n",
    "    open_file.close()\n",
    "    \n",
    "    return result_mafft_all_species\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39595b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_ogs(result_mafft_all_species,ogs_keep_number):\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Filtering MSA started.\")\n",
    "\n",
    "    density_ogs=[]\n",
    "    for msa_og in result_mafft_all_species:\n",
    "\n",
    "        gap_count_og= 0\n",
    "        all_count_og=0\n",
    "        for record in msa_og:\n",
    "            seq=str(record.seq)\n",
    "            gap_count_og += seq.count(\"-\") + seq.count(\"?\") + seq.count(\".\") +seq.count(\"~\")    \n",
    "        #gap_count_ogs.append(gap_count_og)\n",
    "        density_og=gap_count_og/(len(msa_og)*len(msa_og[0]))\n",
    "        density_ogs.append(density_og)\n",
    "        #if density_ogs> treshold_density:\n",
    "    plt.hist(density_ogs,bins=100) # , bins=10\n",
    "    #plt.show()\n",
    "    plt.savefig(\"./_density_ogs.pdf\")\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- The histogram of density ogs is saved.\")\n",
    "    \n",
    "    id_og_keep = np.argsort(density_ogs)[-ogs_keep_number:]\n",
    "    result_mafft_all_species_filtr = [result_mafft_all_species[i] for i in id_og_keep]\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Filtering MSA finished, keeping\",len(result_mafft_all_species_filtr),\"out of\",len(result_mafft_all_species))       \n",
    "    \n",
    "\n",
    "    open_file = open(project_folder+\"_file_msas_filtered.pkl\", \"wb\")\n",
    "    pickle.dump(result_mafft_all_species, open_file)\n",
    "    open_file.close()\n",
    "\n",
    "        \n",
    "    return result_mafft_all_species_filtr\n",
    "\n",
    "\n",
    "\n",
    "def concatante_alignments(result_mafft_all_species, project_folder):\n",
    "    ############## Concatante alignments  ##############\n",
    "    ####################################################\n",
    "\n",
    "    #alignments= result_maf2_all\n",
    "\n",
    "    alignments= result_mafft_all_species\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- alignments len\",len(alignments))\n",
    "    #print([len(aln) for aln in alignments ])\n",
    "    #print([len(seq) for aln in alignments for seq in aln])\n",
    "\n",
    "    all_labels_raw = [seq.id for aln in alignments for seq in aln]\n",
    "    all_labels = set(all_labels_raw)\n",
    "    print(\"ids: \",len(all_labels),len(all_labels_raw))\n",
    "\n",
    "    # Make a dictionary to store info as we go along\n",
    "    # (defaultdict is convenient -- asking for a missing key gives back an empty list)\n",
    "    concat_buf = defaultdict(list)\n",
    "\n",
    "    # Assume all alignments have same alphabet\n",
    "    alphabet = alignments[0]._alphabet\n",
    "\n",
    "    for aln in alignments:\n",
    "        length = aln.get_alignment_length()\n",
    "        #print(\"length\",length)\n",
    "        # check if any labels are missing in the current alignment\n",
    "        these_labels = set(rec.id for rec in aln)\n",
    "        missing = all_labels - these_labels\n",
    "        #print(missing)\n",
    "        # if any are missing, create unknown data of the right length,\n",
    "        # stuff the string representation into the concat_buf dict\n",
    "        for label in missing:\n",
    "            new_seq = UnknownSeq(length, alphabet=alphabet)\n",
    "            concat_buf[label].append(str(new_seq))\n",
    "\n",
    "        # else stuff the string representation into the concat_buf dict\n",
    "        for rec in aln:\n",
    "            concat_buf[rec.id].append(str(rec.seq))\n",
    "\n",
    "    # Stitch all the substrings together using join (most efficient way),\n",
    "    # and build the Biopython data structures Seq, SeqRecord and MultipleSeqAlignment\n",
    "    msa = MultipleSeqAlignment(SeqRecord(Seq(''.join(seq_arr), alphabet=alphabet), id=label)\n",
    "                                for (label, seq_arr) in concat_buf.items())\n",
    "\n",
    "\n",
    "\n",
    "    out_name_msa=project_folder+\"_msa_concatanated.txt\"\n",
    "    handle_msa_fasta = open(out_name_msa,\"w\")\n",
    "    SeqIO.write(msa, handle_msa_fasta,\"fasta\")\n",
    "    handle_msa_fasta.close()\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- \", len(msa),msa.get_alignment_length()) # super matrix size\n",
    "    \n",
    "    return msa\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6955a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def msa_filter_row(msa,project_folder,tresh_ratio_gap_row,query_species_names):\n",
    "\n",
    "    msa_filtered_row = [] # msa_fltr\n",
    "    ratio_records=[]\n",
    "    for record in msa:\n",
    "        species_name = record.id\n",
    "        seq = record.seq\n",
    "        seqLen = len(record)\n",
    "        \n",
    "        gap_count=seq.count(\"-\") + seq.count(\"?\") + seq.count(\".\") +seq.count(\"~\")\n",
    "                \n",
    "        ratio_record_nongap= 1-gap_count/seqLen\n",
    "        ratio_records.append(round(ratio_record_nongap,3))\n",
    "\n",
    "        if ratio_record_nongap > tresh_ratio_gap_row:\n",
    "            msa_filtered_row.append(record)\n",
    "        elif species_name in query_species_names : \n",
    "            msa_filtered_row.append(record)\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(current_time, \"- Many row-wise gap for query\",species_name,\"with a ratio of\",ratio_record_nongap) \n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "    print(current_time, \"- Row-wise filtering of MSA is finished.\") \n",
    "    print(current_time, \"- Out of \",len(msa),\"species,\",len(msa_filtered_row),\"species (row of msa) remained.\")\n",
    "\n",
    "\n",
    "    out_name_msa=project_folder+\"_msa_concatanated_filtered_row_\"+str(tresh_ratio_gap_row)+\".txt\"\n",
    "    handle_msa_fasta = open(out_name_msa,\"w\")\n",
    "    SeqIO.write(msa_filtered_row, handle_msa_fasta,\"fasta\")\n",
    "    handle_msa_fasta.close()\n",
    "    \n",
    "    print(current_time, \"- MSA Row-wise filtered stored in file.\") # super matrix size\n",
    "    \n",
    "    \n",
    "    return msa_filtered_row\n",
    "    \n",
    "    \n",
    "\n",
    "def msa_filter_col(msa_filtered_row, tresh_ratio_gap_col):\n",
    "\n",
    "    ratio_col_all = []\n",
    "\n",
    "    length_record= len(msa_filtered_row[1])\n",
    "    num_records = len(msa_filtered_row)\n",
    "\n",
    "\n",
    "    keep_cols = []\n",
    "    for col_i in range(length_record):  # inspired by https://github.com/andreas-wilm/compbio-utils/blob/master/prune_aln_cols.py \n",
    "\n",
    "        col_values = [record.seq[col_i] for record in msa_filtered_row]\n",
    "\n",
    "        gap_count=col_values.count(\"-\") + col_values.count(\"?\") + col_values.count(\".\") +col_values.count(\"~\")\n",
    "\n",
    "        ratio_col_nongap = 1- gap_count/num_records\n",
    "        ratio_col_all.append(ratio_col_nongap)\n",
    "        if ratio_col_nongap  > tresh_ratio_gap_col:\n",
    "            keep_cols.append(col_i)\n",
    "\n",
    "\n",
    "    plt.hist(ratio_col_all,bins=100) # , bins=10\n",
    "    #plt.show()\n",
    "    plt.savefig(\"./__ratio_col.pdf\")\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Columns indecis extracted. Out of \", length_record,\"columns,\",len(keep_cols),\"is remained.\") \n",
    "\n",
    "    msa_filtered_row_col = []\n",
    "\n",
    "    for record in msa_filtered_row :\n",
    "        record_seq = str(record.seq)\n",
    "\n",
    "        record_seq_edited  = ''.join([record_seq[i] for i in keep_cols  ])\n",
    "        record_edited= SeqRecord(Seq(record_seq_edited), record.id, '', '')\n",
    "        msa_filtered_row_col.append(record_edited)                         \n",
    "\n",
    "\n",
    "    \n",
    "    out_name_msa=project_folder+\"_msa_concatanated_filtered_row_col_\"+str(tresh_ratio_gap_col)+\".txt\"\n",
    "    handle_msa_fasta = open(out_name_msa,\"w\")\n",
    "    SeqIO.write(msa_filtered_row_col, handle_msa_fasta,\"fasta\")\n",
    "    handle_msa_fasta.close()\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Column-wise filtering of MSA is finished\",len(msa_filtered_row_col),len(msa_filtered_row_col[0])) \n",
    "       \n",
    "    #msa_filtered_row_col = MultipleSeqAlignment(msa_filtered_row_col)\n",
    "    return msa_filtered_row_col\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2e4b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_tree(msa, project_folder):\n",
    "    ############## Tree inference  ###################\n",
    "    ##################################################\n",
    "\n",
    "    wrapper_tree=fasttree.Fasttree(msa,datatype=\"PROTEIN\")\n",
    "    wrapper_tree.options.options['-fastest']    \n",
    "    result_tree1 = wrapper_tree()\n",
    "\n",
    "    time_taken_tree = wrapper_tree.elapsed_time \n",
    "    time_taken_tree\n",
    "\n",
    "    result_tree2 = wrapper_tree.result\n",
    "    tree_nwk=str(result_tree2[\"tree\"])\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- \",len(tree_nwk))\n",
    "\n",
    "    out_name_tree=project_folder+\"_tree.txt\"\n",
    "    file1 = open(out_name_tree,\"w\")\n",
    "    file1.write(tree_nwk)\n",
    "    file1.close() \n",
    "    return tree_nwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ba97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a755c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/FAC/FBM/DBC/cdessim2/default/smajidi1/software/miniconda3/lib/python3.8/site-packages/tables/leaf.py:402: PerformanceWarning: The Leaf ``/Protein/_i_Entries/OmaHOG/sorted`` is exceeding the maximum recommended rowsize (104857600 bytes);\n",
      "be ready to see PyTables asking for *lots* of memory and possibly slow\n",
      "I/O.  You may want to reduce the rowsize by trimming the value of\n",
      "dimensions that are orthogonal (and preferably close) to the *main*\n",
      "dimension of this leave.  Alternatively, in case you have specified a\n",
      "very small/large chunksize, you may want to increase/decrease it.\n",
      "  warnings.warn(\"\"\"\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:48:34 - OMA data is parsed and its release name is: All.Apr2021\n",
      "16:48:34 - There are 2424 species in the OMA database.\n",
      "16:48:53 - The hog-og map is read from file with the length of  3396673\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    (oma_db, hog_OG_map, list_speices) = parse_oma(oma_database_address, hog_og_map_address)\n",
    "\n",
    "\n",
    "    (query_species_names, query_prot_records_species) = parse_proteome(project_folder, list_speices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39629bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:48:54 - There are  36  in project folder.\n",
      "16:48:54 - The first species ARAGUA_R00001  contains  2000  proteins.\n",
      "16:48:54 - Extracting proteins mapped only once on a HOG is started\n",
      "16:48:54 - Extracting proteins mapped only once on a HOG is finished\n",
      "16:48:54 - For  36  species, we keep only 61866 HOGs.\n",
      "16:48:55 - Needed HOH-OG map  15605 are extracted from the map file.\n"
     ]
    }
   ],
   "source": [
    "    (query_prot_names_species, query_hogids_species) = parse_hogmap_omamer(project_folder,query_species_names)\n",
    "\n",
    "    \n",
    "    (query_hogids_filtr_species, query_prot_names_filtr_species, query_prot_records_filtr_species) = extract_unique_hog_pure(query_species_names,query_hogids_species, query_prot_names_species,query_prot_records_species) # #extract_unique_hog old function\n",
    "\n",
    "    OGs_queries = gather_OG(query_species_names, query_hogids_filtr_species, query_prot_names_filtr_species, query_prot_records_filtr_species)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeab2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae9ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec6fe9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:48:57  - Combining an OG with length of  93 \t with a query  17  is just finished.\n",
      "16:49:12  - Combining an OG with length of  1709 \t with a query  17  is just finished.\n",
      "16:49:13  - Combining an OG with length of  107 \t with a query  16  is just finished.\n",
      "\n",
      " 16:49:13 - Combining queries with OG is finished! number of OGs 3\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "    threshold_least_query_sepecies_in_OG = 15\n",
    "    seqRecords_all = combine_OG_query(OGs_queries, oma_db,threshold_least_query_sepecies_in_OG,project_folder)\n",
    "\n",
    "    #num_OGs= len(seqRecords_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9daa40f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "336ec4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:53:50 - Parallel msa is started for  3  OGs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/FAC/FBM/DBC/cdessim2/default/smajidi1/software/miniconda3/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "/work/FAC/FBM/DBC/cdessim2/default/smajidi1/software/miniconda3/lib/python3.8/subprocess.py:849: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stderr = io.open(errread, 'rb', bufsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:53:54 - MSA for an OG is just finished:  3.3827171325683594\n",
      "16:55:02 - MSA for an OG is just finished:  67.87296056747437\n",
      "16:55:22 - MSA for an OG is just finished:  19.855766773223877\n",
      "16:55:22 - Filtering MSA started.\n",
      "16:55:22 - The histogram of density ogs is saved.\n",
      "16:55:22 - Filtering MSA finished, keeping 2 out of 3\n",
      "16:55:22 - alignments len 2\n",
      "ids:  1748 1849\n",
      "16:55:22 -  1748 8867\n",
      "16:55:22 - all msa are concatanated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    number_max_workers = 1\n",
    "    result_mafft_all_species = run_msa_OG_parallel(seqRecords_all,number_max_workers,project_folder)\n",
    "\n",
    "    \n",
    "    ogs_keep_number = 2\n",
    "    result_mafft_all_species_filtr = filter_ogs(result_mafft_all_species,ogs_keep_number)\n",
    "\n",
    "\n",
    "    \n",
    "    msa= concatante_alignments(result_mafft_all_species_filtr, project_folder)\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- all msa are concatanated\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e4932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reading pkl file \n",
    "# open_file = open(project_folder+\"_file_msas2.pkl\", \"rb\")\n",
    "# result_mafft_all_species = pickle.load(open_file)\n",
    "# open_file.close()\n",
    "# print(\"seq read is loaded\", len(seqRecords_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e3199f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a5e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4457100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:56:39 - Row-wise filtering of MSA is started.\n",
      "16:56:39 - Many row-wise gap for query ANHANH_2000_X with a ratio of 0.02199165444908091\n",
      "16:56:39 - Many row-wise gap for query ACACHL_2000_X with a ratio of 0.006315552046915585\n",
      "16:56:39 - Many row-wise gap for query AMAGUI_2000_X with a ratio of 0.13217548212473218\n",
      "16:56:39 - Many row-wise gap for query APHCOE_2000_X with a ratio of 0.13544603586331339\n",
      "16:56:39 - Many row-wise gap for query ALERUF_2000_X with a ratio of 0.018157212134882195\n",
      "16:56:39 - Many row-wise gap for query AREINT_2000_X with a ratio of 0.014435547535806892\n",
      "16:56:39 - Many row-wise gap for query ATRCLA_2000_X with a ratio of 0.012743881808954582\n",
      "16:56:39 - Many row-wise gap for query ALOBEC_2000_X with a ratio of 0.00845832863426188\n",
      "16:56:39 - Many row-wise gap for query ALACHE_2000_X with a ratio of 0.018157212134882195\n",
      "16:56:39 - Many row-wise gap for query AGEPHO_2000_X with a ratio of 0.00958610578549679\n",
      "16:56:39 - Many row-wise gap for query ACRARU_2000_X with a ratio of 0.037893312281493174\n",
      "16:56:39 - Many row-wise gap for query ATLROG_2000_X with a ratio of 0.016127213262659312\n",
      "16:56:39 - Many row-wise gap for query APTAUS_2000_X with a ratio of 0.053118303823164514\n",
      "16:56:39 - Many row-wise gap for query APTROW_2000_X with a ratio of 0.06383218675989621\n",
      "16:56:39 - Many row-wise gap for query APTOWE_2000_X with a ratio of 0.06394496447501974\n",
      "16:56:39 - Many row-wise gap for query AGAROS_2000_X with a ratio of 0.0315777602345777\n",
      "16:56:39 - Many row-wise gap for query ANSCYG_2000_X with a ratio of 0.018382767565129132\n",
      "16:56:39 - Many row-wise gap for query AEGBEN_2000_X with a ratio of 0.06924551708582383\n",
      "16:56:39 - Many row-wise gap for query APAVIT_2000_X with a ratio of 0.018721100710499594\n",
      "16:56:39 - Many row-wise gap for query ANAPLA_2000_X with a ratio of 0.018382767565129132\n",
      "16:56:39 - Many row-wise gap for query ANAZON_2000_X with a ratio of 0.018382767565129132\n",
      "16:56:39 - Many row-wise gap for query ASASCU_2000_X with a ratio of 0.05289274839291758\n",
      "16:56:39 - Many row-wise gap for query AQUCHR_2000_X with a ratio of 0.053118303823164514\n",
      "16:56:39 - Many row-wise gap for query AEGCAU_2000_X with a ratio of 0.03293109281605955\n",
      "16:56:39 - Many row-wise gap for query APTHAA_2000_X with a ratio of 0.029209428216984357\n",
      "16:56:39 - Many row-wise gap for query ARAGUA_2000_X with a ratio of 0.06823051764971244\n",
      "16:56:39 - Row-wise filtering of MSA is finished.\n",
      "16:56:39 - Out of  1748 species, 26 species (row of msa) remained.\n",
      "16:56:39 - MSA Row-wise filtered stored in file.\n",
      "16:56:39 - Column-wise filtering of MSA is started.\n",
      "16:56:40 - Columns indecis extracted. Out of  8867 columns, 455 is remained.\n",
      "16:56:40 - Column-wise filtering of MSA is finished 26 455\n"
     ]
    }
   ],
   "source": [
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Row-wise filtering of MSA is started.\") \n",
    "       \n",
    "    tresh_ratio_gap_row = 0.3\n",
    "    msa_filtered_row = msa_filter_row(msa,project_folder,tresh_ratio_gap_row,query_species_names)\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Column-wise filtering of MSA is started.\") \n",
    "    \n",
    "    tresh_ratio_gap_col = 0.3\n",
    "    msa_filtered_row_col=  msa_filter_col(msa_filtered_row, tresh_ratio_gap_col)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c0591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ac326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e262c392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c4274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ STEP 1    Combine OG #########\n",
    "#if __name__ == \"__main__\":\n",
    "    (oma_db, hog_OG_map, list_speices) = parse_oma(oma_database_address, hog_og_map_address)\n",
    "    (query_species_names, query_prot_records_species) = parse_proteome(project_folder, list_speices)\n",
    "    (query_prot_names_species, query_hogids_species) = parse_hogmap_omamer(project_folder,query_species_names )\n",
    "    (query_hogids_filtr_species, query_prot_names_filtr_species, query_prot_records_filtr_species) = extract_unique_hog(query_species_names,query_hogids_species, query_prot_names_species,query_prot_records_species)\n",
    "    OGs_queries = gather_OG(query_species_names, query_hogids_filtr_species, query_prot_names_filtr_species, query_prot_records_filtr_species)\n",
    "    threshold_least_query_sepecies_in_OG = int(argv[2])\n",
    "    seqRecords_all = combine_OG_query(OGs_queries, oma_db,threshold_least_query_sepecies_in_OG)\n",
    "    num_OGs= len(seqRecords_all)\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"-  for \",len(seqRecords_all),\" OGs.\")\n",
    "    print(\"saving as pickle\")\n",
    "    import pickle\n",
    "    open_file = open(project_folder+\"_file.pkl\", \"wb\")\n",
    "    pickle.dump(seqRecords_all, open_file)\n",
    "    open_file.close()\n",
    "    \n",
    "    \n",
    "################ STEP 2   MSA     PARALELL #########\n",
    "\n",
    "\n",
    "    import pickle\n",
    "    open_file = open(project_folder+\"_file.pkl\", \"rb\")\n",
    "    seqRecords_all = pickle.load(open_file)\n",
    "    open_file.close()\n",
    "    print(\"seq read is loaded\", len(seqRecords_all))\n",
    "    number_workers  = int(argv[2])\n",
    "\n",
    "    iterotr_OGs = 0\n",
    "    result_mafft_all_species=[]\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Parallel msa is started for \",len(seqRecords_all),\" OGs.\")\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=number_workers) as executor: # ProcessPoolExecutor(max_workers=5)\n",
    "        for seqRecords_OG_queries, output_values in zip(seqRecords_all, executor.map(run_msa_OG, seqRecords_all)):\n",
    "            result_mafft_all_species.append(output_values)\n",
    "\n",
    "            \n",
    "    msa= concatante_alignments(result_mafft_all_species, project_folder)\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- all msa are concatanated\")\n",
    "\n",
    "    \n",
    "    \n",
    "################ STEP 3   MSA  Filtering #########\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"start reading msa file\")\n",
    "    project_folder = argv[1] # \"/work/FAC/FBM/DBC/cdessim2/default/smajidi1/fastoma/v3a/A/f7_2kA/\" # _msa_concatanated_hogmapX.txt\"\n",
    "    msa_file= argv[2]\n",
    "    msa_input = project_folder+ msa_file  #\"out2/_msa_concatanated_filtered_row_col_0.55.txt\"  # _row_0.0201.txt\"\n",
    "    msa = AlignIO.read(msa_input,\"fasta\")\n",
    "    print(\"finish reading file\",len(msa),len(msa[0]))\n",
    "\n",
    "    project_files = listdir(project_folder)\n",
    "    query_species_names = []\n",
    "    for file in project_files:\n",
    "        if file.split(\".\")[-1]==\"fa\":\n",
    "            file_name_split = file.split(\".\")[:-1]\n",
    "            query_species_names.append('.'.join(file_name_split))\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Row-wise filtering of MSA is started.\")\n",
    "\n",
    "    tresh_ratio_gap_row = float(argv[3]) #0.85\n",
    "    msa_filtered_row = msa_filter_row(msa, project_folder,tresh_ratio_gap_row,query_species_names)\n",
    "\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- Column-wise filtering of MSA is started.\")\n",
    "\n",
    "    tresh_ratio_gap_col = float(argv[4])\n",
    "    msa_filtered_row_col= msa_filter_col(msa_filtered_row, tresh_ratio_gap_col,project_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794fd7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dd14ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
