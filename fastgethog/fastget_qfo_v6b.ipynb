{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1fe123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4fc80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions will be used if we want to start from proteme+hogmap.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Structure of folders:\n",
    "Put proteomes of species as fasta files in /omamer_search/proteome/\n",
    "Run omamer and put the output of omamer in /omamer_search/hogmap/\n",
    "oma_database_address= the address to the oma databases\n",
    "\n",
    "hog and HOG are used interchangeably here. \n",
    "rHOG=rootHOG.  A subHOG itself is a HOG.\n",
    "\n",
    "\"\"\"\n",
    " \n",
    "def parse_oma_db(oma_database_address):    \n",
    "    \"\"\"\n",
    "    a function for loading an oma database in hdf5 format using pyoma.browser.db.\n",
    "    \n",
    "    output: oma_db, list_oma_speices\n",
    "    \"\"\"\n",
    "    oma_db = db.Database(oma_database_address)\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time, \"- OMA data is parsed and its release name is:\", oma_db.get_release_name())\n",
    "    list_oma_speices = [z.uniprot_species_code for z in oma_db.tax.genomes.values()] \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- There are\",len(list_oma_speices),\"species in the OMA database.\")    \n",
    "    return (oma_db, list_oma_speices)\n",
    "\n",
    "def parse_proteome(list_oma_speices):\n",
    "    \"\"\"    \n",
    "    a function for parsing fasta files of proteins located in /omamer_search/proteome/\n",
    "    using Bio.SeqIO.parse\n",
    "    Each fasta file is for one species.  The file name is the species name.\n",
    "    \n",
    "    output: query_species_names: list of species name, prots_record_allspecies: list of Biopython record of species\n",
    "    \"\"\"\n",
    "    project_files = listdir(address_working_folder+\"/omamer_search/proteome/\")\n",
    "    query_species_names = []\n",
    "    for file in project_files:\n",
    "        if file.split(\".\")[-1] == \"fa\":\n",
    "            file_name_split = file.split(\".\")[:-1]\n",
    "            query_species_names.append('.'.join(file_name_split))\n",
    "        if file.split(\".\")[-1] == \"fasta\":\n",
    "            file_name_split = file.split(\".\")[:-1]\n",
    "            query_species_names.append('.'.join(file_name_split))\n",
    "\n",
    "    # we may assert existence of query_species_name+\".fa/hogmap\"\n",
    "    prots_record_allspecies = [ ]\n",
    "    for query_species_name in query_species_names:\n",
    "        prot_address = address_working_folder +\"omamer_search/proteome/\"+ query_species_name + \".fa\" \n",
    "        prots_record = list(SeqIO.parse(prot_address, \"fasta\")) \n",
    "        prots_record_allspecies.append(prots_record)\n",
    "\n",
    "    query_species_num = len(query_species_names)    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- The are\",str(query_species_num),\"species in the proteome folder.\")\n",
    "    # for development\n",
    "    for species_i in range(query_species_num):\n",
    "        len_prot_record_i = len( prots_record_allspecies[species_i] )\n",
    "        species_name_i = query_species_names[species_i]\n",
    "        #print(species_name_i,len_prot_record_i)\n",
    "        if species_name_i in list_oma_speices: \n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(current_time,\"- the species\",species_name_i,\" already exists in the oma database, remove them first\")\n",
    "            exit()\n",
    "    # The proteins are parsed using  Bio.SeqIO.parse\n",
    "    # the first part of the header line before space \n",
    "    # >tr|A0A2I3FYY2|A0A2I3FYY2_NOMLE Uncharacterized protein OS=Nomascus leucogenys OX=61853 GN=CLPTM1L PE=3 SV=1\n",
    "    # will be \">tr|A0A2I3FYY2|A0A2I3FYY2_NOMLE\"\n",
    "    # [i.id for i in prots_record_allspecies[0] if len(i.id)!=30 and len(i.id)!=22 ] #'sp|O47892|CYB_NOMLE',\n",
    "    return (query_species_names, prots_record_allspecies)\n",
    "\n",
    "\n",
    "def parse_hogmap_omamer(query_species_names):\n",
    "    \"\"\"    \n",
    "    a function for parsing output of omamer (hogmap files) located in /omamer_search/hogmap/\n",
    "    Each hogmap file correspond to one fasta file of species, with the same name.\n",
    "    Note that some records of fasta may removed in hogmap, becuase of being so short.\n",
    "    \n",
    "    hogmap file example:\n",
    "    qseqid hogid overlap family-score subfamily-score qseqlen subfamily-medianseqlen\n",
    "    A0A140TAT7_CIOIN HOG:B0833785.1c.8b 1 0.99 0.9 490 503\n",
    "\n",
    "    output as list of list for all species:  \n",
    "    prots_hogmap_name_allspecies, prots_hogmap_hogid_allspecies,\n",
    "    prots_hogmap_subfscore_allspecies, prots_hogmap_seqlen_allspecies,\n",
    "    prots_hogmap_subfmedseqlen_allspecies\n",
    "\n",
    "    The order of species is the same as query_species_names.\n",
    "    \"\"\"\n",
    "    prots_hogmap_name_allspecies = []\n",
    "    prots_hogmap_hogid_allspecies = []\n",
    "    prots_hogmap_subfscore_allspecies = []\n",
    "    prots_hogmap_seqlen_allspecies = []\n",
    "    prots_hogmap_subfmedseqlen_allspecies = []\n",
    "    for query_species_name in query_species_names:\n",
    "        omamer_output_address = address_working_folder + \"omamer_search/hogmap/\"+ query_species_name + \".hogmap\"     \n",
    "        omamer_output_file = open(omamer_output_address,'r');\n",
    "        prots_hogmap_name = []\n",
    "        prots_hogmap_hogid = []\n",
    "        prots_hogmap_subfscore = []\n",
    "        prots_hogmap_seqlen = []\n",
    "        prots_hogmap_subfmedseqlen = []\n",
    "        for line in omamer_output_file:\n",
    "            line_strip=line.strip()\n",
    "            if not line_strip.startswith('qs'):\n",
    "                line_split= line_strip.split(\"\\t\")    \n",
    "                #if line_split[1]!='na':\n",
    "                prots_hogmap_name.append(line_split[0])\n",
    "                prots_hogmap_hogid.append(line_split[1])\n",
    "                prots_hogmap_subfscore.append(line_split[4]) # subfamily\n",
    "                prots_hogmap_seqlen.append(line_split[5])\n",
    "                prots_hogmap_subfmedseqlen.append(line_split[6])\n",
    "        prots_hogmap_name_allspecies.append(prots_hogmap_name)\n",
    "        prots_hogmap_hogid_allspecies.append(prots_hogmap_hogid)\n",
    "        prots_hogmap_subfscore_allspecies.append(prots_hogmap_subfscore)\n",
    "        prots_hogmap_seqlen_allspecies.append(prots_hogmap_seqlen)\n",
    "        prots_hogmap_subfmedseqlen_allspecies.append(prots_hogmap_subfmedseqlen)\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- There are \",len(prots_hogmap_name_allspecies),\" species in the hogmap folder.\")\n",
    "    print(current_time,\"- The first species\",query_species_names[0],\" contains \",len(prots_hogmap_hogid_allspecies[0]),\" proteins.\")\n",
    "    print(current_time,\"- The first protein of first species is \", prots_hogmap_name_allspecies[0][0])\n",
    "    hogmap_allspecies = (prots_hogmap_name_allspecies, prots_hogmap_hogid_allspecies, prots_hogmap_subfscore_allspecies, prots_hogmap_seqlen_allspecies, prots_hogmap_subfmedseqlen_allspecies)\n",
    "    return  hogmap_allspecies\n",
    "    \n",
    "    \n",
    "    \n",
    "def filter_prot_mapped(query_species_names, query_prot_records_species, query_prot_names_species_mapped):\n",
    "    \"\"\"    \n",
    "    a function for filtering biopython records in query_prot_records_species based on hogmaps\n",
    "    The reason is that some very short records of fasta are removed in hogmap.\n",
    "    So, we may lose track of order comparing hogmap and fasta file.\n",
    "    The goal here is to remove those from seq record (of the fasta file).\n",
    "    \n",
    "    output: query_prot_records_species_filtered\n",
    "    \"\"\"    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(current_time,\"- Filtering proteins started.\")\n",
    "    query_prot_records_species_filtered=[]\n",
    "    for species_idx in range(len(query_species_names)):    \n",
    "        # from fasta file\n",
    "        query_species_name=query_species_names[species_idx]\n",
    "        #print(query_species_name)\n",
    "        query_prot_records_species_i = query_prot_records_species[species_idx]\n",
    "        query_prot_ids_records = [record.id for record in query_prot_records_species_i]\n",
    "        # from hogmap file without proteins that are not mapped on any hogs\n",
    "        query_prot_names_species_i = query_prot_names_species_mapped[species_idx]\n",
    "        if len(query_prot_names_species_i) != len(query_prot_records_species_i):\n",
    "            query_prot_records_filterd=[]\n",
    "            for query_prot_name in query_prot_names_species_i:\n",
    "                if query_prot_name in query_prot_ids_records:\n",
    "                    prot_record_idx = query_prot_ids_records.index(query_prot_name)\n",
    "                    prot_record = query_prot_records_species_i[prot_record_idx]\n",
    "                    query_prot_records_filterd.append(prot_record)\n",
    "                else:\n",
    "                    current_time = datetime.now().strftime(\"%H:%M:%S\")                    \n",
    "                    logger_hog.error(str(current_time)+\"- Error 149 \"+query_species_name+\" \"+ query_prot_name)\n",
    "\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")        \n",
    "            print(current_time,\"- For the species\", query_species_name, \", few proteins were ignored by omamer.\")\n",
    "            print(current_time,\"- before filtering: in hogmap\", len(query_prot_names_species_i), \"in proteome\", len(query_prot_records_species_i))\n",
    "            print(current_time,\"- After filtering:  in hogmap\", len(query_prot_names_species_i), \"in proteome\", len(query_prot_records_filterd))            \n",
    "        else:\n",
    "            query_prot_records_filterd = query_prot_records_species_i\n",
    "        query_prot_records_species_filtered.append(query_prot_records_filterd)\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")        \n",
    "    print(current_time,\"- For the rest of species, all proteins were mapped using OMAmer.\")\n",
    "    return query_prot_records_species_filtered\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "def add_species_name(query_prot_records_species, query_species_names):\n",
    "    \"\"\"\n",
    "    adding the name of species to each protein record\n",
    "    \n",
    "    output: updated version of input\n",
    "    \"\"\"\n",
    "    \n",
    "    for ix in range(len(query_species_names)):\n",
    "        query_species_name = query_species_names[ix]\n",
    "        query_prot_records = query_prot_records_species[ix]\n",
    "        for i_prot in range(len(query_prot_records)):\n",
    "            query_prot_record = query_prot_records[i_prot]\n",
    "            query_prot_record.description += \"|species|\"+query_species_name\n",
    "    return query_prot_records_species\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def group_prots_rootHOGs(prots_hogmap_hogid_allspecies, address_rhogs_folder):\n",
    "    \"\"\"    \n",
    "    a function for finding those proteins that are mapped to the same rootHOG.\n",
    "    Then, we write each rootHOG as a seprate fasta file in the address_rhogs_folder folder\n",
    "    \n",
    "    output: rhogid_num_list, rhogids_prot_records_query\n",
    "    \"\"\"  \n",
    "    # extract rootHOG ID  \"B0833755.5c.10g.24e.16c.18b\" ->\"B0833755\"\n",
    "    prots_hogmap_rhogid_allspecies = []\n",
    "    for prots_hogmap_hogid in prots_hogmap_hogid_allspecies:\n",
    "        prots_hogmap_rhogid = []\n",
    "        for prot_hogmap_hogid in prots_hogmap_hogid:\n",
    "            prot_hogmap_rhogid=prot_hogmap_hogid.split(\".\")[0] \n",
    "            prots_hogmap_rhogid.append(prot_hogmap_rhogid)\n",
    "        prots_hogmap_rhogid_allspecies.append(prots_hogmap_rhogid)\n",
    "\n",
    "    # gathering name of prots from all species,  group them based on rHOG that they mapped on\n",
    "    rhogid_prot_idx_dic = {} \n",
    "    for species_idx in range(len(query_species_names)):\n",
    "        species_name = query_species_names[species_idx]\n",
    "        prots_hogmap_rhogid = prots_hogmap_rhogid_allspecies[species_idx]\n",
    "        for prots_hogmap_idx in range(len(prots_hogmap_rhogid)):\n",
    "            prot_hogmap_rhogid = prots_hogmap_rhogid[prots_hogmap_idx]\n",
    "            if prot_hogmap_rhogid in rhogid_prot_idx_dic:\n",
    "                rhogid_prot_idx_dic[prot_hogmap_rhogid].append((species_idx, prots_hogmap_idx))\n",
    "            else:\n",
    "                rhogid_prot_idx_dic[prot_hogmap_rhogid] = [(species_idx, prots_hogmap_idx)]\n",
    "    #print(len(rhogid_prot_idx_dic)) #  rhogid_prot_idx_dic['HOG:0018405']\n",
    "\n",
    "    #extracting prot records for each rootHOG\n",
    "    rhogids_prot_records_query = [ ]\n",
    "    rhogids_list = []\n",
    "    for rhogid in rhogid_prot_idx_dic.keys() :\n",
    "        rhogid_prot_records = []\n",
    "        if rhogid != \"na\" and len(rhogid)>1: # ignore un-mapped prots\n",
    "            rhogids_list.append(rhogid)\n",
    "            rhogid_prot_idx =  rhogid_prot_idx_dic[rhogid]\n",
    "            for (species_idx, prots_hogmap_idx) in rhogid_prot_idx:\n",
    "                prot_record = query_prot_records_species_filtered[species_idx][prots_hogmap_idx] \n",
    "                rhogid_prot_records.append(prot_record)\n",
    "            rhogids_prot_records_query.append(rhogid_prot_records) \n",
    "        #else:\n",
    "        #    print(\"root hog na / lenght of one \",rhogid)\n",
    "        \n",
    "    #print(len(rhogids_prot_records_query),len(rhogids_prot_records_query[0]))\n",
    "    rhogid_num_list= []\n",
    "    for rhogid_idx in range(len(rhogids_list)):\n",
    "        rhogid_prot_records_query= rhogids_prot_records_query[rhogid_idx] \n",
    "        rhogid = rhogids_list[rhogid_idx]\n",
    "        rhogid_B= rhogid.split(\":\")[1]\n",
    "        rhogid_num= int(rhogid_B[1:] ) # # B0613860\n",
    "        rhogid_num_list.append(rhogid_num)\n",
    "        if   len(rhogid_prot_records_query) > 2 : # len(rhogid_prot_records_query) < 100  and\n",
    "            #rhogids_prot_records_oma = []\n",
    "            #for hog_elements in oma_db.member_of_fam(rhogid_num):   # this gets the member of roothog 2 (HOG:000002)\n",
    "            #    prot_hog_element = ProteinEntry(oma_db, hog_elements)\n",
    "            #    #print(prot_hog_element.omaid, prot_hog_element.hog_family_nr, len(prot_hog_element.sequence),prot_hog_element.sequence[0])\n",
    "            #    rhogids_prot_records_oma.append(SeqRecord(Seq(prot_hog_element.sequence), id=prot_hog_element.omaid))\n",
    "            #rhogids_prot_records_both= rhogids_prot_records_oma +  rhogid_prot_records_query\n",
    "            #rhogids_prot_records.append(rhogids_prot_records_both)\n",
    "            SeqIO.write(rhogid_prot_records_query, address_rhogs_folder+\"HOG_\"+str(rhogid_num)+\".fa\", \"fasta\")\n",
    "    #print(\"all HOGs   (>2 <100) has written.\",len(rhogids_prot_records_query),len(rhogids_list), len(rhogid_prot_records_query), len(rhogid_prot_records_query[0]))\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")        \n",
    "    print(current_time,\"- Sequences of rootHOGs are writtend as fasta file in \"+address_rhogs_folder)\n",
    "    \n",
    "    return (rhogid_num_list, rhogids_prot_records_query)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd11df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "730e0d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the following are needed when we start from a rootHOG fasta file.\n",
    "\n",
    "\n",
    "def read_species_tree(species_tree_address):\n",
    "    \"\"\"\n",
    "    reading a species tree in Phyloxml format using ete3 package .\n",
    "    \n",
    "    output (species_tree)   \n",
    "    \"\"\"\n",
    "    logger_hog.info(species_tree_address)\n",
    "    #print(round(os.path.getsize(species_tree_address)/1000),\"kb\")\n",
    "    project = Phyloxml()\n",
    "    project.build_from_file(species_tree_address)\n",
    "    # Each tree contains the same methods as a PhyloTree object\n",
    "    for species_tree in project.get_phylogeny():   \n",
    "        species_tree = species_tree\n",
    "\n",
    "    for node_species_tree in species_tree.traverse(strategy = \"postorder\"):\n",
    "        if node_species_tree.is_leaf():\n",
    "            temp1 =node_species_tree.phyloxml_clade.get_taxonomy()[0]\n",
    "            #print(temp1.get_code())     \n",
    "            node_species_tree.name = temp1.get_code()\n",
    "    #print(len(species_tree)); print(species_tree)\n",
    "    return (species_tree)   \n",
    "        \n",
    "\n",
    "def prepare_species_tree(rhog_i, species_tree):\n",
    "    \"\"\"\n",
    "    a function for extracting a subtree from the input species tree  a.k.a pruning,\n",
    "    based on the names of species in the rootHOG.\n",
    "    \n",
    "    output: species_tree (pruned), species_names_rhog, prot_names_rhog\n",
    "    \"\"\"\n",
    "    species_names_rhog= []\n",
    "    prot_names_rhog=[]\n",
    "    for rec in rhog_i:\n",
    "        prot_name= rec.name #'tr|E3JPS4|E3JPS4_PUCGT\n",
    "        #prot_name = prot_name_full.split(\"|\")[1].strip() # # 'tr|E3JPS4|E3JPS4_PUCGT\n",
    "        species_name = prot_name.split(\"|\")[-1].split(\"_\")[-1]\n",
    "        if species_name=='RAT': species_name=\"RATNO\"\n",
    "        species_names_rhog.append(species_name)\n",
    "        prot_names_rhog.append(prot_name)\n",
    "\n",
    "    species_names_uniqe = set(species_names_rhog)\n",
    "    logger_hog.info(\"The number of unique species in the rHOG is \"+str(len(species_names_uniqe))+\".\")\n",
    "    species_tree.prune(species_names_uniqe, preserve_branch_length=True )\n",
    "    #species_tree.write()\n",
    "    for node in species_tree.traverse(strategy = \"postorder\"):\n",
    "        node_name = node.name\n",
    "        if len(node_name) <1: \n",
    "            if node.is_leaf():\n",
    "                node.name = \"leaf_\"+str(num_leaves_no_name)\n",
    "            else:\n",
    "                node_children = node.children\n",
    "                list_children_names = [node_child.name for node_child in node_children]\n",
    "                node.name = '_'.join(list_children_names)\n",
    "    #print(\"Working on the following species tree.\")\n",
    "    #print(species_tree)\n",
    "    \n",
    "    return (species_tree, species_names_rhog, prot_names_rhog)\n",
    "\n",
    "\n",
    "def merge_msa(list_msas):     \n",
    "    \"\"\"\n",
    "    merge a list of MSAs (multiple sequnce aligmnet)\n",
    "    by run mafft on them.\n",
    "    Each element of msa should be a MultipleSeqAlignment object. \n",
    "    \n",
    "    output: merged (msa)\n",
    "    \"\"\"\n",
    "    logging.debug(list_msas)\n",
    "    logging.debug(str(list_msas[0][0].id)+\"\\n\")\n",
    "    wrapper_mafft_merge = mafft.Mafft(list_msas, datatype=\"PROTEIN\") \n",
    "    wrapper_mafft_merge.options['--merge'].active = True\n",
    "    merged = wrapper_mafft_merge()\n",
    "    logger_hog.info(str(len(list_msas))+\" msas are merged into one with the length of \"+str(len(merged))+\" \"+str(len(merged[0])) )\n",
    "    return merged\n",
    " \n",
    "def infer_gene_tree(msa, gene_tree_file_addr):\n",
    "    \n",
    "    \"\"\"\n",
    "    infere gene tree using fastTree for the input msa\n",
    "    and write it as a file\n",
    "    \n",
    "    \n",
    "    output: gene tree in nwk format \n",
    "    \"\"\"\n",
    "    wrapper_tree=fasttree.Fasttree(msa, datatype=\"PROTEIN\")\n",
    "    wrapper_tree.options.options['-fastest']    \n",
    "    result_tree1 = wrapper_tree()\n",
    "\n",
    "    time_taken_tree = wrapper_tree.elapsed_time \n",
    "    result_tree2 = wrapper_tree.result\n",
    "    tree_nwk=str(result_tree2[\"tree\"])\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    # for development we write the gene tree, the name of file should be limit in size in linux.\n",
    "    # danger of overwriting\n",
    "    if len(gene_tree_file_addr)>255: gene_tree_file_addr = gene_tree_file_addr[:250]+\".nwk\"\n",
    "    file_gene_tree = open(gene_tree_file_addr,\"w\")\n",
    "    file_gene_tree.write(tree_nwk)\n",
    "    file_gene_tree.write(\";\\n\")\n",
    "    file_gene_tree.close() \n",
    "    \n",
    "    return tree_nwk\n",
    "  \n",
    "    \n",
    "    \n",
    "def lable_SD_internal_nodes(tree_out):\n",
    "    \"\"\"\n",
    "    for the input gene tree, run the species overlap method\n",
    "    and label internal nodes of the gene tree\n",
    "    \n",
    "    output: labeled gene tree\n",
    "    \"\"\"\n",
    "    species_name_dic={}\n",
    "    counter_S=0\n",
    "    counter_D=0\n",
    "    \n",
    "    for node in tree_out.traverse(strategy = \"postorder\"):\n",
    "        #print(\"** now working on node \",node.name) # node_children\n",
    "        if node.is_leaf() :\n",
    "            prot_i = node.name\n",
    "            species_name_dic[node] = { str(prot_i).split(\"|\")[-1].split(\"_\")[-1] }\n",
    "        else:\n",
    "            node.name= \"S/D\"\n",
    "            leaves_list = node.get_leaves()   #print(\"leaves_list\", leaves_list)\n",
    "            species_name_set = set([ str(prot_i).split(\"|\")[-1].split(\"_\")[-1] for prot_i in leaves_list])\n",
    "            #print(\"species_name_set\", species_name_set)\n",
    "            species_name_dic[node] = species_name_set\n",
    "\n",
    "            node_children = node.children            #print(node_children)\n",
    "            node_children_species_list = [species_name_dic[node_child] for node_child in node_children] # list of sets\n",
    "            #print(\"node_children_species_list\", node_children_species_list)\n",
    "            node_children_species_intersection = set.intersection(*node_children_species_list)\n",
    "\n",
    "            if  node_children_species_intersection : #print(\"node_children_species_list\",node_children_species_list)\n",
    "                counter_D += 1\n",
    "                node.name = \"D\"+str(counter_D)\n",
    "            else:\n",
    "                counter_S += 1\n",
    "                node.name = \"S\"+str(counter_S)\n",
    "    return tree_out\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def infer_HOG_rhog(rhogid_num, address_rhogs_folder, species_tree_address): \n",
    "    \"\"\"\n",
    "    The prot sequences of a rootHOG are located in the fasta file address_rhogs_folder+\"HOG_rhogid_num.fa,\n",
    "    we want to infer all subHOGs of this rootHOG for different taxanomic levels.\n",
    "    \n",
    "    output: a python dict (HOG_thisLevel):  key=taxanomic level, value= a list of subHOGs.   \n",
    "    \"\"\"\n",
    "    # rhogid_num = rhogid_num_list[rhogid_num_i]\n",
    "    logger_hog.info(\"\\n\"+\"=\"*50+\"\\n\"+\"Working on root hog: \"+str(rhogid_num)+\". \\n\")  # +\", \",rhogid_num_i,\"-th. \\n\"\n",
    "    prot_address = address_rhogs_folder+\"HOG_\"+str(rhogid_num)+\".fa\"\n",
    "    rhog_i = list(SeqIO.parse(prot_address, \"fasta\")) \n",
    "    logger_hog.info(\"number of proteins in the rHOG is \"+str(len(rhog_i))+\".\")\n",
    "\n",
    "    (species_tree) = read_species_tree(species_tree_address)\n",
    "    (species_tree, species_names_rhog, prot_names_rhog) = prepare_species_tree(rhog_i, species_tree)\n",
    "    #species_tree.write() #print(species_tree.write())\n",
    "    \n",
    "    dic_sub_hogs = {}\n",
    "    # finding hogs at each level of species tree (from leaves to root, bottom up)    \n",
    "    for node_species_tree in species_tree.traverse(strategy = \"postorder\"):\n",
    "        if node_species_tree.is_leaf() : \n",
    "            # each leaf itself is a subhog\n",
    "            continue\n",
    "        #print(\"\\n\"+\"*\"*15+\"\\n\",\"Finding hogs for the taxonomic level:\", node_species_tree.name,\"\\n\")\n",
    "        dic_sub_msas = []\n",
    "        (dic_sub_hogs) = infer_HOG_thisLevel(node_species_tree, rhog_i, species_names_rhog, dic_sub_hogs, rhogid_num)\n",
    "        HOG_thisLevel = dic_sub_hogs[node_species_tree.name]\n",
    "        logger_hog.info(\"subHOGs in thisLevel are \"+' '.join([\"[\"+str(i)+\"]\" for i in HOG_thisLevel])+\" .\")\n",
    "    return (HOG_thisLevel)\n",
    "\n",
    "\n",
    "       \n",
    "def infer_HOG_thisLevel(node_species_tree, rhog_i, species_names_rhog, dic_sub_hogs, rhogid_num):\n",
    "    \n",
    "    \n",
    "    gene_trees_folder = address_working_folder + \"/gene_trees_test/\"\n",
    "    if not os.path.exists(gene_trees_folder) :\n",
    "        os.mkdir(gene_trees_folder) \n",
    "    \n",
    "    if len(rhog_i) ==0:\n",
    "        logger_hog.warning('There is no protein in the rHOG: '+str(rhogid_num)) \n",
    "        dic_sub_hogs[node_species_tree.name] = []\n",
    "        return (dic_sub_hogs)\n",
    "    \n",
    "    elif len(rhog_i) ==1:\n",
    "        logger_hog.warning('There is only one protein in the rHOG: '+str(rhogid_num)) \n",
    "        node_species_name = node_species_tree.children[0].name # there is only one species (for the one protein)\n",
    "        prot= rhog_i[0]\n",
    "        sub_hog_leaf = HOG(prot, node_species_name, rhogid_num)\n",
    "        subHOGs_children = [sub_hog_leaf]\n",
    "        HOG_this_level = subHOGs_children  \n",
    "        dic_sub_hogs[node_species_tree.name] = HOG_this_level\n",
    "        return (dic_sub_hogs)\n",
    "        \n",
    "    sub_msa_list_lowerLevel = [] # including subHOGS of lower level \n",
    "    subHOGs_children = []\n",
    "    \n",
    "    #print(\"working on node\", node_species_tree.name,\"with\",len(node_species_tree.children),\"children.\")\n",
    "    for node_child in node_species_tree.children:\n",
    "        if  node_child.is_leaf():\n",
    "            node_species_name = node_child.name\n",
    "            #extracting those proteins of the rHOG that belongs to this species (child node of species tree)             \n",
    "            interest_list = [idx  for idx in range(len(species_names_rhog)) if species_names_rhog[idx] == node_species_name ]\n",
    "            rhog_part = [rhog_i[i] for i in interest_list]\n",
    "            #sub_msa = [MultipleSeqAlignment([i]) for i in rhog_part]             #print(\"len\",len(rhog_part))\n",
    "\n",
    "            for prot in rhog_part : \n",
    "                sub_hog_leaf = HOG(prot, node_species_name, rhogid_num  ) # node_species_tree.name\n",
    "                #list_all_hogs_ever.append(sub_hog_leaf)\n",
    "                subHOGs_children.append(sub_hog_leaf)                \n",
    "        else:   # the child node is an internal node, subHOGs are inferred till now during traversing.\n",
    "            #print(\"sub msa for internal node\", node_child.name,\"is read from dic.\")\n",
    "            if node_child.name in dic_sub_hogs:\n",
    "                sub_hogs_child  = dic_sub_hogs[node_child.name]\n",
    "                subHOGs_children += sub_hogs_child\n",
    "            else:\n",
    "                logger_hog.error(\"Error 131, no sub msa for the internal node \",node_child.name, node_child, \"\\n\",dic_sub_hogs)\n",
    "                assert 2==1 \n",
    "    temp11=[]\n",
    "    for temp in [i._members for i in subHOGs_children]:\n",
    "        temp11.append([ prot.split('|')[2] for prot in temp])\n",
    "    #print(\"there are \",len(subHOGs_children),\"subHOGs lower of this level:\",[i._hogid for i in subHOGs_children],temp11)\n",
    "    #print(\"We want to infer subHOGs at this level,i.e. merge few of them.\")    \n",
    "    subHOG_to_be_merged_set_other_Snodes = []\n",
    "    \n",
    "    if len(subHOGs_children) ==0:\n",
    "        logger_hog.error('Error 139, There is no protein in this subhog, for rhog'+str(rhogid_num)) \n",
    "\n",
    "    elif len(subHOGs_children) ==1:\n",
    "        HOG_this_level = subHOGs_children        \n",
    "        #print(\"**** error 134 *** \", len(subHOGs_children),subHOGs_children) #return (-1,-1,-1)\n",
    "\n",
    "    else:\n",
    "\n",
    "        sub_msa_list_lowerLevel_ready = [hog._msa for hog in subHOGs_children]\n",
    "        merged_msa = merge_msa(sub_msa_list_lowerLevel_ready) \n",
    "        logger_hog.info(\"All subHOGs are merged, merged msa is with length of\"+str(len(merged_msa))+\" \"+str(len(merged_msa[0]))+\".\")\n",
    "        \n",
    "        gene_tree_file_addr =  gene_trees_folder+ \"/tree_\"+str(rhogid_num)+\"_\"+str(node_species_tree.name)+\".nwk\"\n",
    "        gene_tree_raw = infer_gene_tree(merged_msa, gene_tree_file_addr)\n",
    "        gene_tree = Tree(gene_tree_raw+\";\", format=0)\n",
    "        logger_hog.info(\"Gene tree is infered with length of \"+str(len(gene_tree))+\".\")\n",
    "        #gene_tree_i +=1\n",
    "        R = gene_tree.get_midpoint_outgroup()\n",
    "        gene_tree.set_outgroup(R)  #print(\"Midpoint rooting is done for gene tree.\")\n",
    "        gene_tree = lable_SD_internal_nodes(gene_tree)\n",
    "        #print(\"Overlap speciation is done for internal nodes of gene tree, as following:\")\n",
    "        print(str(gene_tree.write(format=1))[:-1]+str(gene_tree.name)+\":0;\")\n",
    "\n",
    "        tree_leaves = [i.name for i in gene_tree.get_leaves() ]\n",
    "        #assigned_leaves_to_hog = []        #sub_msas_list_this_level = []\n",
    "        subHOGs_id_children_assigned = [] # the same as  subHOG_to_be_merged_all_id \n",
    "        HOG_this_level = []\n",
    "        subHOG_to_be_merged_set_other_Snodes = []\n",
    "        subHOG_to_be_merged_set_other_Snodes_flattned_temp  = []\n",
    "        for node in gene_tree.traverse(strategy = \"preorder\", is_leaf_fn= lambda n :   hasattr(n, \"processed\") and   n.processed == True  ): # start from root\n",
    "            #print(\"Leaves assigned to hog are \", assigned_leaves_to_hog)   #print(\"Traversing gene tree. Now at node\", node.name)\n",
    "            if not node.is_leaf() : \n",
    "                node_leaves_name = [ i.name for i in node.get_leaves() ] \n",
    "                #print(node_leaves_name)\n",
    "\n",
    "                if node.name[0] ==\"S\":  # this is a sub-hog.\n",
    "                    subHOG_to_be_merged = [ ]\n",
    "                    for node_leave_name in node_leaves_name: #print(node_leave_name)\n",
    "                        for subHOG in subHOGs_children :\n",
    "                            subHOG_members= subHOG._members\n",
    "                            if node_leave_name in subHOG_members:  # could be improved\n",
    "                                if subHOG._hogid  not in subHOG_to_be_merged_set_other_Snodes_flattned_temp:\n",
    "                                    subHOG_to_be_merged.append(subHOG)\n",
    "                                    subHOGs_id_children_assigned.append(subHOG._hogid)\n",
    "                                else:\n",
    "                                    print(\"issue 184\",node.name,subHOG._hogid, node_leave_name)\n",
    "                                    if  \"processed\" in  node: print(node.name)\n",
    "                                    else:  print(\"processed not in \", node.name) # print(node_leave_name,\"is in \",subHOG._hogid)\n",
    "                    if subHOG_to_be_merged :\n",
    "                        subHOG_to_be_merged_set = set(subHOG_to_be_merged)     \n",
    "                        taxnomic_range= node_species_tree.name\n",
    "                        HOG_this_node = HOG(subHOG_to_be_merged_set,taxnomic_range,rhogid_num , msa= merged_msa) \n",
    "                        HOG_this_level.append(HOG_this_node)\n",
    "                        subHOG_to_be_merged_set_other_Snodes.append([i._hogid for i in subHOG_to_be_merged_set])\n",
    "                        subHOG_to_be_merged_set_other_Snodes_flattned_temp= [item for items in subHOG_to_be_merged_set_other_Snodes for item in items]            \n",
    "                    #  I don't need to traverse deeper in this clade\n",
    "                    node.processed = True #print(\"?*?*  \", node.name)\n",
    "                                        \n",
    "            subHOG_to_be_merged_set_other_Snodes_flattned= [item for items in subHOG_to_be_merged_set_other_Snodes for item in items]       \n",
    "            if  [i._hogid for i in subHOGs_children] == subHOG_to_be_merged_set_other_Snodes_flattned:\n",
    "                break\n",
    "        for subHOG in subHOGs_children :        # for the single branch  ( D include a  subhog and a S node. )\n",
    "            if  subHOG._hogid  not in subHOGs_id_children_assigned :   #print(\"here\", subHOG)\n",
    "                HOG_this_level.append(subHOG)\n",
    "        prot_list_sbuhog= [i._members for i in HOG_this_level]\n",
    "        prot_list_sbuhog_short = []\n",
    "        for prot_sub_list_sbuhog in prot_list_sbuhog:\n",
    "            prot_list_sbuhog_short.append([ prot.split('|')[2] for prot in prot_sub_list_sbuhog])\n",
    "        logger_hog.info(\"- \"+str(len(prot_list_sbuhog_short))+\"HOGs are inferred at the level \"+node_species_tree.name+\": \"+ \" \".join([str(i) for i in prot_list_sbuhog_short]))\n",
    "    #print(\"By merging \",subHOG_to_be_merged_set_other_Snodes)\n",
    "\n",
    "    #check for conflicts in merging\n",
    "    #     for i in range(subHOG_to_be_merged_set_other_Snodes):\n",
    "    #         if \n",
    "    #         for i in range(subHOG_to_be_merged_set_other_Snodes):\n",
    "    # print(\"*&*& \",node_species_tree.name)\n",
    "    dic_sub_hogs[node_species_tree.name] = HOG_this_level\n",
    "    return (dic_sub_hogs)\n",
    "\n",
    "\n",
    "\n",
    "class HOG:\n",
    "    _hogid_iter = 1000\n",
    "    def __init__(self, input_instantiate, taxnomic_range, rhogid_num, msa = None):       # _prot_names\n",
    "        # the input_instantiate could be either\n",
    "        #     1) a protein as the biopython seq record  SeqRecord(seq=Seq('MAPSSRSPSPRT. ] \n",
    "        # or  2) a set of intances of class HOG   wit a big msa\n",
    "        # those variable starting with _ are local to the class, should not access directly  (although it is possbile)\n",
    "        self._rhogid_num= rhogid_num\n",
    "        self.__class__._hogid_iter += 1\n",
    "        self._hogid= \"hog\"+str(self._rhogid_num)+\"_\"+str(self.__class__._hogid_iter)\n",
    "        self._taxnomic_range = taxnomic_range  #print(\"**** a new HOG is instantiated with id\", self._hogid)\n",
    "\n",
    "        if  isinstance(input_instantiate, SeqRecord):    #if len(sub_hogs)==1:\n",
    "            only_protein = input_instantiate # only one seq, only on child, leaf            \n",
    "            self._members = set([only_protein.id])\n",
    "            self._msa =  MultipleSeqAlignment([only_protein])\n",
    "            self._subhogs = []\n",
    "            # <<class 'Bio.Align.MultipleSeqAlignment'> instance (1 records of length 314) at 7f0e86c713d0>\n",
    "            \n",
    "        elif  msa    and   all(isinstance(x, HOG) for x in input_instantiate):  \n",
    "            # here we want to merge few subHOGs and creat a new HOG.   #the n\n",
    "            sub_hogs = input_instantiate\n",
    "            hog_members = set()\n",
    "            for sub_hog in sub_hogs: \n",
    "                hog_members |= sub_hog.get_members()  #union\n",
    "            self._members =  hog_members              #set.union(*tup) \n",
    "            self._subhogs = list(input_instantiate)  # full members\n",
    "            \n",
    "            max_num_seq=30 # subsampling in msa \n",
    "            records_full = [record for record in msa if record.id in self._members]\n",
    "            if len(records_full)> max_num_seq: \n",
    "                records_sub_sampled = sample(records_full, max_num_seq)   #  without replacement.\n",
    "                logger_hog.info(\"we are doing subsamping now from \"+str(len(records_full))+\" to \"+str(max_num_seq)+\"seqs.\")\n",
    "            else:\n",
    "                records_sub_sampled = records_full\n",
    "            # removing some columns completely gap -  (not x   )\n",
    "            # now select those proteins \n",
    "            self._msa =  MultipleSeqAlignment(records_sub_sampled)\n",
    "            # without replacement sampling ,  # self._children = sub_hogs # as legacy  ?\n",
    "        else:\n",
    "            logger_hog.error(\"Error 169,  check the input format to instantiate a HOG class\")\n",
    "            assert False\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"an object of class HOG of hogID=\"+self._hogid+\", length=\"+str(len(self._members))+\", taxonomy= \"+str(self._taxnomic_range)\n",
    "            \n",
    "    def get_members(self):\n",
    "        return set(self._members)\n",
    "        #merge, gene tree, midpoint, lable_SD_internal_nodes, traverse_geneTree_assign_hog\n",
    "\n",
    "    def to_orthoxml(self, indent=0):\n",
    "        hog_elemnt = ET.Element('orthologGroup', attrib={\"id\":str(self._hogid)})\n",
    "        property_element = ET.SubElement(hog_elemnt, \"property\", attrib={\"name\":\"TaxRange\", \"value\":str(self._taxnomic_range)})\n",
    "        # the following could be improved ???   without this if it will be like, one property is enough\n",
    "        #<orthologGroup>\n",
    "        #    <property name=\"TaxRange\" value=\"GORGO_HUMAN_PANTR\"/>\n",
    "        #    <property name=\"TaxRange\" value=\"GORGO_HUMAN_PANTR\"/>\n",
    "        #if property_element not in hog_elemnt:\n",
    "        #    hog_elemnt.append(property_element)\n",
    "        #    print(\"*\")\n",
    "        #gene = ET.SubElement(species, \"gene\", attrib={\"id\":str(gene_counter), \"protId\":query_prot_record.id})               \n",
    "        #hog_elemnt = ET.SubElement(species,\n",
    "        \n",
    "        if len(self._subhogs) == 0:\n",
    "            #print(\"we are here   ********???--??? \",self._hogid)\n",
    "            geneRef_elemnt = ET.Element('geneRef', attrib={'id': str(gene_id_name[list(self._members)[0]] )}) # # gene_id_name[query_prot_record.id]\n",
    "            #hog_elemnt.append(geneRef_elemnt)\n",
    "            # could be improved when the rhog contains only one protein\n",
    "            return geneRef_elemnt # hog_elemnt\n",
    "        \n",
    "        def _sorter_key(sh): \n",
    "            return sh._taxnomic_range\n",
    "        self._subhogs.sort(key=_sorter_key)  #print(f'{\" \"*indent}subhog: {self._taxnomic_range}:')\n",
    "        for sub_clade, sub_hogs in itertools.groupby(self._subhogs, key=_sorter_key):\n",
    "            list_of_subhogs_of_same_clade = list(sub_hogs) #print(f'{\" \"*(indent+1)} clade: {sub_clade} with {str(len(list_of_subhogs_of_same_clade))}')\n",
    "            if len(list_of_subhogs_of_same_clade) > 1:\n",
    "                paralog_element = ET.Element('paralogGroup')\n",
    "                for sh in list_of_subhogs_of_same_clade:\n",
    "                    paralog_element.append(sh.to_orthoxml(indent+2))\n",
    "                hog_elemnt.append(paralog_element)\n",
    "            else:\n",
    "                hog_elemnt.append(list_of_subhogs_of_same_clade[0].to_orthoxml(indent+2))           \n",
    "        return hog_elemnt\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ae9a7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program has started. The oma database address is in  ./test_fastgethog/omamer_database/oma_path/OmaServer.h5\n",
      "14:25:24 - OMA data is parsed and its release name is: OmaStandalone; 1.0.x\n",
      "14:25:24 - There are 5 species in the OMA database.\n",
      "14:25:24 - The are 3 species in the proteome folder.\n",
      "14:25:24 - There are  3  species in the hogmap folder.\n",
      "14:25:24 - The first species UP000000798_224324  contains  1553  proteins.\n",
      "14:25:24 - The first protein of first species is  sp|D0VWU4|SECE_AQUAE\n",
      "14:25:24 - Filtering proteins started.\n",
      "14:25:24 - For the rest of species, all proteins were mapped using OMAmer.\n",
      "3 1553\n",
      "14:25:24 - Sequences of rootHOGs are writtend as fasta file in ./test_fastgethog/rhogs_out/\n"
     ]
    }
   ],
   "source": [
    "# rHog_is_ready= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd3418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hog:\n",
      "==================================================\n",
      "Working on root hog: 556836. \n",
      "\n",
      "INFO:hog:number of proteins in the rHOG is 9.\n",
      "INFO:hog:./test_fastgethog/lineage_tree_qfo.phyloxml\n",
      "INFO:hog:The number of unique species in the rHOG is 8.\n",
      "INFO:hog:2 msas are merged into one with the length of 2 204\n",
      "INFO:hog:All subHOGs are merged, merged msa is with length of2 204.\n",
      "INFO:hog:Gene tree is infered with length of 2.\n",
      "INFO:hog:- 1HOGs are inferred at the level MOUSE_RATNO: ['NTAL_MOUSE', 'NTAL_RAT']\n",
      "INFO:hog:subHOGs in thisLevel are [object of class HOG of hogID=hog556836_1003, length=2, taxonomy= MOUSE_RATNO] .\n",
      "INFO:hog:3 msas are merged into one with the length of 3 243\n",
      "INFO:hog:All subHOGs are merged, merged msa is with length of3 243.\n",
      "INFO:hog:Gene tree is infered with length of 3.\n",
      "INFO:hog:- 1HOGs are inferred at the level GORGO_HUMAN_PANTR: ['G3SKS4_GORGO', 'H2QUR1_PANTR', 'NTAL_HUMAN']\n",
      "INFO:hog:subHOGs in thisLevel are [object of class HOG of hogID=hog556836_1007, length=3, taxonomy= GORGO_HUMAN_PANTR] .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sp|Q9JHL0|NTAL_MOUSE:0.0908936,sp|Q8CGL2|NTAL_RAT:0.0908936)S1:0;\n",
      "(tr|H2QUR1|H2QUR1_PANTR:0.00843292,(tr|G3SKS4|G3SKS4_GORGO:0.00833703,sp|Q9GZY6|NTAL_HUMAN:5e-09)S1:0.00843292)S2:0;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hog:2 msas are merged into one with the length of 5 250\n",
      "INFO:hog:All subHOGs are merged, merged msa is with length of5 250.\n",
      "INFO:hog:Gene tree is infered with length of 5.\n",
      "INFO:hog:- 1HOGs are inferred at the level MOUSE_RATNO_GORGO_HUMAN_PANTR: ['NTAL_RAT', 'NTAL_MOUSE', 'G3SKS4_GORGO', 'H2QUR1_PANTR', 'NTAL_HUMAN']\n",
      "INFO:hog:subHOGs in thisLevel are [object of class HOG of hogID=hog556836_1008, length=5, taxonomy= MOUSE_RATNO_GORGO_HUMAN_PANTR] .\n",
      "INFO:hog:2 msas are merged into one with the length of 2 383\n",
      "INFO:hog:All subHOGs are merged, merged msa is with length of2 383.\n",
      "INFO:hog:Gene tree is infered with length of 2.\n",
      "INFO:hog:- 1HOGs are inferred at the level CANLF_BOVIN: ['G3MYV0_BOVIN', 'A0A5F4CAL9_CANLF']\n",
      "INFO:hog:subHOGs in thisLevel are [object of class HOG of hogID=hog556836_1011, length=2, taxonomy= CANLF_BOVIN] .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((sp|Q9JHL0|NTAL_MOUSE:0.0945749,sp|Q8CGL2|NTAL_RAT:0.0850889)S1:0.204182,(tr|H2QUR1|H2QUR1_PANTR:0.0167884,(tr|G3SKS4|G3SKS4_GORGO:0.00826907,sp|Q9GZY6|NTAL_HUMAN:5e-09)S2:5e-09)S3:0.204182)S4:0;\n",
      "(tr|A0A5F4CAL9|A0A5F4CAL9_CANLF:0.219168,tr|G3MYV0|G3MYV0_BOVIN:0.219168)S1:0;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hog:2 msas are merged into one with the length of 7 422\n",
      "INFO:hog:All subHOGs are merged, merged msa is with length of7 422.\n",
      "INFO:hog:Gene tree is infered with length of 7.\n",
      "INFO:hog:- 1HOGs are inferred at the level MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN: ['G3MYV0_BOVIN', 'NTAL_RAT', 'NTAL_MOUSE', 'G3SKS4_GORGO', 'H2QUR1_PANTR', 'A0A5F4CAL9_CANLF', 'NTAL_HUMAN']\n",
      "INFO:hog:subHOGs in thisLevel are [object of class HOG of hogID=hog556836_1012, length=7, taxonomy= MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN] .\n",
      "INFO:hog:3 msas are merged into one with the length of 9 429\n",
      "INFO:hog:All subHOGs are merged, merged msa is with length of9 429.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((sp|Q9JHL0|NTAL_MOUSE:0.0648269,sp|Q8CGL2|NTAL_RAT:0.109031)S1:0.130208,((tr|A0A5F4CAL9|A0A5F4CAL9_CANLF:0.316505,tr|G3MYV0|G3MYV0_BOVIN:0.110056)S2:0.0501216,(tr|H2QUR1|H2QUR1_PANTR:0.0160516,(tr|G3SKS4|G3SKS4_GORGO:0.00791884,sp|Q9GZY6|NTAL_HUMAN:5e-09)S3:5e-09)S4:0.172186)S5:0.130208)S6:0;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hog:Gene tree is infered with length of 9.\n",
      "INFO:hog:- 1HOGs are inferred at the level CHICK_MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN: ['A0A1D5P223_CHICK', 'NTAL_RAT', 'A0A5F4CAL9_CANLF', 'NTAL_HUMAN', 'G3MYV0_BOVIN', 'NTAL_MOUSE', 'G3SKS4_GORGO', 'NTAL_CHICK', 'H2QUR1_PANTR']\n",
      "INFO:hog:subHOGs in thisLevel are [object of class HOG of hogID=hog556836_1015, length=9, taxonomy= CHICK_MOUSE_RATNO_GORGO_HUMAN_PANTR_CANLF_BOVIN] .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((tr|A0A1D5P223|A0A1D5P223_CHICK:0.00462657,sp|Q5S7W5|NTAL_CHICK:5e-09)D1:0.492744,(((tr|A0A5F4CAL9|A0A5F4CAL9_CANLF:0.308172,tr|G3MYV0|G3MYV0_BOVIN:0.101099)S1:0.0475405,(sp|Q9JHL0|NTAL_MOUSE:0.0629393,sp|Q8CGL2|NTAL_RAT:0.105049)S2:0.248291)S3:0.00311226,(tr|H2QUR1|H2QUR1_PANTR:0.0157155,(tr|G3SKS4|G3SKS4_GORGO:0.00775932,sp|Q9GZY6|NTAL_HUMAN:5e-09)S4:5e-09)S5:0.167083)S6:0.492744)S7:0;\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "from ete3 import Phyloxml\n",
    "from ete3 import Tree\n",
    "\n",
    "import zoo.wrappers.aligners.mafft as mafft\n",
    "import zoo.wrappers.treebuilders.fasttree as fasttree\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    logging.basicConfig()\n",
    "    logger_hog = logging.getLogger(\"hog\")\n",
    "    logger_hog.setLevel(logging.INFO) # WARN  \n",
    "    # make sure addresses end with \"/\" \n",
    "    address_rhogs_folder =  \"./rhogs/\"\n",
    "    address_working_folder = \"./\" \n",
    "\n",
    "    species_tree_address= address_working_folder+\"lineage_tree_qfo.phyloxml\"\n",
    "    \n",
    "    rHog_is_ready= True\n",
    "\n",
    "\n",
    "\n",
    "    if rHog_is_ready : \n",
    "        \n",
    "        ## create a list of rootHOG IDs  stored in the folder of rHOG .\n",
    "        #rhog_files = listdir(address_rhogs_folder)\n",
    "        #rhogid_num_list= []\n",
    "        #for rhog_file in rhog_files:\n",
    "        #    if rhog_file.split(\".\")[-1] == \"fa\":\n",
    "        #        rhogid_num = int(rhog_file.split(\".\")[0].split(\"_\")[1])\n",
    "        #        rhogid_num_list.append(rhogid_num)\n",
    "\n",
    "        rhogid_num = 556836  \n",
    "        (HOG_thisLevel) = infer_HOG_rhog(rhogid_num, address_rhogs_folder, species_tree_address)\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        import pyoma.browser.db as db\n",
    "        \n",
    "        oma_database_address = address_working_folder+\"omamer_database/oma_path/OmaServer.h5\"\n",
    "        print(\"program has started. The oma database address is in \",oma_database_address)\n",
    "        (oma_db, list_oma_speices) = parse_oma_db(oma_database_address)\n",
    "\n",
    "        (query_species_names, query_prot_records_species) = parse_proteome(list_oma_speices)   \n",
    "\n",
    "        query_prot_records_species = add_species_name(query_prot_records_species,query_species_names)\n",
    "\n",
    "        hogmap_allspecies_elements = parse_hogmap_omamer(query_species_names)\n",
    "\n",
    "        (query_prot_names_species_mapped, prots_hogmap_hogid_allspecies, prots_hogmap_subfscore_allspecies, prots_hogmap_seqlen_allspecies, prots_hogmap_subfmedseqlen_allspecies) = hogmap_allspecies_elements\n",
    "        query_prot_records_species_filtered =  filter_prot_mapped(query_species_names, query_prot_records_species, query_prot_names_species_mapped)\n",
    "        print(len(query_prot_records_species_filtered),len(query_prot_records_species_filtered[0]))\n",
    "        \n",
    "        (rhogid_num_list, rhogids_prot_records_query) = group_prots_rootHOGs(prots_hogmap_hogid_allspecies, address_rhogs_folder)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c483d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a66919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613ca3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca90d132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f731fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca32ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519071c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef945f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04749e93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#     rhogid_num_list_temp = rhogid_num_list[:10] # [\"476045\"] #[\"836500\"]# \n",
    "    \n",
    "#     all_prot_temp_list= []\n",
    "#     for rhogid_num in rhogid_num_list_temp:\n",
    "#         prot_address = address_out_hog+\"HOG_\"+str(rhogid_num)+\".fa\"\n",
    "#         rhog_i = list(SeqIO.parse(prot_address, \"fasta\")) \n",
    "#         for prot_i in rhog_i:\n",
    "#             all_prot_temp_list.append(prot_i.id)\n",
    "#     print(\"there are proteins \",len(all_prot_temp_list))   \n",
    "#     ortho_file  = ET.Element(\"orthoXML\", attrib={\"xmlns\":\"http://orthoXML.org/2011/\", \"origin\":\"OMA\", \"originVersion\":\"Nov 2021\", \"version\":\"0.3\"} ) # \n",
    "#     gene_counter =1000\n",
    "#     gene_id_name = {}\n",
    "    \n",
    "#     for species_i in range(len(query_species_names)):\n",
    "#         no_gene_species = True  # for code develop ment\n",
    "#         species_name = query_species_names[species_i]\n",
    "#         species = ET.SubElement(ortho_file, \"species\", attrib={\"name\":species_name, \"NCBITaxId\":\"1\"})\n",
    "#         database = ET.SubElement(species, \"database\", attrib={\"name\":\"QFO database \", \"version\":\"2020\"})\n",
    "#         genes = ET.SubElement(database, \"genes\")\n",
    "#         query_prot_records =  query_prot_records_species[species_i]\n",
    "#         for gene_i in range(len(query_prot_records)):                # [12:15]\n",
    "#             query_prot_record= query_prot_records[gene_i]\n",
    "#             gene_id_name[query_prot_record.id]= gene_counter\n",
    "\n",
    "#             protid_short = query_prot_record.id.split(\"|\")[1].strip() # tr|E3JPS4|E3JPS4_PUCGT\n",
    "            \n",
    "#             # make the file big\n",
    "#             if query_prot_record.id in all_prot_temp_list:\n",
    "                \n",
    "#                 gene = ET.SubElement(genes, \"gene\", attrib={\"id\":str(gene_counter), \"protId\":protid_short})               \n",
    "#                 no_gene_species = False  # for code develop ment             \n",
    "#             gene_counter += 1\n",
    "#         if no_gene_species :   # for code develop ment\n",
    "#             gene = ET.SubElement(genes, \"gene\", attrib={\"id\":str(gene_counter), \"protId\":protid_short})               \n",
    "#     groups_xml = ET.SubElement(ortho_file, \"groups\")\n",
    "#     dic_rhog_resolved={}\n",
    "#     list_rhog_done = []\n",
    "#     logger_hog.info(\"parrallel is started\")\n",
    "    \n",
    "#     number_max_workers = 4\n",
    "#     with concurrent.futures.ProcessPoolExecutor(max_workers = number_max_workers) as executor: \n",
    "#         for rhogid_num, output_values in zip(rhogid_num_list_temp, executor.map(infer_HOG_rhog, rhogid_num_list_temp)):\n",
    "#             (HOG_thisLevel) = output_values\n",
    "#             dic_rhog_resolved[rhogid_num]=1\n",
    "#             list_rhog_done.append(rhogid_num)\n",
    "#             for hog_i in HOG_thisLevel:\n",
    "#                 if len(hog_i._members)>1:\n",
    "#                     # could be improved \n",
    "#                     HOG_thisLevel_xml = hog_i.to_orthoxml()\n",
    "#                     groups_xml.append(HOG_thisLevel_xml)\n",
    "#                     #print(hog_i._members)\n",
    "#     xml_str = minidom.parseString(ET.tostring(ortho_file)).toprettyxml(indent=\"   \")\n",
    "#     output_xml_name= address_working_folder + \"/xml_100_short.xml\"\n",
    "#     with open(output_xml_name, \"w\") as file_out:\n",
    "#         file_out.write(xml_str)\n",
    "#     logger_hog.info(\"output orthoxml is written in the file\"+str(output_xml_name))\n",
    "#     logger_hog.info(\"all done !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85fd0d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#     rhogid_num_list_temp = rhogid_num_list[:10*1000] \n",
    "#     all_prot_temp_list= []\n",
    "#     for rhogid_num in rhogid_num_list_temp:\n",
    "#         prot_address = address_out_hog+\"HOG_\"+str(rhogid_num)+\".fa\"\n",
    "#         rhog_i = list(SeqIO.parse(prot_address, \"fasta\")) \n",
    "#         for prot_i in rhog_i:\n",
    "#             all_prot_temp_list.append(prot_i.id)\n",
    "#     print(\"there are proteins \",len(all_prot_temp_list))   \n",
    "#     ortho_file  = ET.Element(\"orthoXML\", attrib={\"xmlns\":\"http://orthoXML.org/2011/\", \"origin\":\"OMA\", \"originVersion\":\"Nov 2021\", \"version\":\"0.3\"} ) # \n",
    "#     gene_counter =1000\n",
    "#     gene_id_name = {}  \n",
    "#     for species_i in range(len(query_species_names)):\n",
    "#         no_gene_species = True  # for code develop ment\n",
    "#         species_name = query_species_names[species_i]\n",
    "#         species = ET.SubElement(ortho_file, \"species\", attrib={\"name\":species_name, \"NCBITaxId\":\"1\"})\n",
    "#         database = ET.SubElement(species, \"database\", attrib={\"name\":\"QFO database \", \"version\":\"2020\"})\n",
    "#         genes = ET.SubElement(database, \"genes\")\n",
    "#         query_prot_records =  query_prot_records_species[species_i]\n",
    "#         for gene_i in range(len(query_prot_records)):                # [12:15]\n",
    "#             query_prot_record= query_prot_records[gene_i]\n",
    "#             gene_id_name[query_prot_record.id]= gene_counter\n",
    "\n",
    "#             protid_short = query_prot_record.id.split(\"|\")[1].strip() # tr|E3JPS4|E3JPS4_PUCGT\n",
    "#             # make the file big\n",
    "#             if query_prot_record.id in all_prot_temp_list:\n",
    "#                 gene = ET.SubElement(genes, \"gene\", attrib={\"id\":str(gene_counter), \"protId\":protid_short})               \n",
    "#                 no_gene_species = False  # for code develop ment\n",
    "#             gene_counter += 1\n",
    "#         if no_gene_species :   # for code develop ment\n",
    "#             gene = ET.SubElement(genes, \"gene\", attrib={\"id\":str(gene_counter), \"protId\":protid_short})                   \n",
    "#     groups_xml = ET.SubElement(ortho_file, \"groups\")\n",
    "#     dic_rhog_resolved={}\n",
    "#     list_rhog_done = []\n",
    "#     logger_hog.info(\"parrallel is started\")\n",
    "    \n",
    "#     xml_str = minidom.parseString(ET.tostring(ortho_file)).toprettyxml(indent=\"   \")\n",
    "#     output_xml_name= address_working_folder + \"/xml_10k_gene_name.xml\"\n",
    "#     with open(output_xml_name, \"w\") as file_out:\n",
    "#         file_out.write(xml_str)\n",
    "#     logger_hog.info(\"output orthoxml is written in the file\"+str(output_xml_name))\n",
    "#     logger_hog.info(\"all done !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import time\n",
    "# from sys import argvs    \n",
    "# #from scipy.cluster.hierarchy import dendrogram, linkage, ward, leaves_list, fcluster\n",
    "# import ete3\n",
    "# import concurrent.futures\n",
    "# #import ast #import pickle #import zoo #zoo.__file__ \n",
    "# from Bio.Seq import Seq \n",
    "# from collections import defaultdict\n",
    "# import matplotlib      #matplotlib.use('Agg')\n",
    "# import matplotlib.pyplot as plt\n",
    "# from random import sample\n",
    "# import itertools\n",
    "# from pyoma.browser.models import ProteinEntry\n",
    "# from pyoma.browser.hoghelper import build_hog_to_og_map\n",
    "# import xml.etree.ElementTree as ET\n",
    "# from xml.etree import ElementTree\n",
    "# from xml.dom import minidom\n",
    "\n",
    "# def run_one_msa(seqRecords_queries):\n",
    "#     #current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "#     #print(current_time, \"- working on new OG with length of \",len(seqRecords_OG_queries))    \n",
    "#     wrapper_mafft = mafft.Mafft(seqRecords_queries,datatype=\"PROTEIN\") \n",
    "#     # MAfft error: Alphabet 'U' is unknown. -> add --anysymbol argument needed to define in the sourse code\n",
    "#     # workaround sed \"s/U/X/g\"\n",
    "#     wrapper_mafft.options.options['--retree'].set_value(1)\n",
    "#     run_mafft = wrapper_mafft() # it's wrapper  storing the result  and time \n",
    "#     time_taken_mafft = wrapper_mafft.elapsed_time\n",
    "\n",
    "#     result_mafft = wrapper_mafft.result \n",
    "#     time_taken_mafft2 = wrapper_mafft.elapsed_time\n",
    "#     current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "#     #print(current_time,\"- time elapsed for MSA: \",time_taken_mafft2)\n",
    "#     #print(current_time,\"- MSA for an OG is just finished: \",time_taken_mafft2)\n",
    "#     return(result_mafft)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
